[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The bioDSC blog",
    "section": "",
    "text": "Example plots for R and Python\n\n\n\n\n\n\nplotting\n\n\nscripting\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nInstalling Python Software\n\n\n\n\n\n\npython\n\n\nsoftware\n\n\ninstallation\n\n\ntutorial\n\n\n\nInstalling software that allows you to easily work with Python.\n\n\n\n\n\nMar 11, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nShould I learn Python or R?\n\n\n\n\n\n\ngeneral\n\n\nworkshop-related\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nVisualizing Arabidopsis whole-genome alignments\n\n\n\n\n\n\ncomparative genomics\n\n\narabidopsis\n\n\ncrunchomics\n\n\n\nLearn how to plot genome rearrangements in this blog.\n\n\n\n\n\nNov 4, 2024\n\n\nMisha Paauw\n\n\n\n\n\n\n\nThe bioDSC blog: first post\n\n\n\n\n\n\nintro\n\n\ngeneral\n\n\n\nAn introduction to the blog and its purpose.\n\n\n\n\n\nOct 30, 2024\n\n\nMisha Paauw\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "*bio*DSC",
    "section": "",
    "text": "Tutorial pages will be uploaded to this section of the site! Stay tuned!"
  },
  {
    "objectID": "tutorials.html#tutorials",
    "href": "tutorials.html#tutorials",
    "title": "*bio*DSC",
    "section": "",
    "text": "Tutorial pages will be uploaded to this section of the site! Stay tuned!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The biological Data Science Center",
    "section": "",
    "text": "As a biologist at the Science Park of the University of Amsterdam, it’s likely that you will generate large amounts of data. The biological Data Science Center (bioDSC) was initiated to help you analyse that data.\nOur aim is to enable you to perform analyses yourself, which is why we offer workshops, on-line tutorials and in-person mentoring. We have an open-door policy: we encourage you to send an email or drop by one of our desks when you need help or input with your data analysis problem. For quick questions, check out our Slack channel!\nSend an email Drop by Slack"
  },
  {
    "objectID": "index.html#our-approach",
    "href": "index.html#our-approach",
    "title": "The biological Data Science Center",
    "section": "Our approach",
    "text": "Our approach\n\n\nThe bioDSC was initiated in September ’24, and we are currently finding out what works best to optimally support people with their data analyses.\nOur core members are embedded in several research groups at the Swammerdam Institute for Life Sciences (SILS), to be integrated in the community of researchers at Science Park. In addition, other researchers at SILS sometimes volunteer to further support data analyses and good practices."
  },
  {
    "objectID": "index.html#check-out-our-online-resources",
    "href": "index.html#check-out-our-online-resources",
    "title": "The biological Data Science Center",
    "section": "Check out our online resources",
    "text": "Check out our online resources\nCheck out our blog for quick reads on coding, which maybe inspiring for your research. Check out our tutorials for full protocols on analyses that are often performed by biologists at the UvA Science Park, such as RNA-seq.\nBlog Tutorials Further reading"
  },
  {
    "objectID": "index.html#check-out-our-workshops",
    "href": "index.html#check-out-our-workshops",
    "title": "The biological Data Science Center",
    "section": "Check out our workshops",
    "text": "Check out our workshops\nOur aim is to organize workshops on topics researchers are interested in and that will help them. We’re currently working on composing a list of workshops and will start to organize them soon. Check out our workshop page to see whether we already have a workshop that is of interest to you. If you’re interested in a topic that’s not on the list, don’t hesitate to send us an email or drop by. We can either help you out with your specific problem, or organize a workshop if it’s a topic of potential interest to many researchers.\nWorkshops"
  },
  {
    "objectID": "index.html#history",
    "href": "index.html#history",
    "title": "The biological Data Science Center",
    "section": "History",
    "text": "History\n\n\nAt the University of Amsterdam Science Park, enthusiastic researchers started the Science Park Study Group in 2017. The idea was to create a local culture of connecting and sharing knowledge with regard to coding in the biological sciences. Having started in September ’24 with the bioDSC, we aim to further boast this initiative and spirit.\nThe original website of the Science Park Study group can be found here."
  },
  {
    "objectID": "posts/python-or-R.html",
    "href": "posts/python-or-R.html",
    "title": "Should I learn Python or R?",
    "section": "",
    "text": "When you haven’t had any exposure to scripting languages, but know some type of scripting language might be convenient to analyze your data, it might be hard to determine which language (R, Python, Javascript, ..) you should look into.\nIn this blog post I list some typical use cases of Python and R such that you have an impression which language you use for what."
  },
  {
    "objectID": "posts/python-or-R.html#motivation-for-writing-this-blog-post",
    "href": "posts/python-or-R.html#motivation-for-writing-this-blog-post",
    "title": "Should I learn Python or R?",
    "section": "",
    "text": "When you haven’t had any exposure to scripting languages, but know some type of scripting language might be convenient to analyze your data, it might be hard to determine which language (R, Python, Javascript, ..) you should look into.\nIn this blog post I list some typical use cases of Python and R such that you have an impression which language you use for what."
  },
  {
    "objectID": "posts/python-or-R.html#python-versus-r",
    "href": "posts/python-or-R.html#python-versus-r",
    "title": "Should I learn Python or R?",
    "section": "Python versus R",
    "text": "Python versus R\nWhen analyzing scientific data, there are multiple tools available. For simple analyses, Excel, GraphPad or FIJI/ImageJ may suffice. When handling large amounts of data, or when you demand more specialized, custom types of analyses, it becomes convenient to use scripting languages such as Python or R. Many programming and scripting languages exist, but the main ones many researchers use are R and Python, on which I’ll focus here.\nBelow a list of what people typically use R and Python for.\n\nR\nTypical use cases for R include:\n\nbioinformatics, including specialized packages for\n\ngenomics, transcriptomics, proteomics, e.g.\n\nGene annotation (GO terms)\nEnrichment analyses, differential gene expression\nSeurat package for single cell analysis\n\n\nextensive data visualization\n\n(through the ggplot library)\ndisplaying high-dimensional data e.g. in tSNE or UMAPs, PCA analysis etc\n\ncomplex statistical modeling, specialized e.g. for\n\nEcology\nBioinformatics (as mentioned above)\n\n\n\n\nPython\n\nImage analysis\n\nAlthough MATLAB is a go-to tool for image analysis, Python has become the open-source almost on-par alternative\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images, e.g. to\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nE.g. SciPy, OpenCV, scikit, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\nThis offers similar capabilities as R\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nE.g. Keras and PyTorch\n\n\nhigh-throughput data analysis and automation\n\npython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nbioinformatics\n\nseems there’s less tools available, however offers packages to:\nPerform single cell analysis, such as SCANPY\nCustom tasks in bioinformatics pipelines (specialized mapping, e.g.) can use python\n\n\nWhat ChatGPT had to say about it: “R excels at statistical and visualization tasks, particularly for biostatistics and bioinformatics, while Python is preferred for machine learning, automation, and handling diverse data types in scalable pipelines. Many researchers use both, leveraging each for its strengths.”"
  },
  {
    "objectID": "posts/python-or-R.html#so-what-should-i-choose",
    "href": "posts/python-or-R.html#so-what-should-i-choose",
    "title": "Should I learn Python or R?",
    "section": "So, what should I choose?",
    "text": "So, what should I choose?\nIf you’re uncertain what you should learn, feel free to drop us an email, walk by our desks, or contact us in another way to have a chat about what is most suitable for you!\nSend an email Drop by Slack"
  },
  {
    "objectID": "posts/installing_conda_python.html",
    "href": "posts/installing_conda_python.html",
    "title": "Installing Python Software",
    "section": "",
    "text": "This tutorial explains how to install software on your computer that allows you to write and run Python scripts."
  },
  {
    "objectID": "posts/installing_conda_python.html#python-itself",
    "href": "posts/installing_conda_python.html#python-itself",
    "title": "Installing Python Software",
    "section": "1: Python itself",
    "text": "1: Python itself\nAs explained during our workshops, Python itself is a software that interprets written computer code, and uses that to make the computer do things. Sometimes Python is already installed on your computer, but often you need to install it.\nPython makes use of “libraries” (pre-written code with functions for specific tasks), that you likely need to download/install as well."
  },
  {
    "objectID": "posts/installing_conda_python.html#a-text-editor-dedicated-to-writing-python-code",
    "href": "posts/installing_conda_python.html#a-text-editor-dedicated-to-writing-python-code",
    "title": "Installing Python Software",
    "section": "2: A text editor dedicated to writing Python code",
    "text": "2: A text editor dedicated to writing Python code\nIt is convenient to also install another piece of software, in which you can write Python code, and which also allows you to “send” the script towards the actual Python software.\nThis way you can write Python code in a pleasant way, and also run it immediately. In practice, you’ll only interact with this editor."
  },
  {
    "objectID": "posts/installing_conda_python.html#software-installing-software.",
    "href": "posts/installing_conda_python.html#software-installing-software.",
    "title": "Installing Python Software",
    "section": "3: Software installing-software.",
    "text": "3: Software installing-software.\nInstalling software to manage software installation sounds perhaps a bit complicated, but it’ll make your life easier.\nInstalling Python, its libraries, and code editors, comes challenges, such as:\n\nIt is tedious to install all these things manually.\nYou might need a set of libraries for a specific task, which have installation requirements that exclude you from installing other libraries that you might need for another task.\n\nThese challenges are adressed by software that manages such “dependencies” and “conflicts”."
  },
  {
    "objectID": "posts/installing_conda_python.html#how-to-install-python-software-using-miniconda",
    "href": "posts/installing_conda_python.html#how-to-install-python-software-using-miniconda",
    "title": "Installing Python Software",
    "section": "How to install Python software using Miniconda",
    "text": "How to install Python software using Miniconda\n\n1. Open a terminal with Miniconda activated\n\nIn windows, after installing Miniconda, open the “Anaconda Powershell Prompt (miniconda3)”.\nOn a Macbook, if you have followed all the instructions, (close and re-)open the terminal. It should now display “(base)” in front of your username (see screenshot from my terminal above).\n\n\n\n2. Install python and libraries.\nPython will be automatically installed in step 3. However, libraries often need to be installed separately. You can do this at any point you like, whenever it turns out you need a library that you haven’t installed yet.\nThis can be done easily using miniconda. To install something using Conda, simply open the terminal and type the following command in the terminal, and press enter.\nconda install XXXX\nWhere XXXX is the library you want.\nYou sometimes need to specifiy on which online “database” (“channel”) software is found. You also need to know the very specific name of the package.\nTo address both these questions, it is easiest to just google for “conda install XXX”, where XXX is the package you want. For example, to install pandas, after Googling, we end up at this page, and see the installation command is:\nconda install anaconda::pandas\nThe part anaconda:: specifies further where to find pandas installation files.\nSimply copy that code into your terminal, and press enter. You will see something like this:\n\n\n\nA 2nd screenshot of my terminal, using conda.\n\n\nType a “y” and press enter to proceed. This means all the listed libraries will be installed. They are needed because in this case Pandas requires these other libraries to be installed in order to run. This is usually the case.\n\n\n3. Choose the editor software you want and install it\nTo work with Python, there are multiple options for software to write scripts and run them using Python. I’ve listed them below.\n\n\n\n\n\n\n\nSoftware\nDescription\n\n\n\n\nJupyter \n(+) Write code and text in blocks, and see the output directly below in one document. (-) Not suitable for large amounts of code (-) Adds extra layer complexity.\n\n\nSpyder\n(+) Easily write simply .py files, inspect parameters. (+) Rstudio or MATLAB like IDE. (-) Lacks some modern features. (-) No co-pilot integration.\n\n\nVisual Studio Code\n(+) ±Industry standard, has most advanced features. (+) Understands different conda environments. (+) Plugins for many features. (-) Very counter-intuitive to use. Makes it difficult to use.\n\n\nPyCharm\n(We don’t have much experience with this, but it’s also good and often used.)\n\n\n\nFor the workshop, we used Jupyter notebooks. We also recommend Spyder.\nThe installation of both of these can be done through Conda. See below.\n\nInstalling Jupyter using Conda\nJupyter can be installed using Conda. (On the official website they say use something called “pip” (link), but we recommend conda.)\nInstall jupyter similar as to how you installed other python libraries (see section 2 above):\nconda install conda-forge::jupyterlab\n(Source.). Copy and paste this into a terminal which has conda activated, and press enter to run. (See above.)\nTo run Jupyter, simply type the following command in your terminal and press enter:\njupyter lab\nThis will start Jupyter running on your local computer.\n\n\nInstalling spyder using Conda\nSpyder can be installed using Conda.\nInstall Spyder similar as to how you installed other python libraries (see section 2 above). It is also described in detail at: https://docs.spyder-ide.org/current/installation.html\nPaste the following command in your terminal (and hit enter) for an extended installation:\nconda install -c conda-forge spyder numpy scipy pandas matplotlib sympy cython\nPaste the following command in your terminal (and hit enter) for an minimal installation:\nconda install -c conda-forge spyder\n(Note: the official installation instructions use “environments”, if you use the official instructions, read carefully about using environments.)\nTo run, simply open a terminal, and type (+ hit enter afterwards):\nspyder\n\n\nInstalling VS Code\nFollow the instructions at: https://code.visualstudio.com/\n\n\nInstalling PyCharm\nFollow the instructions at: https://www.jetbrains.com/pycharm/"
  },
  {
    "objectID": "posts/installing_conda_python.html#a-final-note-about-this-blog",
    "href": "posts/installing_conda_python.html#a-final-note-about-this-blog",
    "title": "Installing Python Software",
    "section": "A final note about this blog",
    "text": "A final note about this blog\nThese installation instructions are intended for relative novices to using Python. For intermediates and experts, we recommend to use different conda environments when working with Python. However, for novices, we think it’s better to do everything in the base environment.\nWe do recommend people reading up on how to use conda environments. And while I’m at it, I also recommend people reading up on version tracking using Git and Github. But that’s another topic."
  },
  {
    "objectID": "posts/plotting_galleries.html",
    "href": "posts/plotting_galleries.html",
    "title": "Example plots for R and Python",
    "section": "",
    "text": "There are many resources online where you can find examples of nice plots. You can use google to find them (e.g. search for “python plotting gallery”), or take a look at the examples below.\n\n\n\nhttps://matplotlib.org/stable/gallery/, Matplotlib official gallery\nhttps://seaborn.pydata.org/examples/index.html, official Seaborn package plot gallery\nhttps://python-graph-gallery.com/, a collection of Python plotting examples (click on the pictures to see code).\n\n\n\n\n\nhttps://r-graph-gallery.com/, a collection of R plotting examples (click on the pictures to see the source code)\nhttps://exts.ggplot2.tidyverse.org/gallery/, official database of ggplot2 extensions"
  },
  {
    "objectID": "posts/plotting_galleries.html#example-plotting-code",
    "href": "posts/plotting_galleries.html#example-plotting-code",
    "title": "Example plots for R and Python",
    "section": "",
    "text": "There are many resources online where you can find examples of nice plots. You can use google to find them (e.g. search for “python plotting gallery”), or take a look at the examples below.\n\n\n\nhttps://matplotlib.org/stable/gallery/, Matplotlib official gallery\nhttps://seaborn.pydata.org/examples/index.html, official Seaborn package plot gallery\nhttps://python-graph-gallery.com/, a collection of Python plotting examples (click on the pictures to see code).\n\n\n\n\n\nhttps://r-graph-gallery.com/, a collection of R plotting examples (click on the pictures to see the source code)\nhttps://exts.ggplot2.tidyverse.org/gallery/, official database of ggplot2 extensions"
  },
  {
    "objectID": "workshop-materials/py-intro-adaptedlessons/lesson-08-dataframes-2.html",
    "href": "workshop-materials/py-intro-adaptedlessons/lesson-08-dataframes-2.html",
    "title": "Lesson 8, Pandas dataframes",
    "section": "",
    "text": "Copyright information\n\n\n\nThese are notes based on the Carpentries materials (licenced under the CC BY 4.0). These notes are intended to present the materials related to Dataframes (part2). The contents of the lessons were modified by Martijn Wehrens (bioDSC).\n\n\n\nLesson 8, Pandas dataframes\n# Make sure everyone has\n    # Gapminder in ./data folder\n        # https://swcarpentry.github.io/python-novice-gapminder/ (python-novice-gapminder-data.zip)\n    # Kohela in ./data folder\n        # https://www.biodsc.nl/workshop-materials/\n# Pandas dataframe\n    # Collection of \"series\"   \n        # one series is one column\n        # on which you can perform operations\n    # Built on \"numpy\", \n        # a library for working with arrays and mathematical operations\n        # series very similar to numpy.array\n    # Advantages\n        # Access to individual records\n        # Convenient ways combining information multiple dataframes\n# how to get dataframe\n\nimport pandas as pd\n\n# data = pd.read_csv('../data/gapminder_gdp_europe.csv', index_col='country')\ndata = pd.read_csv('/Users/m.wehrens/Data_UVA/2024_teaching/2025-03-gapminder/data/gapminder_gdp_europe.csv', \\\n    index_col='country')\n    \nprint(data.head())    \n# TWO METHODS TO ACCESS DATA\n    # iloc\n    # loc\n\n# \"iloc\" method\n# numerical indexing of rows and columns\n    # remember, it's a 2D table\n    # indexing similar as accessing chars in string\n\nprint(data.iloc[0, 0])\n\n# \"loc\" method\n# acess entries by their label\n\n# Other synthax\n    # \":\" means all columns, or all rows\n    \n# print(data.loc[\"Albania\", :])\n# print(data.loc[\"Albania\"])\n    # in loc, first entry is row\n    \n# other way around\n# print(data.loc[:, \"gdpPercap_1952\"])\n# More ways to access data\n\n# print(data[\"gdpPercap_1952\"])\n    # no \"loc\"!!!\n    # &lt;--&gt; series\n    \n# also works:\n# data.gdpPercap_1992\n    # not recommended, confusion with methods\n# slicing works on labels as well\nprint(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'])\n\n# can be used to perform operations on subsets\nprint(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'].max())\n    # note: applied per column\n# Selecting data\n\n# Use a subset of data to keep output readable.\nsubset = data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972']\nprint('Subset of data:\\n', subset)\n    # notice the escape \"\\\"; \\n is signifier for nextline\n\n# Which values were greater than 10000 ?\nprint('\\nWhere are values large?\\n', subset &gt; 10000)\n    # comparisons are done element-wise\n    \n# a frame full of \"booleans\" (true/false)\n    # sometimes called mask\n    # (MW: ±filter)\n\nmask = subset &gt; 10000\nprint(mask)\nprint(subset[mask])\n\n# returns value where True\n# returns NaN where False\n    # NaN = Not a number\n    # NaNs are ignored in operations\n        # e.g. min, max, etc\n\n# e.g.\nprint(subset[subset &gt; 10000].describe())\n# Group By: split-apply-combine\n\n# Group by: split-apply-combine\n    # note to self: see related/202503_RNAseq-data.py\n\n# Goal\n    # table in which rows match condition\n    # calculate something per condition\n    # use group_by\n    # Using data from Kohela et al. \n        # RNA-seq data (see powerpoint)\n\nimport pandas as pd\n\n# Import data\ndf_cells_kohela = pd.read_csv('/Users/m.wehrens/Data_UVA/example-datasets/kohela-et-al/kohela-et-al.csv', index_col=0)\ndf_cells_kohela2 = df_cells_kohela.T\ndf_cells_kohela2.head()\n\n# create new 'masks'\nepicardial_cells = df_cells_kohela2['WT1']&gt;3\nfibroblast_cells = df_cells_kohela2['COL2A1']&gt;30\nfat_cells = df_cells_kohela2['PPARG']&gt;2\n\n# Create a new column\ndf_cells_kohela2['Celltype'] = 'unknown'\n# Set values for the column\ndf_cells_kohela2.loc[epicardial_cells,'Celltype'] = 'epicardial'\ndf_cells_kohela2.loc[fibroblast_cells, 'Celltype'] = 'fibroblast'\ndf_cells_kohela2.loc[fat_cells, 'Celltype'] = 'fat'\n\n# OPTIONAL: give an overview of the frequencies of 'Celltype' values\ndf_cells_kohela2['Celltype'].value_counts()\n\n# now use group_by to calculate gene expression median values per group\n    # explain:\n        # creates grouped object (groupby object)\n        # now any aggregate function \n            # e.g. aggregate('XX'), max(), min(), etc\n            # applied per group\n            # will paste together the dataframe\ndf_cells_kohela2_groupedType = df_cells_kohela2.groupby('Celltype')\n\n# Now calculate mean expression\ndf_results = df_cells_kohela2_groupedType.mean() \nprint(df_results.head())\n\nprint(df_cells_kohela2.columns[0])\nprint(df_cells_kohela2.columns[-2])\n\ndf_means = df_cells_kohela2.loc[:,'A1BG':'ZZZ3'].mean()\n\n# And normalize\ndf_results_normalized = df_results/df_means\n\n# \"Check\" their TFAP2A claim\nprint(df_results_normalized.loc[:,'TFAP2A'])\n\n# Check some other highly differentially expressed genes\nprint(df_results_normalized.loc[:,(df_results_normalized&gt;10).any()])"
  },
  {
    "objectID": "workshop-materials/py-intro/08-dataframes-2.html",
    "href": "workshop-materials/py-intro/08-dataframes-2.html",
    "title": "Exercises Lesson 8, Dataframes (2)",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\n\nExercises Lesson 8, Dataframes (2)\n\nSelecting individual values\n\n\n\n\n\n\nQuestion\n\n\n\nImport GDP data for Europe:\nimport pandas as pd\ndata_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\nFind the per capita GDP of Serbia in 2007.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe selection can be done by using the labels for both the row (“Serbia”) and the column (“gdpPercap_2007”):\nprint(data_europe.loc['Serbia', 'gdpPercap_2007'])\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nExtent of slicing\n\n\n\n\n\n\nQuestion\n\n\n\n\nDo the two statements below produce the same output?\nBased on this, what rule governs what is included (or not) in numerical slices and named slices in Pandas?\n\nprint(data_europe.iloc[0:2, 0:2])\nprint(data_europe.loc['Albania':'Belgium', 'gdpPercap_1952':'gdpPercap_1962'])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNo, they do not produce the same output! The output of the first statement is:\n        gdpPercap_1952  gdpPercap_1957\ncountry\nAlbania     1601.056136     1942.284244\nAustria     6137.076492     8842.598030\nThe second statement gives:\n        gdpPercap_1952  gdpPercap_1957  gdpPercap_1962\ncountry\nAlbania     1601.056136     1942.284244     2312.888958\nAustria     6137.076492     8842.598030    10750.721110\nBelgium     8343.105127     9714.960623    10991.206760\nClearly, the second statement produces an additional column and an additional row compared to the first statement. What conclusion can we draw? We see that a numerical slice, 0:2, omits the final index (i.e. index 2) in the range provided, while a named slice, ‘gdpPercap_1952’:‘gdpPercap_1962’, includes the final element.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nReconstructing Data\n\n\n\n\n\n\nQuestion\n\n\n\nExplain what each line in the following short program does: what is in first, second, etc.?\nfirst = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsecond = first[first['continent'] == 'Americas']\nthird = second.drop('Puerto Rico')\nfourth = third.drop('continent', axis = 1)\nfourth.to_csv('result.csv')\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s go through this piece of code line by line.\nfirst = pd.read_csv('data/gapminder_all.csv', index_col='country')\nThis line loads the dataset containing the GDP data from all countries into a dataframe called first. The index_col=‘country’ parameter selects which column to use as the row labels in the dataframe.\nsecond = first[first['continent'] == 'Americas']\nThis line makes a selection: only those rows of first for which the ‘continent’ column matches ‘Americas’ are extracted. Notice how the Boolean expression inside the brackets, first[‘continent’] == ‘Americas’, is used to select only those rows where the expression is true. Try printing this expression! Can you print also its individual True/False elements? (hint: first assign the expression to a variable)\nthird = second.drop('Puerto Rico')\nAs the syntax suggests, this line drops the row from second where the label is ‘Puerto Rico’. The resulting dataframe third has one row less than the original dataframe second.\nfourth = third.drop('continent', axis = 1)\nAgain we apply the drop function, but in this case we are dropping not a row but a whole column. To accomplish this, we need to specify also the axis parameter (we want to drop the second column which has index 1).\nfourth.to_csv('result.csv')\nThe final step is to write the data that we have been working on to a csv file. Pandas makes this easy with the to_csv() function. The only required argument to the function is the filename. Note that the file will be written in the directory from which you started the Jupyter or Python session.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nSelecting Indices\n\n\n\n\n\n\nQuestion\n\n\n\nExplain in simple terms what idxmin() and idxmax() do in the short program below. When would you use these methods?\ndata = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\nprint(data.idxmin())\nprint(data.idxmax())\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor each column in data, idxmin will return the index value corresponding to each column’s minimum; idxmax will do accordingly the same for each column’s maximum value.\nYou can use these functions whenever you want to get the row index of the minimum/maximum value and not the actual minimum/maximum value.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nPractice with Selection\n\n\n\n\n\n\nQuestion\n\n\n\nAssume Pandas has been imported and the Gapminder GDP data for Europe has been loaded. Write an expression to select each of the following:\n\nGDP per capita for all countries in 1982.\nGDP per capita for Denmark for all years.\nGDP per capita for all countries for years after 1985.\nGDP per capita for each country in 2007 as a multiple of GDP per capita for that country in 1952.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1:\ndata['gdpPercap_1982']\n2:\ndata.loc['Denmark',:]\n3:\ndata.loc[:,'gdpPercap_1985':]\nPandas is smart enough to recognize the number at the end of the column label and does not give you an error, although no column named gdpPercap_1985 actually exists. This is useful if new columns are added to the CSV file later.\n4:\ndata['gdpPercap_2007']/data['gdpPercap_1952']\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nEpicardial cells\nDisclaimer: we are analyzing single cell data in this exercise. Within the constraints of this introductory course, we don’t use proper statistic or methodological approaches to analyze the data. These exercises are meant to teach you Python concepts, but also give you a flavor of biological data analysis.\n\n\n\n\n\n\nTo load the data from the kohela study Kohela et al. (GSE149331) we can also download it directly from the bioDSC website, and immediately transpose it in one line:\n\n\n\n# Import kohela data; note the .T at the end, which immediately transposes!\ndf_cells_kohela2 = \\\n    pd.read_csv(\"https://biodsc.nl/static/data/kohela-et-al.csv\",header=0,index_col=0).T\n\n\n\n\n\n\n\n\nQuestion A\n\n\n\nIn the RNA-seq data, we can create another column that reflects the condition of the cells, WT or mutant. Fill in the blanks to achieve this:\ndf_cells_kohela2['Condition'] = ____\n\ndf_cells_kohela2.loc[df_cells_kohela2.index.str.contains('WT_'), 'Condition'] = ____\ndf_cells_kohela2.loc[df_cells_kohela2.index._______, _________] = ______\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndf_cells_kohela2['Condition'] = 'unknown'\ndf_cells_kohela2.loc[df_cells_kohela2.index.str.contains('WT_'), 'Condition'] = 'WT'\ndf_cells_kohela2.loc[df_cells_kohela2.index.str.contains('mutant_'), 'Condition'] = 'mutant'\nYou can inspect the values in the new column to check if the contents are as expected:\nprint(df_cells_kohela2['Condition'])\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nWhat is the difference between str.contains() and str.match()?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee documentation for str.contains and str.match. str.contains tests whether the search string is contained in each string (at any location), whilst str.match determines if each string starts with a match.\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nNow calculate the mean value of TFAP2A expression in WT cells vs. mutant cells. Does there appear to be more TFAP2A expression in the mutant cells?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe first select TFAP2A gene expression levels, then group by condition, and then calculate the mean:\ndf_cells_kohela2.loc[:,['TFAP2A','Condition']].groupby('Condition').mean()\nThis gives us:\n             TFAP2A\nCondition          \nWT         1.535655\nmutant     9.601550\nSo this indeed appears to be the case.\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\n\n\nAdditional exercises\n\n\nMany ways of access\n\n\n\n\n\n\nQuestion\n\n\n\nThere are at least two ways of accessing a value or slice of a DataFrame: by name or index. However, there are many others. For example, a single column or row can be accessed either as a DataFrame or a Series object.\nSuggest different ways of doing the following operations on a DataFrame:\n\nAccess a single column\nAccess a single row\nAccess an individual DataFrame element\nAccess several columns\nAccess several rows\nAccess a subset of specific rows and columns\nAccess a subset of row and column ranges\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nAccess a single column:\n\n# by name\ndata[\"col_name\"]   # as a Series\ndata[[\"col_name\"]] # as a DataFrame\n\n# by name using .loc\ndata.T.loc[\"col_name\"]  # as a Series\ndata.T.loc[[\"col_name\"]].T  # as a DataFrame\n\n# Dot notation (Series)\ndata.col_name\n\n# by index (iloc)\ndata.iloc[:, col_index]   # as a Series\ndata.iloc[:, [col_index]] # as a DataFrame\n\n# using a mask\ndata.T[data.T.index == \"col_name\"].T\n\nAccess a single row:\n\n# by name using .loc\ndata.loc[\"row_name\"] # as a Series\ndata.loc[[\"row_name\"]] # as a DataFrame\n\n# by name\ndata.T[\"row_name\"] # as a Series\ndata.T[[\"row_name\"]].T # as a DataFrame\n\n# by index\ndata.iloc[row_index]   # as a Series\ndata.iloc[[row_index]]   # as a DataFrame\n\n# using mask\ndata[data.index == \"row_name\"]\n\nAccess an individual DataFrame element:\n\n# by column/row names\ndata[\"column_name\"][\"row_name\"]         # as a Series\n\ndata[[\"col_name\"]].loc[\"row_name\"]  # as a Series\ndata[[\"col_name\"]].loc[[\"row_name\"]]  # as a DataFrame\n\ndata.loc[\"row_name\"][\"col_name\"]  # as a value\ndata.loc[[\"row_name\"]][\"col_name\"]  # as a Series\ndata.loc[[\"row_name\"]][[\"col_name\"]]  # as a DataFrame\n\ndata.loc[\"row_name\", \"col_name\"]  # as a value\ndata.loc[[\"row_name\"], \"col_name\"]  # as a Series. Preserves index. Column name is moved to `.name`.\ndata.loc[\"row_name\", [\"col_name\"]]  # as a Series. Index is moved to `.name.` Sets index to column name.\ndata.loc[[\"row_name\"], [\"col_name\"]]  # as a DataFrame (preserves original index and column name)\n\n# by column/row names: Dot notation\ndata.col_name.row_name\n\n# by column/row indices\ndata.iloc[row_index, col_index] # as a value\ndata.iloc[[row_index], col_index] # as a Series. Preserves index. Column name is moved to `.name`\ndata.iloc[row_index, [col_index]] # as a Series. Index is moved to `.name.` Sets index to column name.\ndata.iloc[[row_index], [col_index]] # as a DataFrame (preserves original index and column name)\n\n# column name + row index\ndata[\"col_name\"][row_index]\ndata.col_name[row_index]\ndata[\"col_name\"].iloc[row_index]\n\n# column index + row name\ndata.iloc[:, [col_index]].loc[\"row_name\"]  # as a Series\ndata.iloc[:, [col_index]].loc[[\"row_name\"]]  # as a DataFrame\n\n# using masks\ndata[data.index == \"row_name\"].T[data.T.index == \"col_name\"].T\n\nAccess several columns:\n\n# by name\ndata[[\"col1\", \"col2\", \"col3\"]]\ndata.loc[:, [\"col1\", \"col2\", \"col3\"]]\n\n# by index\ndata.iloc[:, [col1_index, col2_index, col3_index]]\n\nAccess several rows\n\n# by name\ndata.loc[[\"row1\", \"row2\", \"row3\"]]\n\n# by index\ndata.iloc[[row1_index, row2_index, row3_index]]\n\nAccess a subset of specific rows and columns\n\n# by names\ndata.loc[[\"row1\", \"row2\", \"row3\"], [\"col1\", \"col2\", \"col3\"]]\n\n# by indices\ndata.iloc[[row1_index, row2_index, row3_index], [col1_index, col2_index, col3_index]]\n\n# column names + row indices\ndata[[\"col1\", \"col2\", \"col3\"]].iloc[[row1_index, row2_index, row3_index]]\n\n# column indices + row names\ndata.iloc[:, [col1_index, col2_index, col3_index]].loc[[\"row1\", \"row2\", \"row3\"]]\n\nAccess a subset of row and column ranges\n\n# by name\ndata.loc[\"row1\":\"row2\", \"col1\":\"col2\"]\n\n# by index\ndata.iloc[row1_index:row2_index, col1_index:col2_index]\n\n# column names + row indices\ndata.loc[:, \"col1_name\":\"col2_name\"].iloc[row1_index:row2_index]\n\n# column indices + row names\ndata.iloc[:, col1_index:col2_index].loc[\"row1\":\"row2\"]\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nAnalyzing GDPs\n\n\n\n\n\n\nQuestion\n\n\n\n\nBetween ’87 and ’92 the GDP of many countries took a hit. Are there any countries which had an increase between those two years? Which ones?\nCalculate the average GDP between all European countries per year.\nNormalize the dataframe by this trend.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nAre there any countries which had a positive increase between those two years? Which ones?\n\n# First load the data again\ndata_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\n\n# We can calculate the difference between those two years\nGDP_change_87_92 = \\\n    data_europe.loc[:, 'gdpPercap_1992'] - data_europe.loc[:, 'gdpPercap_1987']\n# And check which countries show a positive change\nprint(GDP_change_87_92.index[GDP_change_87_92&gt;0])\n\nCalculate the average GDP between all European countries per year.\n\ndata_europe_avgGDPperyear = data_europe.mean()\n\nNormalize the dataframe by this trend.\n\ndata_europe_relative = data_europe.div(data_europe_avgGDPperyear, axis=1)\nWe can now more easily see how countries compared to other countries over the years. Greece, for example, starts out its economy with a GDP of 62% of the average European country, but ends up with a GDP that’s 110% of the average European country.\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nGene expression\n\n\n\n\n\n\nQuestion\n\n\n\n\nConvert the data below to a file you can import (e.g.: csv, tsv), import it to a pandas df, and determine the following:\n\nThe average CRP gene expression per condion.\nThe corresponding standard deviations.\nThe log2-fold change between WT, condition A, and condition B.\nDo the same for ACTA1.\nNormalize all gene expression levels to their average respective wild type levels.\n\n\ngene    expression  condition\nCRP 873 WT\nCRP 324 WT\nCRP 214 WT\nCRP 151 WT\nCRP 1220    A\nCRP 450 A\nCRP 300 A\nCRP 210 A\nCRP 800 B\nCRP 200 B\nCRP 200 B\nCRP 130 B\nACTA1   7457    WT\nACTA1   2342    WT\nACTA1   8000    WT\nACTA1   9000    WT\nACTA1   6500    A\nACTA1   2200    A\nACTA1   7500    A\nACTA1   8000    A\nACTA1   1000    B\nACTA1   1123    B\nACTA1   3211    B\nACTA1   1231    B\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nConverting data to .csv and load the file:\nThis exercise is meant to illustrate that it can sometimes be tedious to handle data. Many data files are however plain text, and you can open them to see what’s going on. You may have discovered that blindly copying this list of data into a .txt file gives you a file where the contents are separated by varying number of spaces! One way to convert the exercise data to a more convenient format, could be:\n\nOpen a text editor like Notepad (Windows), TextEdit (Mac), or your own favorite.\nCopy the text to the text editor.\nUse search and replace to replace double spaces by single spaces.\nRepeat this until you only have single spaces.\nNow replace spaces by commas.\nYou now have a comma separated file, save it e.g. as gene_expression.csv.\n\nSee also this example gene_expression.csv.\nTo load the file, use:\ndf_gene_expression = \\\n    pd.read_csv('gene_expression.csv')\nCalculating average CRP gene expression per condion:\nThis can be done in multiple ways. Here’s an elegant solution that gives information about the std and mean in one go:\ndf_gene_expression.groupby(['gene', 'condition']).describe()\nAlternatively, we can do this using group_by, the mean and std function. We can use the column names directly as argument for the group by, then apply mean, then apply “reset_index()” to convert the multi-index to columns.\ndf_gene_expression_avg = df_gene_expression.groupby(by = ['gene', 'condition']).mean().reset_index()\n# CRP expression\nprint(df_gene_expression_avg.loc[df_gene_expression_avg['gene']=='CRP',:])\nCalculating the standard deviation:\nThe corresponding standard deviations can be calculated in a similar way:\ndf_gene_expression_std = df_gene_expression.groupby(by = ['gene', 'condition']).std().reset_index()\nprint(df_gene_expression_std.loc[df_gene_expression_std['gene']=='CRP',:])\nAn alternative strategy:\nA disadvantage of this strategy is that for large dataframes, you might be calculating many mean values that you’re not interested in in the end. You could also first select the gene of interest.\ndf_gene_expression_avg_CRP = df_gene_expression.loc[df_gene_expression['gene']=='CRP',:].groupby(by = ['gene', 'condition']).mean().reset_index()\ndf_gene_expression_std_CRP = df_gene_expression.loc[df_gene_expression['gene']=='CRP',:].groupby(by = ['gene', 'condition']).std().reset_index()\nprint(df_gene_expression_avg_CRP)\nprint(df_gene_expression_std_CRP)\nCalculating the log2-fold change between WT, condition A, and condition B:\nNote that for a real-world analysis, you might use specialized tools for this, which take into account more statistical considerations (e.g. like DESeq2). But to directly calculate the log2-fold change, we need to divide mean condition X by mean WT, calculate the ratios, and take the logarithm with base 2.\nThere are multiple ways to do this. The solution below exploits the fact that when dividing two dataframes, the indices are “aligned”. Meaning here that if the index of the numerator dataframe corresponds to ACTA1, it will be divided by a record from the denominator’s dataframe that’s also has an ACTA1 index, even if the numerator and denominator’s dataframes are of different length. This also applies to series.\n# Calculate averages again: group by, selecting the grouped expression series, calculate the mean\ngene_expression_avg = df_gene_expression.groupby(['gene','condition'])['expression'].mean()\n# Identify the WT records with a boolean array\nWT_records = gene_expression_avg.index.get_level_values(1)=='WT'\n# Select such that we divide a non-WT series by a WT series\n# We use \"reset index\" to set the index to gene names only, this allows for alignment\nratios = gene_expression_avg.loc[~WT_records].reset_index(level=1,drop=True)/gene_expression_avg.loc[WT_records].reset_index(level=1,drop=True)\n# The parameters above were series, let's convert back to dataframe\ndf_ratios = ratios.to_frame()\n# Now we need to restore the index to identify the conditions\ndf_ratios.index=gene_expression_avg[~WT_records].index\n# And we can take the log2\ndf_log2fc = np.log2(df_ratios)\n# search for \"expression\" column name and replace by \"log2fc\"\ndf_log2fc = df_log2fc.rename(columns={'expression':'log2fc'})\nprint(df_log2fc)\nDo the same for ACTA1.:\nSee above.\nNormalizing all gene expression levels to their average respective wild type levels:\nWe can use a same strategy as above.\n# First set the index to the gene column, then take the expression values\ngene_expression = df_gene_expression.set_index('gene')['expression']\n# Take the average information from above, and also set the index to the gene value,\n# such that we have average values indexed in the proper way to match the gene_expression.\nwt_avg_values   = gene_expression_avg.loc[WT_records].reset_index(level=1,drop=True)\n# We can now divide one by the other\ngene_expression_normalized = gene_expression/wt_avg_values\n# We can create a new dataframe with normalized data\ndf_gene_expression_norm = df_gene_expression.copy()\ndf_gene_expression_norm['expression_normalized'] = gene_expression_normalized.values\nAlternative strategy for the above two exercises:\nAn alternative could be to pivot the dataframe. This can also be used to calculate log2fc values.\n# Create a copy of the dataframe to avoid confusion with other exercises.\ndf_gene_expression_2 = df_gene_expression.copy()\n# To pivot, it is required to be able to match rows uniquely. Add an index to do this.\ndf_gene_expression_2['rows'] = df_gene_expression_2.groupby(['gene','condition']).cumcount()\n# Pivot df_gene_expression_2 such that the gene column is used to expand the 'expression', remove 'rows'\ndf_gene_expression_wide = df_gene_expression_2.pivot(index=['condition','rows'], columns='gene', values='expression').reset_index().drop(columns='rows')\n# Now calculate the averages\ndf_averages =  df_gene_expression_wide.groupby(['condition']).mean()\n# Now divide df_gene_expression_wide columns by df_averages\nvalues_WT = df_averages.T['WT'].values\n# We can use the dataframe's division method to divide all columns by the WT averages\ndf_gene_expression_norm2 = df_gene_expression_wide.loc[:,'ACTA1':'CRP'].div(values_WT, axis=1)\n# Add annotation condition\ndf_gene_expression_norm2['condition'] = df_gene_expression_wide['condition']\n# We now have the normalized dataframe\nprint(df_gene_expression_norm2)\n# Calculate L2FC:\ndf_log2fc_2 = df_gene_expression_norm2.groupby(['condition']).mean().apply(np.log2)\n# We now have the log2-fold changes:\nprint(df_log2fc_2)\nThat was a lot of code! Congratulations if you made it until the end :).\n\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nExploring methods using dir()\nPython includes a dir() function that can be used to display all of the available methods (functions) that are built into a data object. In Episode 4, we used some methods with a string. But we can see many more are available by using dir():\nmy_string = 'Hello world!'   # creation of a string object \ndir(my_string)\nThis command returns:\n['__add__',\n...\n'__subclasshook__',\n'capitalize',\n'casefold',\n'center',\n...\n'upper',\n'zfill']\nYou can use help() or Shift+Tab to get more information about what these methods do.\n\n\n\n\n\n\nQuestion\n\n\n\nAssume Pandas has been imported and the Gapminder GDP data for Europe has been loaded as data. Then, use dir() to find the function that prints out the median per-capita GDP across all European countries for each year that information is available.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAmong many choices, dir() lists the median() function as a possibility. Thus,\ndata.median()\n\n\n\n\n\nSource: Carpentries workshop materials."
  },
  {
    "objectID": "workshop-materials/py-intro/07-dataframes-1.html",
    "href": "workshop-materials/py-intro/07-dataframes-1.html",
    "title": "Exercises Lesson 7: Dataframes (1)",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 7: Dataframes (1)\n\n\nReading other data\n\n\n\n\n\n\nQuestion\n\n\n\nRead the data in gapminder_gdp_americas.csv (which should be in the same directory as gapminder_gdp_oceania.csv) into a variable called data_americas and display its summary statistics.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo read in a CSV, we use pd.read_csv and pass the filename ‘data/gapminder_gdp_americas.csv’ to it. We also once again pass the column name ‘country’ to the parameter index_col in order to index by country. The summary statistics can be displayed with the DataFrame.describe() method.\ndata_americas = pd.read_csv('data/gapminder_gdp_americas.csv', index_col='country')\ndata_americas.describe()\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nInspecting data\n\n\n\n\n\n\nQuestion\n\n\n\nAfter reading the data for the Americas, use help(data_americas.head) and help(data_americas.tail) to find out what DataFrame.head and DataFrame.tail do.\n\nWhat method call will display the first three rows of this data?\nWhat method call will display the last three columns of this data? (Hint: you may need to change your view of the data.)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWe can check out the first five rows of data_americas by executing data_americas.head() which lets us view the beginning of the DataFrame. We can specify the number of rows we wish to see by specifying the parameter n in our call to data_americas.head(). To view the first three rows, execute:\n\ndata_americas.head(n=3)\ncontinent  gdpPercap_1952  gdpPercap_1957  gdpPercap_1962  \\\ncountry\nArgentina  Americas     5911.315053     6856.856212     7133.166023\nBolivia    Americas     2677.326347     2127.686326     2180.972546\nBrazil     Americas     2108.944355     2487.365989     3336.585802\n\n          gdpPercap_1967  gdpPercap_1972  gdpPercap_1977  gdpPercap_1982  \\\ncountry\nArgentina     8052.953021     9443.038526    10079.026740     8997.897412\nBolivia       2586.886053     2980.331339     3548.097832     3156.510452\nBrazil        3429.864357     4985.711467     6660.118654     7030.835878\n\n           gdpPercap_1987  gdpPercap_1992  gdpPercap_1997  gdpPercap_2002  \\\ncountry\nArgentina     9139.671389     9308.418710    10967.281950     8797.640716\nBolivia       2753.691490     2961.699694     3326.143191     3413.262690\nBrazil        7807.095818     6950.283021     7957.980824     8131.212843\n\n           gdpPercap_2007\ncountry\nArgentina    12779.379640\nBolivia       3822.137084\nBrazil        9065.800825\n\nTo check out the last three rows of data_americas, we would use the command, americas.tail(n=3), analogous to head() used above. However, here we want to look at the last three columns so we need to change our view and then use tail(). To do so, we create a new DataFrame in which rows and columns are switched:\n\namericas_flipped = data_americas.T\nWe can then view the last three columns of americas by viewing the last three rows of americas_flipped:\namericas_flipped.tail(n=3)\ncountry        Argentina  Bolivia   Brazil   Canada    Chile Colombia  \\\ngdpPercap_1997   10967.3  3326.14  7957.98  28954.9  10118.1  6117.36\ngdpPercap_2002   8797.64  3413.26  8131.21    33329  10778.8  5755.26\ngdpPercap_2007   12779.4  3822.14   9065.8  36319.2  13171.6  7006.58\n\ncountry        Costa Rica     Cuba Dominican Republic  Ecuador    ...     \\\ngdpPercap_1997    6677.05  5431.99             3614.1  7429.46    ...\ngdpPercap_2002    7723.45  6340.65            4563.81  5773.04    ...\ngdpPercap_2007    9645.06   8948.1            6025.37  6873.26    ...\n\ncountry          Mexico Nicaragua   Panama Paraguay     Peru Puerto Rico  \\\ngdpPercap_1997   9767.3   2253.02  7113.69   4247.4  5838.35     16999.4\ngdpPercap_2002  10742.4   2474.55  7356.03  3783.67  5909.02     18855.6\ngdpPercap_2007  11977.6   2749.32  9809.19  4172.84  7408.91     19328.7\n\ncountry        Trinidad and Tobago United States  Uruguay Venezuela\ngdpPercap_1997             8792.57       35767.4  9230.24   10165.5\ngdpPercap_2002             11460.6       39097.1     7727   8605.05\ngdpPercap_2007             18008.5       42951.7  10611.5   11415.8\nThis shows the data that we want, but we may prefer to display three columns instead of three rows, so we can flip it back:\namericas_flipped.tail(n=3).T    \nNote: we could have done the above in a single line of code by ‘chaining’ the commands:\ndata_americas.T.tail(n=3).T\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nNavigating directories\n\n\n\n\n\n\nQuestion\n\n\n\nThe data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis:\nyour_home_directory\n+-- field_data/\n|   +-- microbes.csv\n+-- thesis/\n    +-- analysis.ipynb\nWhat value(s) should you pass to read_csv to read microbes.csv in analysis.ipynb?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe need to specify the path to the file of interest in the call to pd.read_csv. We first need to ‘jump’ out of the folder thesis using ‘../’ and then into the folder field_data using ‘field_data/’. Then we can specify the filename `microbes.csv. The result is as follows:\ndata_microbes = pd.read_csv('../field_data/microbes.csv')\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nWriting data\n\n\n\n\n\n\nQuestion\n\n\n\nAs well as the read_csv() function for reading data from a file, Pandas provides a to_csv() function to write dataframes to files. Applying what you’ve learned about reading from files, write one of your dataframes to a file called processed.csv. You can use help to get information on how to use to_csv().\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn order to write the DataFrame data_americas to a file called processed.csv, execute the following command:\ndata_americas.to_csv('processed.csv')\nFor help on read_csv or to_csv, you could execute, for example:\nhelp(data_americas.to_csv)\nhelp(pd.DataFrame.read_csv)\nNote that help(to_csv) or help(pd.to_csv) throws an error! This is due to the fact that to_csv is not a global Pandas function, but a member function of DataFrames. This means you can only call it on an instance of a DataFrame e.g., data_americas.to_csv or data_oceania.to_csv\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\n\n\n\n\nAdditional exercises\n\n\nWorking with data\n\n\n\n\n\n\nQuestion\n\n\n\n\nCreate some sample data\n\nCreate a plain text file on your computer, and give it the extension .csv.\nFind out what the comma-separated format looks like.\nUse your imagination to complete the following table and put it in the .csv file.\n\n\n\nreplicate\ncond1\ncond2\n\n\n\n\n\n10\n\n\n\n\n11\n\n\n\n\n10\n\n\n\n\n12\n\n\n\n\n13\n\n\n\n\n13\n\n\n\n\n\nNow try to read in that table in your python notebook.\nGet the following code to run on your dataframe (referred to as df below):\n\nfrom scipy.stats import ttest_ind\nt_stat, p_value = ttest_ind(df['cond1'], df['cond2'])\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nWhat does that code do?\nIf you haven’t already, let’s do some data massaging, and tweak the csv such that you get a significant p-val between the two conditions. (Don’t manipulate real data like this ;).)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nIn a csv file, the values are separated by a comma. As an example we compare cond1 vs cond2 for 6 replicate samples (R1 through R6). A possible data file (saved as mydata.csv) could look like:\n\nreplicate,cond1,cond2\nR1, 10, 1\nR2, 11, 2\nR3, 10, 2\nR4, 12, 3\nR5, 13, 2\nR6, 13, 1\n\nTo load this file in a dataframe, use\n\ndf = pd.read_csv('mydata.csv',header=0)\n\nRunning the code results in\n\nT-statistic: 15.076382102391054, P-value: 3.329703635867358e-08\n\nThe code calulates the t-statistic between the two conditions (with values for cond1 in column 2, accessed by df['cond1'], and values for cond2 in column 3 accessed by df['cond2']) and its associated p-value.\n\n\n\n\n\n\nSource: bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/04-functions.html",
    "href": "workshop-materials/py-intro/04-functions.html",
    "title": "Exercises Lesson 4: Functions",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 4: Functions\n\n\nOrder\n\n\n\n\n\n\nQuestion\n\n\n\n\nExplain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc.\nWhat is the final value of radiance?\n\nradiance = 1.0\nradiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nradiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))\nDivision and multiplication precede subtraction and addtition. The use of parentheses can change the evaluation order. In this example the order of operations:\n\n1.1 * radiance = 1.1\n1.1 - 0.5 = 0.6\nmin(radiance, 0.6) = 0.6\n2.0 + 0.6 = 2.6\nmax(2.1, 2.6) = 2.6\nAt the end, radiance = 2.6\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nLast string character\n\n\n\n\n\n\nQuestion\n\n\n\nIf Python starts counting from zero, and len returns the number of characters in a string, what index expression will get the last character in the string name? (Note: we will see a simpler way to do this in a later episode.)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nname[len(name) - 1]\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\nAdditional exercises\n\n\nWhy not?\n\n\n\n\n\n\nQuestion\n\n\n\nWhy is it that max and min do not return None when they are called with no arguments?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmax and min return TypeErrors in this case because the correct number of parameters was not supplied. If it just returned None, the error would be much harder to trace as it would likely be stored into a variable and used later in the program, only to likely throw a runtime error.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nSpot the difference\n\n\n\n\n\n\nQuestion\n\n\n\nPredict what each of the print statements in the program below will print. Does max(len(rich), poor) run or produce an error message? If it runs, does its result make any sense?\neasy_string = \"abc\"\nprint(max(easy_string))\nrich = \"gold\"\npoor = \"tin\"\nprint(max(rich, poor))\nprint(max(len(rich), len(poor)))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nprint(max(easy_string)), returns c, the largest of the characters (in Unicode values) in the string\nprint(max(rich, poor)), returns tin, the string that contains the largest character (in Unicode values)\nprint(max(len(rich), len(poor))), returns 4, the length of the string that contains the most characters (i.e. rich)\n\n\n\n\n\n\nSource: Carpentries workshop materials."
  },
  {
    "objectID": "workshop-materials/py-intro/plot.html",
    "href": "workshop-materials/py-intro/plot.html",
    "title": "Workshop image",
    "section": "",
    "text": "Workshop image\n\n\n\nGDP of European countries 1952-2007."
  },
  {
    "objectID": "workshop-materials/py-intro/13-conditionals.html",
    "href": "workshop-materials/py-intro/13-conditionals.html",
    "title": "Exercises Lesson 13: Conditionals",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 13: Conditionals\n\n\nTracing Execution\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this program print?\npressure = 71.9\nif pressure &gt; 50.0:\n    pressure = 25.0\nelif pressure &lt;= 50.0:\n    pressure = 0.0\nprint(pressure)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n25.0\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nTrimming Values\n\n\n\n\n\n\nQuestion\n\n\n\nFill in the blanks so that this program creates a new list containing zeroes where the original list’s values were negative and ones where the original list’s values were positive.\noriginal = [-1.5, 0.2, 0.4, 0.0, -1.3, 0.4]\nresult = ____\nfor value in original:\n    if ____:\n        result.append(0)\n    else:\n        ____\nprint(result)\nDesired output:\n[0, 1, 1, 1, 0, 1]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\noriginal = [-1.5, 0.2, 0.4, 0.0, -1.3, 0.4]\nresult = []\nfor value in original:\n    if value &lt; 0.0:\n        result.append(0)\n    else:\n        result.append(1)\nprint(result)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nProcessing Small Files\n\n\n\n\n\n\nQuestion\n\n\n\nModify this program so that it only processes files with fewer than 50 records.\nimport glob\nimport pandas as pd\nfor filename in glob.glob('data/*.csv'):\n    contents = pd.read_csv(filename)\n    ____:\n        print(filename, len(contents))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport glob\nimport pandas as pd\nfor filename in glob.glob('data/*.csv'):\n    contents = pd.read_csv(filename)\n    if len(contents) &lt; 50:\n        print(filename, len(contents))\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\nAdditional Exercises Lesson 13\n\n\nList comprehension\n\n\n\n\n\n\nQuestion A\n\n\n\nAdapt the following code to select only positive values:\nexample_list = [1,2,3,4,-5,1,34,6,-10, 39]\nexample_list_pos = [___ for item in example_list if ___]\nprint(example_list_pos)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nexample_list = [1,2,3,4,-5,1,34,6,-10, 39]\nexample_list_pos = [item for item in example_list if item&gt;0]\nprint(example_list_pos)\n[1, 2, 3, 4, 1, 34, 6, 39]\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nUse the same code, but:\n\nselect items between 30 and 40\nselect items &lt;0 or &gt;10\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nexample_list = [1,2,3,4,-5,1,34,6,-10, 39]\nexample_list_1 = [item for item in example_list if item&gt;30 and item&lt;40 ]\nprint(example_list_1)\n[34 39]\n\n\n\nexample_list = [1,2,3,4,-5,1,34,6,-10, 39]\nexample_list_2 = [item for item in example_list if item&lt;0 or item&gt;10 ]\nprint(example_list_2)\n[ -5  34 -10  39]\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nUse a np.array (see additional exercises Lesson 3) to do the same more elegantly.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nimport numpy as np\nexample_list = np.array([1,2,3,4,-5,1,34,6,-10, 39])\nprint(example_list[(example_list&gt;30) & (example_list&lt;40)])\n[34 39]\nNote that in this case we have to use the & operator because this is the logical and operation used in numpy.\n\n\n\nimport numpy as np\nexample_list = np.array([1,2,3,4,-5,1,34,6,-10, 39])\nprint(example_list[(example_list&lt;0) | (example_list&gt;10)])\n[ -5  34 -10  39]\n\nNote that in this case we have to use the | operator because this is the logical or operation in used in numpy.\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nEnumerate, zip\nThese exercises introduce two new concepts. You might need google.\n\n\n\n\n\n\nQuestion A\n\n\n\nWhat does the following code do? What is the meaning of the output?\nfor idx, item in enumerate([1,2,3,4,-5,1,34,6,-10]):\n    if item&gt;5:\n        print(idx)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe program prints the index when the value in the array is larger than 5, so in this case\n6\n7\nThe enumerate function ensures that in each loop iteration, idx gets the value corresponding to the list’s index (enumerating each item from 0 .. length of the list), and item gets the value of that list’s element.\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nModify the following code such that it will compare each item i in apples with each item i in pears, and tell you which one is heavier. You need to edit the code.\napples = [123, 436, 123, 654, 117, 193, 120]\npears  = [543, 163, 178, 165, 123, 187, 190]\n\nfor apple_weight, pear_weight in zip(apples, pears):\n    print('='*10)\n    print('weigth apple: ', apple_weight)\n    print('weigth pear:  ', pear_weight)\n    ___\n    print('the XXX is heavier')\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\napples = [123, 436, 123, 654, 117, 193, 120]\npears  = [543, 163, 178, 165, 123, 187, 190]\n\nfor apple_weight, pear_weight in zip(apples, pears):\n    print('='*10)\n    print('weigth apple: ', apple_weight)\n    print('weigth pear:  ', pear_weight)\n    XXX = 'apple' if apple_weight&gt;pear_weight else 'pear'\n    print(f\"the {XXX} is heavier\")\n==========\nweigth apple:  123\nweigth pear:   543\nthe pear is heavier\n==========\nweigth apple:  436\nweigth pear:   163\nthe apple is heavier\n==========\nweigth apple:  123\nweigth pear:   178\nthe pear is heavier\n==========\nweigth apple:  654\nweigth pear:   165\nthe apple is heavier\n==========\nweigth apple:  117\nweigth pear:   123\nthe pear is heavier\n==========\nweigth apple:  193\nweigth pear:   187\nthe apple is heavier\n==========\nweigth apple:  120\nweigth pear:   190\nthe pear is heavier\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nInitializing\n\n\n\n\n\n\nQuestion\n\n\n\nModify this program so that it finds the largest and smallest values in the list no matter what the range of values originally is.\nvalues = [...some test data...]\nsmallest, largest = None, None\nfor v in values:\n    if ____:\n        smallest, largest = v, v\n    ____:\n        smallest = min(____, v)\n        largest = max(____, v)\nprint(smallest, largest)\n\nWhat are the advantages and disadvantages of using this method to find the range of the data?\n\nThe loop could also look as follows:\nvalues = [...some test data...]\nsmallest, largest = None, None\nfor v in values:\n    smallest = min(____, v)\n    largest = max(____, v)\n\nWhy wouldn’t this work and is the if statement needed.\nHow can we test whether we are in the first iteration?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nvalues = [-2,1,65,78,-54,-24,100]\nfor idx,v in enumerate(values):\n    if idx == 0:\n        smallest = v\n        largest = v\n    else:\n        smallest = min(smallest, v)\n        largest = max(largest, v)\nprint(smallest, largest)\n\nIf you wrote == None instead of is None, that works too, but Python programmers always write is None because of the special way None works in the language.\nThe min and max function do not accept None as an argument and we need to test for this sitation.\nAn iterator counter should be added to the for loop.\n\n\nvalues = [-2,1,65,78,-54,-24,100]\nfor idx,v in enumerate(values):\n    if idx == 0:\n        smallest = v\n        largets = v\n    else:\n        smallest = min(smallest, v)\n        largest = max(largest, v)\nprint(smallest, largest)\nThis allows the code to be implemented in a slightly different way, but this is less robust in case there is a None value in the values array.\nIn some other cases (e.g. when every loop iteration writes something to a file and the file needs to be initialized in the first loop iteration) this type of looping can be convenient.\nNote that these exercises are merely illustrations of looping, the simplest solution would be:\nvalues = [-2,1,65,78,-54,-24,100]\nsmallest = min(values)\nlargest = max(values)\nprint(smallest, largest)\nA more efficient loop would be:\nvalues = [-2,1,65,78,-54,-24,100]\nsmallest, largest = None, None\nfor v in values:\n    if smallest is None or v &lt; smallest:\n        smallest = v\n    if largest is None or v &gt; largest:\n        largest = v\nprint(smallest, largest)\n\n\n\n\n\nSource: Carpentries workshop materials, adapted by the bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/06-libraries.html",
    "href": "workshop-materials/py-intro/06-libraries.html",
    "title": "Exercises Lesson 6: Libraries",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 6: Libraries\n\n\nExplore\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat function from the math module can you use to calculate a square root without using the sqrt function?\nSince the library contains this function, why does sqrt exist?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nUsing help(math) we see that we’ve got pow(x,y) in addition to sqrt(x), so we could use pow(x, 0.5) to find a square root.\nThe sqrt(x) function is arguably more readable than pow(x, 0.5) when implementing equations. Readability is a cornerstone of good programming, so it makes sense to provide a special function for this specific common case.\n\nAlso, the design of Python’s math library has its origin in the C standard, which includes both sqrt(x) and pow(x,y), so a little bit of the history of programming is showing in Python’s function names.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nFind the right module\nYou want to select a random character from a string, such as the following DNA sequence:\nbases = 'ACTTGCTTGAC'\n\n\n\n\n\n\nQuestion\n\n\n\nWhich standard library module could help you? Which function would you select from that module? Are there alternatives? Try to write a program that uses the function.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe random module seems like it could help, and there are multiple ways to reach the same goal.\nThe string has 11 characters, each having a positional index from 0 to 10. You could use random.randrange or random.randint functions to get a random integer between 0 and 10, and then select the bases character at that index:\nfrom random import randrange\nrandom_index = randrange(len(bases))\nprint(bases[random_index])\nor more compactly:\nfrom random import randrange\nprint(bases[randrange(len(bases))])\nPerhaps you found the random.sample function? It allows for slightly less typing but might be a bit harder to understand just by reading:\nfrom random import sample\nprint(sample(bases, 1)[0])\nNote that this function returns a list of values. We will learn about lists in episode 11.\nThe simplest and shortest solution is the random.choice function that does exactly what we want:\nfrom random import choice\nprint(choice(bases))\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nHelp!\n\n\n\n\n\n\nQuestion\n\n\n\nWhen a colleague of yours types help(math), Python reports an error:\nNameError: name 'math' is not defined\nWhat has your colleague forgotten to do?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour colleague forgot to import the math module (import math)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nImporting with aliases\n\n\n\n\n\n\nQuestion\n\n\n\n\nFill in the blanks so that the program below prints 90.0.\nRewrite the program so that it uses import without as.\nWhich form do you find easier to read?\n\nimport math as m\nangle = ____.degrees(____.pi / 2)\nprint(____)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nFilling in the blanks\n\nimport math as m\nangle = m.degrees(m.pi / 2)\nprint(angle)\n\ncan be written as:\n\nimport math\nangle = math.degrees(math.pi / 2)\nprint(angle)\n\nSince you just wrote the code and are familiar with it, you might actually find the first version easier to read. But when trying to read a huge piece of code written by someone else, or when getting back to your own huge piece of code after several months, non-abbreviated names are often easier, except where there are clear abbreviation conventions.\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nMultiple ways of importing\n\n\n\n\n\n\nQuestion\n\n\n\nMatch the following print statements with the appropriate library calls.\nPrint commands:\n\nprint(\"sin(pi/2) =\", sin(pi/2))\nprint(\"sin(pi/2) =\", m.sin(m.pi/2))\nprint(\"sin(pi/2) =\", math.sin(math.pi/2))\n\nLibrary calls:\n\nfrom math import sin, pi\nimport math\nimport math as m\nfrom math import *\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLibrary calls 1 and 4. In order to directly refer to sin and pi without the library name as prefix, you need to use the from … import … statement. Whereas library call 1 specifically imports the two functions sin and pi, library call 4 imports all functions in the math module.\nLibrary call 3. Here sin and pi are referred to with a shortened library name m instead of math. Library call 3 does exactly that using the import … as … syntax - it creates an alias for math in the form of the shortened name m.\nLibrary call 2. Here sin and pi are referred to with the regular library name math, so the regular import … call suffices.\n\nNote: although library call 4 works, importing all names from a module using a wildcard import is not recommended as it makes it unclear which names from the module are used in the code. In general it is best to make your imports as specific as possible and to only import what your code uses. In library call 1, the import statement explicitly tells us that the sin function is imported from the math module, but library call 4 does not convey this information.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\nAdditional exercises\n\n\n“Jigsaw”: progamming example\n\n\n\n\n\n\nQuestion\n\n\n\nRearrange the following statements so that a random DNA base is printed and its index in the string. Not all statements may be needed. Feel free to use/add intermediate variables.\nbases=\"ACTTGCTTGAC\"\nimport math\nimport random\n___ = random.randrange(n_bases)\n___ = len(bases)\nprint(\"random base \", bases[___], \"base index\", ___)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport math \nimport random\nbases = \"ACTTGCTTGAC\" \nn_bases = len(bases)\nidx = random.randrange(n_bases)\nprint(\"random base\", bases[idx], \"base index\", idx)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nImporting specific items\n\n\n\n\n\n\nQuestion\n\n\n\n\nFill in the blanks so that the program below prints 90.0.\nDo you find this version easier to read than preceding ones?\nWhy wouldn’t programmers always use this form of import?\n\n____ math import ____, ____\nangle = degrees(pi / 2)\nprint(angle)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom math import degrees, pi\nangle = degrees(pi / 2)\nprint(angle)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nReading error messages\n\n\n\n\n\n\nQuestion\n\n\n\n\nRead the code below and try to identify what the errors are without running it.\nRun the code, and read the error message. What type of error is it?\n\nfrom math import log\nlog(0)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-1-d72e1d780bab&gt; in &lt;module&gt;\n      1 from math import log\n----&gt; 2 log(0)\n\nValueError: math domain error\n\nThe logarithm of x is only defined for x &gt; 0, so 0 is outside the domain of the function.\nYou get an error of type ValueError, indicating that the function received an inappropriate argument value. The additional message “math domain error” makes it clearer what the problem is.\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nWrite your own function\nWhen you have a notebook file, you can also create another file, with a .py extension, and write functions in that file. The .py file can be imported like a library, and the functions in the file can be used as if they came from a library.\n\n\n\n\n\n\nQuestion\n\n\n\nUsing the information below, try to\n\ncreate two files, one .ipynb (notebook) file and one .py (python plain text code) file.\nrename the myfunctionname functions in the .py file and use them in the notebook.\ncreate a third function, which returns C when you provide A and B, assuming A^2+B^2 = C^2, and use it in your notebook.\n\n\nUseful information:\n\nYou can also make .py files. Unlike notebooks, every text here is assumed to be python code.\nFor Jupyter notebooks:\n\nYou can make a .py file with file &gt; new &gt; python file.\n\nSave the file to myfilename.py (replacing myfilename with your own favorite name).\n\n\nFor Google colabs:\n\nTo create a .py file, right click in the file overview (where you also put the gapminder .csv files), and select ‘new file’. Then create a file ‘myfilename.py’ and double click to edit it.*\n\nYou can import your file in a python notebook using:\n\nimport myfilename where myfilename.py should exist and hold your code. You can also put your file in a different directory, but then you need to import it like import mydirectoryname.myfilename.\n\n\nYou can write a function using the following template:\ndef myfunctionname():\n    print(\"hello world\")\n    \ndef myfunctionname2(input1, input2):\n    print(\"input 1 = \", input1, ', input 2 = ', input2)\n\n\n\n\n\nSource: bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/16-functions.html",
    "href": "workshop-materials/py-intro/16-functions.html",
    "title": "Exercises Lesson 16: Functions",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 16: Functions\n\n\nIdentifying Syntax Errors\n\n\n\n\n\n\nQuestion\n\n\n\n\nRead the code below and try to identify what the errors are without running it.\nRun the code and read the error message. Is it a SyntaxError or an IndentationError?\nFix the error.\nRepeat steps 2 and 3 until you have fixed all the errors.\n\ndef another_function\n  print(\"Syntax errors are annoying.\")\n   print(\"But at least python tells us about them!\")\n  print(\"So they are usually not too hard to fix.\")\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndef another_function(): # the function definition was not finished\n  print(\"Syntax errors are annoying.\")\n  print(\"But at least Python tells us about them!\") # this line was not properly indented\n  print(\"So they are usually not too hard to fix.\")\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nDefinition and Use\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the following program print?\ndef report(pressure):\n    print('pressure is', pressure)\n\nprint('calling', report, 22.5)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ncalling &lt;function report at 0x7fd128ff1bf8&gt; 22.5\nA function call always needs parenthesis, otherwise you get memory address of the function object. So, if we wanted to call the function named report, and give it the value 22.5 to report on, we could have our function call as follows\nprint(\"calling\")\nreport(22.5)\nwhich outputs:\ncalling\npressure is 22.5\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nEncapsulation\n\n\n\n\n\n\nQuestion\n\n\n\nFill in the blanks to create a function that takes a single filename as an argument, loads the data in the file named by the argument, and returns the minimum value in that data.\nimport pandas as pd\n\ndef min_in_data(____):\n    data = ____\n    return ____\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport pandas as pd\n\ndef min_in_data(filename):\n    data = pd.read_csv(filename)\n    return data.min()\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\n\nAdditional Exercises\n\n\nOrder of Operations\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat’s wrong in this example?\n\nresult = print_time(11, 37, 59)\n\ndef print_time(hour, minute, second):\n   time_string = str(hour) + ':' + str(minute) + ':' + str(second)\n   print(time_string)\n\nAfter fixing the problem above, explain why running this example code:\n\nresult = print_time(11, 37, 59)\nprint('result of call is:', result)\ngives this output:\n11:37:59\nresult of call is: None\n\nWhy is the result of the call None?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe problem with the example is that the function print_time() is defined after the call to the function is made. Python doesn’t know how to resolve the name print_time since it hasn’t been defined yet and will raise a NameError e.g., NameError: name 'print_time' is not defined\nThe first line of output 11:37:59 is printed by the first line of code, result = print_time(11, 37, 59) that binds the value returned by invoking print_time to the variable result. The second line is from the second print call to print the contents of the result variable.\nprint_time() does not explicitly return a value, so it automatically returns None.\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nFind the First\n\n\n\n\n\n\nQuestion\n\n\n\nFill in the blanks to create a function that takes a list of numbers as an argument and returns the first negative value in the list. What does your function do if the list is empty? What if the list has no negative numbers?\ndef first_negative(values):\n    for v in ____:\n        if ____:\n            return ____\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndef first_negative(values):\n    for v in values:\n        if v &lt; 0:\n            return v\nIf an empty list or a list with all positive values is passed to this function, it returns None:\nmy_list = []\nprint(first_negative(my_list))\nNone\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nCalling by Name\n\n\n\n\n\n\nQuestion\n\n\n\nEarlier we saw this function:\ndef print_date(year, month, day):\n    joined = str(year) + '/' + str(month) + '/' + str(day)\n    print(joined)\nWe saw that we can call the function using named arguments, like this:\nprint_date(day=1, month=2, year=2003)\n\nWhat does print_date(day=1, month=2, year=2003) print?\nWhen have you seen a function call like this before?\nWhen and why is it useful to call functions this way?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n2003/2/1\nWe saw examples of using named arguments when working with the pandas library. For example, when reading in a dataset using data = pd.read_csv(‘data/gapminder_gdp_europe.csv’, index_col=‘country’), the last argument index_col is a named argument.\nUsing named arguments can make code more readable since one can see from the function call what name the different arguments have inside the function. It can also reduce the chances of passing arguments in the wrong order, since by using named arguments the order doesn’t matter.\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nPrimes\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a function that looks as follows:\ndef calculate_primes(N):\n    ...\nthat returns an array with prime numbers between 0 and N.\nHints:\n\nStart with writing a function is_number_prime(X), that checks whether X is a prime number.\nYou probably need the following ingredients for that function:\n\nHow to test if a number is divisible by any number?\nUse a for loop to test whether X can be divided by all numbers &lt;X.\nCould be convenient to make smart use of the return function.\n\n\ndef is_number_prime(X):\n    \n    # &lt;insert explanatory comment&gt;\n    for y in range(X):\n    \n        # &lt;insert explanatory comment&gt;\n        if ...:\n            return False\n    \n    # &lt;insert explanatory comment&gt;\n    return True\n\ndef calculate_primes(N):\n    ...\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndef is_number_prime(X):\n    '''\n    This functions tests whether X is a prime number,\n    and returns either False if not, or True if so.\n    '''\n    \n    # Prime numbers are not divisible by any number\n    # other than 1 and itself.\n    # So if X can be divided by any number between\n    # 1 and X, it is not a prime number.\n    # So we'll test for each number 2 .. (X-1)\n    # whether X can be divided by it.\n    for y in range(2, X):\n    \n        # We test this by checking whether there's \n        # a remainder after a division.\n        # (E.g. 7/3 has a remainder of 1 as \n        # 7 = 3 + 3 + 1, and so 7 is not divisible\n        # by 3.)\n        if X % y == 0:\n            # if there's no remainder,\n            # it could be divided, \n            # and it is not a prime number\n            # --&gt; return false\n            return False\n    \n    # if for all numbers tested a division\n    # wasn't possible, the number is a prime\n    # number --&gt; return true.\n    return True\n\ndef calculate_primes(N):\n    '''\n    Returns all prime numbers between\n    0 and N (not including N).\n    '''\n    \n    prime_list = []\n    \n    # Loop over 2..N, and test \n    # whether the number is prime\n    # and if so add it to the list.\n    # (Note that 1 is excluded since\n    # it's not prime anyways).\n    for X in range(2, N):\n        \n        # Add X to list if it's prime\n        if is_number_prime(X):\n            prime_list.append(X)\n    \n    return(prime_list)\n\n# Check whether it works            \nprimes_known_upto100 = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\nprimes_calculated_by_me = calculate_primes(100+1)\nif (primes_known_upto100 == primes_calculated_by_me):\n    print('Hurray it works!')\nelse:\n    print('Oh nooo.')\n    \n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nWrite your own function\nIf you still have a taste for exercises, and haven’t finished the exercise “Writing your own function” from the additional exercises in lesson 6, maybe check that one out!\n\n\n\nEven more exercises\nIf you even still have a taste for more exercises, check out exercises that the Carpentries drafted, at the section of their workshop materials regarding functions. (Continue exercises at: “Encapsulation of an If/Print Block”.)"
  },
  {
    "objectID": "workshop-materials/index.html",
    "href": "workshop-materials/index.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\n\n\nPython workshop exercises:\n\nLesson 2 - variables\nLesson 3 - data-types\nLesson 4 - built-in functions\nLesson 6 - libraries\nLesson 7 - dataframes part-1\nLesson 8 - dataframes part-2\nLesson 9 - plotting\nLesson 11 - lists\nLesson 12 - loops\nLesson 13 - conditionals\nLesson 14 - looping over data\nLesson 16 - writing functions\n\nCustomized workshop materials (currently only raw notes):\n\nLesson 8 - dataframes part 2\nLesson 9 - plotting\n\nFor the RNA-seq dataset from Kohela et al. (GSE149331), click this link.\nFor the example plot, see here: link."
  },
  {
    "objectID": "workshops/2025-working-on.html",
    "href": "workshops/2025-working-on.html",
    "title": "List of past and upcoming workshops",
    "section": "",
    "text": "Below a list of planned workshops:\n\nAn introduction to R (Jan 2025)\n\nBasics of R\nImporting, manipulating and analyzing data (mostly using dataframes).\nPlotting using ggplot.\n\nBring your own data (February 25th, 2025)\n\nBring your own dataset and make publication-quality figures.\n\nAn introduction to Python (March 12 & 14, 2025).\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib).\nUsing loops and conditionals, writing functions.\nGood practices in programming and software engineering.\n\nImage analysis with Python (planning: April/May 2025).\n\nBasics of working with images\nImage manipulation, cellular segmentation, and other often-used concepts.\n(..)\n\nAnalysis of RNA-sequencing data (planning: April/May 2025)\n\nRead quality control\nMapping reads to a reference genome\nDifferentially expressed gene analysis using DEseq2 in R.\n\n\nWe aim to give workshops based on demand. So if there’s enough people interested, workshops will be recurring."
  },
  {
    "objectID": "workshops/2025-working-on.html#should-i-learn-python-or-r",
    "href": "workshops/2025-working-on.html#should-i-learn-python-or-r",
    "title": "List of past and upcoming workshops",
    "section": "Should I learn Python or R?",
    "text": "Should I learn Python or R?\nSee the blog post about Python vs. R in case you’re wondering whether you should start learning: R, or Python."
  },
  {
    "objectID": "workshops/2025-introduction-python-3.html",
    "href": "workshops/2025-introduction-python-3.html",
    "title": "May 19, 21 and 26 (2025): Introduction to Python",
    "section": "",
    "text": "An introduction to Python\nThe bioDSC organizes an introductory workshop that will cover basic functionalities of Python.\nPython is a computer script language that has become a workhorse of data analysis throughout many fields and disciplines. It can process and analyze big data sets, perform image analysis, and create great visualizations.\nSee below for small overview of what types of tasks can be done using Python.\n\n\nWorkshop goals\nAfter this workshop, you’ll be familiar with Python basics. We’ll discuss and let you practice on:\n\n\nUsing Python (Jupyter notebooks, VS Studio code, Spyder)\n\nThroughout the workshop, we’ll use an online version of Jupyter notebook to avoid installation issues)\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib and Seaborn).\nUsing loops and conditionals.\nWriting functions.\nGood practices in programming and software engineering.\n\n\n\n\nWorkshop materials\n\nContents\nThis workshop will largely (&gt;80%) follow the Carpentries training material “Plotting and programming in Python”. We customized materials for lessons 8 and 9 (dataframes and plotting, see below) to include biological examples. We are working to publish these materials online too. The course does not address specific programming solutions for common biological challenges. Follow-up courses, e.g. about RNA-seq analysis and image analysis, will delve into more biological programming topics.\n\n\n\nDatasets\nThese materials use the “Gapminder” dataset (see [1] and [2]). Programming examples from this dataset will not relate to biological problems, but the examples offer good insights in how Python works. Additionally, we included custom materials based on single cell sequencing data from Kohela et al. (GSE149331), which will highlight Python functionalities through biological examples.\n\n\nChanges compared previous workshops\nThis workshop will be similar to previous Python workshops [1], [2]. We changed the scheduling (3 shorter timeslots instead of 2), and edited the materials to include more biologically relevant examples (and plotting with Seaborn).\n\n\nQuestions\nIf you have any questions regarding whether this course is relevant for you, please send us an email (info@biodsc.nl) or walk by our desks.\n\n\n\nRequirements\n\nThis workshop assumes no prior knowledge on Python or programming!\nYou have to bring your own laptop.\n\n\n\nWorkshop logistics\nThe course will be given by Misha Paauw, Martijn Wehrens and Frans van der Kloet from the bioDSC. There is room for 4-20 participants. You have to bring your own laptop, we’ll use an online version of Python, so there are no software installation requirements.\n\n\nWorkshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\nMay 19 (Mon)\n13:00 - 17:00\nSP L1.13*\nThe basics, episodes 1-6\n\n\nMay 21 (Wed)\n13:00 - 17:00\nSP L1.13*\nDataframes and plotting, episodes 7-9\n\n\nMay 26 (Mon)\n13:00 - 17:00\nSP L1.12*\nProgramming concepts, episodes 11-16\n\n\n\n*) These rooms are in the “Lab 42” building!\n\n\nSign up\nSign up using this link. Sign up deadline: 28th of April. First come, first serve.\n\n\n\n\nTypical use cases for Python\nPython is a scripting language, meaning that your write a file with commands for the computer to execute. This allows you to perform complicated tasks, such as:\n\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\n\nImage analysis\n\nPython has become a great open-source tool for image analysis\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images. You can:\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nExtract summary parameters relevant for your analysis.\nRelated libraries: scikit-image, SciPy, OpenCV, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nLibraries: E.g. Keras and PyTorch\n\n\nHigh-throughput data analysis and automation\n\nPython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nBioinformatics\n\nNot the tool for RNA-seq statistics and gene expression analysis, for which R is superior.\nCan be used to handle, manipulate, process, quantify raw sequence data or similar.\nPerform single cell analysis, e.g. using the SCANPY lirbary.\n\nMuch more ..\n\n\nSee also the post I wrote earlier to decide whether you should Python or R.\n\n\nIdentifier\nThis meeting’s identifier: 2025-05-19-UvA-bioDSC."
  },
  {
    "objectID": "workshops/2025-introduction-python.html",
    "href": "workshops/2025-introduction-python.html",
    "title": "March 12 and 14 (2025): Introduction to Python",
    "section": "",
    "text": "This workshop is full.\n\n\n\n12 people have registered, we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\nAn introduction to Python\nThe bioDSC organizes an introductory workshop that will cover basic functionalities of Python.\nPython is a computer script language that has become a workhorse of data analysis throughout many fields and disciplines. It can process and analyze big data sets, perform image analysis, and create great visualizations.\nSee below for small overview of what types of tasks can be done using Python.\n\n\nWorkshop goals\nAfter this workshop, you’ll be familiar with Python basics. We’ll discuss and let you practice on:\n\n\nUsing Python (Jupyter notebooks, VS Studio code, Spyder)\n\nThroughout the workshop, we’ll use an online version of Jupyter notebook to avoid installation issues)\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib).\nUsing loops and conditionals.\nWriting functions.\nGood practices in programming and software engineering.\n\n\nThis workshop will largely (&gt;95%) follow the Carpentries training material “Plotting and programming in Python”. All training material can be found at:\n\nhttps://swcarpentry.github.io/python-novice-gapminder/\n\nSee this workshop page for a summary of desired learning results. The materials we’ll use do not use examples specific to biology, but instead use the “Gapminder” dataset (see [1] and [2]). We think this offers thorough insights into how Python works. Follow-up courses, e.g. into image analysis, will be delve into more biological datasets.\nIf you have any questions regarding whether this course is relevant for you, please send us an email (info@biodsc.nl) or walk by our desks.\n\n\nRequirements\nThis workshop assumes no prior knowledge on Python or programming!\n\n\nWorkshop logistics\nThe course will be given by Martijn Wehrens and Misha Paauw from the bioDSC. There is room for 4-12 participants. You have to bring your own laptop, we’ll use an online version of Python, so there are no software installation requirements.\nAs mentioned, the contents of this workshops are almost completely based on: https://swcarpentry.github.io/python-novice-gapminder/\n\n\nWorkshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\n12 March\n12:30 - 17:30\nSP A1.06\nIntroduction to Python, episodes 1-9\n\n\n14 March\n12:30 - 17:30\nSP B0.209\nIntroduction to Python, episodes 11-18\n\n\n\n\n\nSign up\n\n\n\n\n\n\n\nThis workshop is full.\n\n\n\n12 people have registered, we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\n\n\nTypical use cases for Python\nPython is a scripting language, meaning that your write a file with commands for the computer to execute. This allows you to perform complicated tasks, such as:\n\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\n\nImage analysis\n\nPython has become a great open-source tool for image analysis\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images. You can:\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nExtract summary parameters relevant for your analysis.\nRelated libraries: scikit-image, SciPy, OpenCV, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nLibraries: E.g. Keras and PyTorch\n\n\nHigh-throughput data analysis and automation\n\nPython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nBioinformatics\n\nCan be used to handle, manipulate, process, quantify raw sequence data or similar.\nPerform single cell analysis, e.g. using the SCANPY lirbary.\nNot the tool for RNA-seq statistics and gene expression analysis, for which R is superior.\n\nMuch more ..\n\n\nSee also the post I wrote earlier to decide whether you should Python or R."
  },
  {
    "objectID": "workshops/2025-introduction-python-2.html",
    "href": "workshops/2025-introduction-python-2.html",
    "title": "April 2 and 4 (2025): Introduction to Python",
    "section": "",
    "text": "This workshop is full.\n\n\n\n12 people have registered (surprisingly fast), we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\nAn introduction to Python\nThe bioDSC organizes an introductory workshop that will cover basic functionalities of Python.\nPython is a computer script language that has become a workhorse of data analysis throughout many fields and disciplines. It can process and analyze big data sets, perform image analysis, and create great visualizations.\nSee below for small overview of what types of tasks can be done using Python.\n\n2nd session\nThis workshop will be exactly the same as the previous one. We are simply organizing another session since the previous workshop was booked completely full.\n\n\n\nWorkshop goals\nAfter this workshop, you’ll be familiar with Python basics. We’ll discuss and let you practice on:\n\n\nUsing Python (Jupyter notebooks, VS Studio code, Spyder)\n\nThroughout the workshop, we’ll use an online version of Jupyter notebook to avoid installation issues)\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib).\nUsing loops and conditionals.\nWriting functions.\nGood practices in programming and software engineering.\n\n\nThis workshop will largely (&gt;95%) follow the Carpentries training material “Plotting and programming in Python”. All training material can be found at:\n\nhttps://swcarpentry.github.io/python-novice-gapminder/\n\nSee this workshop page for a summary of desired learning results. The materials we’ll use do not use examples specific to biology, but instead use the “Gapminder” dataset (see [1] and [2]). We think this offers thorough insights into how Python works. Follow-up courses, e.g. into image analysis, will be delve into more biological datasets.\nIf you have any questions regarding whether this course is relevant for you, please send us an email (info@biodsc.nl) or walk by our desks.\n\n\nRequirements\nThis workshop assumes no prior knowledge on Python or programming!\n\n\nWorkshop logistics\nThe course will be given by Misha Paauw, Martijn Wehrens and Frans van der Kloet from the bioDSC. There is room for 4-12 participants. You have to bring your own laptop, we’ll use an online version of Python, so there are no software installation requirements.\nAs mentioned, the contents of this workshops are almost completely based on: https://swcarpentry.github.io/python-novice-gapminder/\n\n\nWorkshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\nApril 2\n13:00 - 17:30\nSP L1.13\nIntroduction to Python, episodes 1-9\n\n\nApril 4\n12:30 - 17:30\nSP L2.06\nIntroduction to Python, episodes 11-18\n\n\n\n\n\nSign up\n\n\n\n\n\n\n\n\nThis workshop is full.\n\n\n\n12 people have registered, we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\n\n\nTypical use cases for Python\nPython is a scripting language, meaning that your write a file with commands for the computer to execute. This allows you to perform complicated tasks, such as:\n\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\n\nImage analysis\n\nPython has become a great open-source tool for image analysis\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images. You can:\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nExtract summary parameters relevant for your analysis.\nRelated libraries: scikit-image, SciPy, OpenCV, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nLibraries: E.g. Keras and PyTorch\n\n\nHigh-throughput data analysis and automation\n\nPython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nBioinformatics\n\nCan be used to handle, manipulate, process, quantify raw sequence data or similar.\nPerform single cell analysis, e.g. using the SCANPY lirbary.\nNot the tool for RNA-seq statistics and gene expression analysis, for which R is superior.\n\nMuch more ..\n\n\nSee also the post I wrote earlier to decide whether you should Python or R."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html",
    "href": "workshops/2025-BYOD-1.html",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "",
    "text": "The bioDSC organizes a “Bring Your Own Data” data visualization workshop. In this workshop, we will offer help and guidance to turn your own dataset into insightful visualizations using ggplot2 in R. You are expected to bring your own table with data to work on, ranging from a small pilot experiment to large-scale experimental assays. We can help improving your existing plotting scripts, or build one from scratch.\nThe aims of this workshop are to help you with:\n\nMaking (publication-quality) plots of your dataset\nWriting reusable scripts to generate more plots later with new datasets\nBecoming more experienced and confident in using R and ggplot2\n\n\n\n\nBoxplots, barplots, and lineplots made in ggplot2, taken from Paauw et al, 2024."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#bring-your-own-data-data-visualization-workshop",
    "href": "workshops/2025-BYOD-1.html#bring-your-own-data-data-visualization-workshop",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "",
    "text": "The bioDSC organizes a “Bring Your Own Data” data visualization workshop. In this workshop, we will offer help and guidance to turn your own dataset into insightful visualizations using ggplot2 in R. You are expected to bring your own table with data to work on, ranging from a small pilot experiment to large-scale experimental assays. We can help improving your existing plotting scripts, or build one from scratch.\nThe aims of this workshop are to help you with:\n\nMaking (publication-quality) plots of your dataset\nWriting reusable scripts to generate more plots later with new datasets\nBecoming more experienced and confident in using R and ggplot2\n\n\n\n\nBoxplots, barplots, and lineplots made in ggplot2, taken from Paauw et al, 2024."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#prior-experience",
    "href": "workshops/2025-BYOD-1.html#prior-experience",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Prior experience",
    "text": "Prior experience\nFor this workshop, we assume that you have some experience with data manipulation and visualisation in R, using the tidyverse packages dplyr, tidyr and ggplot2. For example, you have followed our recent workshop Introduction to R where we covered the materials of R for Social Scientists. Alternatively, you have similar experience from other courses or your own explorations."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#workshop-logistics",
    "href": "workshops/2025-BYOD-1.html#workshop-logistics",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Workshop logistics",
    "text": "Workshop logistics\nIn this workshop, you will work by yourself on your own dataset. Martijn Wehrens, Misha Paauw, and Frans van der Kloet from the bioDSC will be available to discuss your approach and help you with all the problems you encounter along the way. There is room for 4-12 participants. You have to bring your own laptop, with and R and RStudio installed, including the tidyverse package."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#workshop-schedule",
    "href": "workshops/2025-BYOD-1.html#workshop-schedule",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Workshop schedule",
    "text": "Workshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\nMonday 24 february\n13:00 - 16:00\nScience Park B0.206\nVisualisation of datasets"
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#sign-up",
    "href": "workshops/2025-BYOD-1.html#sign-up",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Sign up",
    "text": "Sign up\nSign up by sending an email to info@biodsc.nl.\nIn your email, please include the following:\n\nYour research group\nYour dataset in comma separated, tab separated, or excel table format (.csv, .tsv, .xlsx).\nA brief explanation of your dataset and the plots you want to make\nIdeally, a quick sketch of the plot you want to make\n\nSign up deadline: 19th of february!"
  },
  {
    "objectID": "workshops/2025-introduction-R.html",
    "href": "workshops/2025-introduction-R.html",
    "title": "15 and 17 January 2025: Introduction to R",
    "section": "",
    "text": "The bioDSC organizes an introductory course on data analysis and visualisation in R. R is a programming language for statistical programming and data visualation, and is very popular in the field of biology. Chances are high that those nice figures you see in papers were generated in R! In addition, understanding the basics of R sets you up for more complex data-heavy analysis such as the analysis of RNA-seq data.\nThis workshop assumes no prior knowledge on R or programming!"
  },
  {
    "objectID": "workshops/2025-introduction-R.html#should-i-learn-python-or-r",
    "href": "workshops/2025-introduction-R.html#should-i-learn-python-or-r",
    "title": "15 and 17 January 2025: Introduction to R",
    "section": "Should I learn Python or R?",
    "text": "Should I learn Python or R?\nSee the blog post about Python vs. R in case you’re wondering whether you should start learning: R, or Python. We have more workshops about Python and R coming soon.\n\nSign up\nSign up here. Sign up deadline: 8th of january!"
  },
  {
    "objectID": "workshop-materials/py-intro/11-lists.html",
    "href": "workshop-materials/py-intro/11-lists.html",
    "title": "Exercises Lesson 11: Lists",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 11: Lists\n\n\nFill in the blanks\n\n\n\n\n\n\nQuestion\n\n\n\nFill in the blanks so that the program below produces the output shown.\nvalues = ____\nvalues.____(1)\nvalues.____(3)\nvalues.____(5)\nprint('first time:', values)\nvalues = values[____]\nprint('second time:', values)\nOutput:\nfirst time: [1, 3, 5]\nsecond time: [3, 5]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nvalues = []\nvalues.append(1)\nvalues.append(3)\nvalues.append(5)\nprint('first time:', values)\nvalues = values[1:]\nprint('second time:', values)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nHow Large is a Slice?\n\n\n\n\n\n\nQuestion\n\n\n\nIf start and stop are both non-negative integers, how long is the list values[start:stop]?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe list values[start:stop] has up to stop - start elements. For example, values[1:4] has the 3 elements values[1], values[2], and values[3]. Why ‘up to’? As we saw in episode 2, if stop is greater than the total length of the list values, we will still get a list back but it will be shorter than expected.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nFrom Strings to Lists and Back\nGiven this code:\nprint('string to list:', list('tin'))\nprint('list to string:', ''.join(['g', 'o', 'l', 'd']))\nwhich outputs:\nstring to list: ['t', 'i', 'n']\nlist to string: gold\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat does list('some string') do?\nWhat does '-'.join(['x', 'y', 'z']) generate?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlist('some string') converts a string into a list containing all of its characters.\n.join() returns a string that is the concatenation of each string element in the list and adds the separator between each element in the list. This results in x-y-z. The separator between the elements is the string that provides this method.\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nWorking With the End\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat does the following program print?\n\nelement = 'helium'\nprint(element[-1])\n\nHow does Python interpret a negative index?\nIf a list or string has N elements, what is the most negative index that can safely be used with it, and what location does that index represent?\nIf values is a list, what does del values[-1] do?\nHow can you display all elements but the last one without changing values? (Hint: you will need to combine slicing and negative indexing.)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe program prints ‘m’.\nPython interprets a negative index as starting from the end (as opposed to starting from the beginning). The last element is -1.\nThe last index that can safely be used with a list of N elements is element -N, which represents the first element.\ndel values[-1] removes the last element from the list.\nvalues[:-1]\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\n\nAdditional Exercises\n\n\nStepping Through a List\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat does the following program print?\n\nelement = 'fluorine'\nprint(element[::2])\nprint(element[::-1])\n\nIf we write a slice as low:high:stride, what does stride do?\nWhat expression would select all of the even-numbered items from a collection?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe program prints\n\nfurn\neniroulf\n\nstride is the step size of the slice.\nThe slice 1::2 selects all even-numbered items from a collection: it starts with element 1 (which is the second element, since indexing starts at 0), goes on until the end (since no end is given), and uses a step size of 2 (i.e., selects every second element).\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nSlice Bounds\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does the following program print?\nelement = 'lithium'\nprint(element[0:20])\nprint(element[-1:3])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nlithium\nThe first statement prints the whole string, since the slice goes beyond the total length of the string. The second statement returns an empty string, because the slice goes “out of bounds” of the string.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nSort and Sorted\n\n\n\n\n\n\nQuestion\n\n\n\nWhat do these two programs print? In simple terms, explain the difference between sorted(letters) and letters.sort().\n# Program A\nletters = list('gold')\nresult = sorted(letters)\nprint('letters is', letters, 'and result is', result)\n# Program B\nletters = list('gold')\nresult = letters.sort()\nprint('letters is', letters, 'and result is', result)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nProgram A prints:\nletters is ['g', 'o', 'l', 'd'] and result is ['d', 'g', 'l', 'o']\nProgram B prints:\nletters is ['d', 'g', 'l', 'o'] and result is None\nsorted(letters) returns a sorted copy of the list letters (the original list letters remains unchanged), while letters.sort() sorts the list letters in-place and does not return anything.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nCopying (or Not)\nWhat do these two programs print? In simple terms, explain the difference between new = old and new = old[:].\n\n\n\n\n\n\nQuestion\n\n\n\n# Program A\nold = list('gold')\nnew = old      # simple assignment\nnew[0] = 'D'\nprint('new is', new, 'and old is', old)\n# Program B\nold = list('gold')\nnew = old[:]   # assigning a slice\nnew[0] = 'D'\nprint('new is', new, 'and old is', old)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nProgram A prints:\nnew is ['D', 'o', 'l', 'd'] and old is ['D', 'o', 'l', 'd']\nProgram B outputs:\nnew is ['D', 'o', 'l', 'd'] and old is ['g', 'o', 'l', 'd']\nnew = old makes new a reference to the list old; new and old point towards the same object.\nnew = old[:] however creates a new list object new containing all elements from the list old; new and old are different objects.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nNegative slicing\n\n\n\n\n\n\nQuestion\n\n\n\nelement = 'lithium'\nWhat does element[-7:3] print and why?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom geeksforgeeks:\nThe slicing operator “:” has the following syntax\nsequence[Start : End : Step]\n\nParameters:\n\n    Start: It is the starting point of the slice or substring.\n    End: It is the ending point of the slice or substring but it does not include the last index.\n    Step: It is number of steps it takes.\nIn the world of programming, indexing starts at 0, and Python also follows 0-indexing what makes Python different is that Python also follows negative indexing which starts from -1. -1 denotes the last index, -2, denotes the second last index, -3 denotes the third last index, and so on.\nIf we look at the image below we can see that -7 points to the letter ‘L’.\n\nThe second index 3 points to the endpoint but does not include it so the output will be\nlit\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nMore list comprehensions and filtering\n\n\n\n\n\n\nQuestion A\n\n\n\nA very simple comprehension syntax looks like this:\n[x for x in range(10)]\nEdit the code such that we make a new list consisting of values 2x+x^2-1, where x is the index of the list element.\n____ = [____ for x in range(10)]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsome_list = [2*x+x**2-1 for x in range(10)]\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nNow from that list, select values that are &gt; 10, by modifying the following code:\n[x for x in your_list if ______]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsome_list_sel = [x for x in some_list if x&gt;10]\n\nprint(some_list)\nprint(some_list_sel)\nThe print statements show it has worked.\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nConvert your list to a np.array, and do the same in a more elegant way.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsome_list_np = np.array(some_list)\nsome_list_np_sel = some_list_np[some_list_np&gt;10]\n\nprint(some_list_np_sel)\n\n\n\n\n\n\n\n\n\nQuestion D\n\n\n\nGiven:\nlist_withtop = [1000+-10*(x-7)**2 for x in range(20)]\n\nFind the position of the maximum value in this array.\nEdit the code above such that the maximum value shifts to an index of your choice. Check whether you succeedded by finding the maximum value.\n\nMultiply your list with -1, and put the result in another list.\n\nWhere are now the maximum and minimum values?\nDoes this make sense?\n\nlist_line = [70*x-1000 for x in range(20)]\n\nWhat’s the biggest value, either negative or positive, in this list?\nAnd the index of that number?\nWhat’s the standard deviation?\nCan you calculate the correlation between list_withtop and list_line?\nCan you make a scatter plot of list_withtop versus list_line?\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n# 1. Find the position of the maximum value in this array.\nlist_withtop.argmax()\n# 2. Edit the code above such that the maximum value shifts to an index of your choice.   \n#       Check whether you succeedded by finding the maximum value.\nmy_shift = 15\nlist_withtop_2 = np.array([1000+-10*(x-my_shift)**2 for x in range(20)])\nlist_withtop_2.argmax()\n# 3. Multiply your list with `-1`, and put the result in another list.\n#     1. Where are now the maximum and minimum values?\n#     2. Does this make sense?\nlist_withtop_2_inv = list_withtop_2*-1 # inverted list (*-1)\n# The maximum and minimum positions have swapped, as we multiplied by -1.\nlist_withtop_2_inv.argmin()\nlist_withtop_2_inv.argmax()\n# 4. `list_line = [70*x-1000 for x in range(20)]`\n#     1. What's the biggest value, either negative or positive, in this list?\nlist_line = np.array([70*x-1000 for x in range(20)])\n# That number is:\nlist_line[np.argmax(np.abs(list_line))]\n#     2. And the index of that number?\nnp.argmax(np.abs(list_line))\n#     3. What's the standard deviation?\nnp.std(list_line)\n#     4. Can you calculate the correlation between list_withtop and list_line?\n# Using np:\nR_calculated = np.corrcoef(list_withtop, list_line)[0,1]\n# Manually:\nR_calculated2 = \\\n    np.sum((list_withtop-np.mean(list_withtop)) * (list_line-np.mean(list_line))) / \\\n        np.sqrt(  np.sum((np.mean(list_withtop)-list_withtop)**2) * np.sum((np.mean(list_line)-list_line)**2)  )\nprint(R_calculated)\nprint(R_calculated2)\n#     5. Can you make a scatter plot of  list_withtop versus list_line?\nplt.scatter(list_withtop, list_line)\nplt.xlabel('list_withtop'); plt.ylabel('list_line')\n\n\n\n\n\nSource: bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/09-plotting.html",
    "href": "workshop-materials/py-intro/09-plotting.html",
    "title": "Exercises Lesson 9: Plotting",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\n\nExercises Lesson 9: Plotting\n\nMinima and maxima\n\n\n\n\n\n\nQuestion\n\n\n\nFill in the blanks below to plot the minimum GDP per capita over time for all the countries in Europe.\nModify it again to plot the maximum GDP per capita over time for Europe, you need to edit the code beyond the ___ for this.\ndata_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\ndata_europe_transposed = data_europe.T\n\ndata_europe_transposed['min'] = data_europe.____\ndata_europe_transposed['max'] = ____\ndata_europe_transposed['year'] = ____\n\nsns.lineplot(data_europe_transposed, x='year', y='min')\nsns.lineplot(data_europe_transposed, x='year', y='max')\nplt.legend(loc='best')\nplt.xticks(rotation=90)\nHINT: if you don’t see the solution, take it step by step. Break down the task in subtasks, and adress the first step towards the solution first. Try that first. Running code is free.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndata_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\ndata_europe_transposed = data_europe.T\n\ndata_europe_transposed['min'] = data_europe.min()\ndata_europe_transposed['max'] = data_europe.max()\ndata_europe_transposed['year'] = data_europe_transposed.index.str.replace(\"gdpPercap_\",\"\")\n\nsns.lineplot(data_europe_transposed, x='year', y='min', label='min')\nsns.lineplot(data_europe_transposed, x='year', y='max', label='max')\n\nplt.legend(loc='best')\nplt.xticks(rotation=90)\nThe figure now looks like:\n\n\n\n\n\n\nSource: Carpentries workshop materials, edited by the bioDSC.\n\n\n\n\nMean gene expression\nUse the kohela data again, and process it as such:\n# Load data, note the \".T\" at the end here\ndf_kohela = pd.read_csv('data/kohela-et-al.csv', index_col=0).T\n# create new 'masks'\nepicardial_cells = df_kohela['WT1']&gt;3\nfibroblast_cells = df_kohela['COL2A1']&gt;30\nfat_cells = df_kohela['PPARG']&gt;2\n# Add cell type\ndf_kohela['Celltype'] = 'unknown'\ndf_kohela.loc[epicardial_cells,'Celltype'] = 'epicardial'\ndf_kohela.loc[fibroblast_cells, 'Celltype'] = 'fibroblast'\ndf_kohela.loc[fat_cells, 'Celltype'] = 'fat'\n# Add conditions\ndf_kohela['Condition'] = 'unknown'\ndf_kohela.loc[df_kohela.index.str.contains('WT_'), 'Condition'] = 'WT'\ndf_kohela.loc[df_kohela.index.str.contains('mutant_'), 'Condition'] = 'mutant'\nDisclaimer: we are analyzing single cell data in this exercise. Within the constraints of this introductory course, we don’t use proper statistic or methodological approaches to analyze the data. These exercises are meant to teach you Python concepts, but also give you a flavor of biological data analysis.\n\n\n\n\n\n\nQuestion A\n\n\n\nUse seaborn to create a ‘stripplot’ plot for WT1 expression per cell type. Then create a similar plot for TBX18 (both epicardial cell markers). What information can be extracted from this plot?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsns.stripplot(df_kohela, x='Celltype', y='TBX18', jitter=True, color='red')\nplt.tick_params(axis='x', rotation=45)\nsns.stripplot(df_kohela, x='Celltype', y='WT1', jitter=True, color='blue')\nplt.tick_params(axis='x', rotation=45)\n\nSince we classified cells with high WT1 levels as epicardial cells, we expected to see high WT1 levels in epicardial cells by construction. This is indeed the case.\nWe see one stray fibroblast cell with a high WT1 level. This cell appears to be WT1+ and COL2A1+. Because of the order in which we executed the code, this cell got classified as fibroblast.\nTBX18 is expected to show high expression in epicardial cells too, and, if both WT1 and TBX18 are specific for epicardial cells, they should not occur in other cell types. This also appears to be the case.\nQuite some cells still have the ‘unknown’ phenotype, but otherwise these plots are supportive of our strategy of using WT1 and TBX18 as epicardial markers.\nFor a real experiment, you might use packages like scanpy (Python) or Seurat (R) to analyze your data. The authors of the Kohela et al. study used Monocle 2 (R).\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nNow create a scatter plot, showing WT1 expression vs. TBX18 expression across all cells. What does this tell us?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCreating a scatterplot is straightforward:\nsns.scatterplot(df_kohela, x='WT1', y='TBX18')\n\nThis plot highlights a few challenges with data analysis.\nSingle cell RNA-seq data can typically show only a few reads per gene per cell, or even zero, even when the gene is expressed. This is a limitation of the RNA-seq technique. This results in many cells having only few or zero counts for the WT1 and TBX18 gene expression. In addition, the high number number of cells results in many overlapping datapoints. It is therefore hard to see any pattern in the data.\nWe can make the individual point slightly transparent by adding alpha=0.1 as argument to the scatterplot method. This already helps to visualize the data better:\nsns.scatterplot(df_kohela, x='WT1', y='TBX18', alpha=.1)\n\nIt appears that there is not much correlation between the TBX18 and WT1 expression. This can either be because cells do not typically express both at the same time, or because our detection is not sufficient to identify such a correlation from this plot.\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nColor individual datapoints of the scatter plot per cell type. What does this tell us?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nsns.scatterplot(df_kohela, x='WT1', y='TBX18', hue='Celltype')\n\n\nThis plot is still somewhat hard to interpret, because many points are overlaying on top of each other.\nOne way to disentangle the points, is to plot the different celltypes separately:\nmaxval = np.max(df_kohela.loc[:,['WT1','TBX18']])\n\nplt.title('Epicardial cells')\nsns.scatterplot(df_kohela.loc[df_kohela['Celltype']=='epicardial',:], x='WT1', y='TBX18', alpha=.1)\nplt.xlim([0, maxval+1])\nplt.ylim([0, maxval+1])\nplt.show()\n\nplt.title('Fibroblast cells')\nsns.scatterplot(df_kohela.loc[df_kohela['Celltype']=='fibroblast',:], x='WT1', y='TBX18', alpha=.1)\nplt.xlim([0, maxval+1])\nplt.ylim([0, maxval+1])\nplt.show()\n\nsns.scatterplot(df_kohela.loc[df_kohela['Celltype']=='fat',:], x='WT1', y='TBX18', alpha=.1)\nplt.title('Fat cells')\nplt.xlim([0, maxval+1])\nplt.ylim([0, maxval+1])\nplt.show()\n\nsns.scatterplot(df_kohela.loc[df_kohela['Celltype']=='unknown',:], x='WT1', y='TBX18', alpha=.1)\nplt.title('Unknown cells')\nplt.xlim([0, maxval+1])\nplt.ylim([0, maxval+1])\nplt.show()\n \n \nInterestingly, we now see that fibroblast (COL2A1+) and fat cells (PPARG+) typically don’t express WT1 or TBX18, whilst epicardial cells (WT1+) do also often show TBX18 expression.\nThere are nevertheless still WT+ cells that don’t show TBX18 expression, and moreover many WT- cells (the unknown cells) that show TBX18 expression, so these plots are not conclusive, and more sophisticated single cell methods might be needed to further study these cells.\n\n\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nMore correlations\nThis short program creates a plot showing the correlation between GDP and life expectancy for 2007, scaling marker size by population:\ndata_all = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsns.scatterplot(data_all, x='gdpPercap_2007', y='lifeExp_2007', size='pop_2007', sizes=(1,40**2), legend=False)\n\n\n\n\n\n\nQuestion\n\n\n\nUsing online help and other resources, explain what each plotting argument does.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee e.g. the scatterplot documentation.\n\ndata_all: The first argument specifies which dataframe to use as input.\nx='gdpPercap_2007': The x argument expects the name of the column that holds values to plot on the x-axis.\ny='lifeExp_2007': Same as x, but for the y axis.\nsize='pop_2007': This tells seaborn to scale the point size with the values in a column, in this case the population sizes of the countries (in 2007).\nsizes=(1,40**2): This sets the range of the sizes to be used for the points, given as (min, max). These are areas, which is why it’s convenient to use the square operator **2, such that you can choose a radius (in this case minimally 1 and maximally 40).\nlegend=False: This turns off the legend.\n\nThe result:\n\n\n\n\n\n\nSource: Carpentries workshop materials, edited by the bioDSC.\n\n\n\n\n\n\n\nAdditional Exercises\n\n\nEven more correlations\n\n\n\n\n\n\n\nQuestion A\n\n\n\nUse the code from the “More Correlations” exercise, and try to add the following lines to the plotting code:\nplt.text(data_all.loc['United States','gdpPercap_2007'], data_all.loc['United States','lifeExp_2007'], 'United States')\nplt.text(data_all.loc['Netherlands','gdpPercap_2007'], data_all.loc['Netherlands','lifeExp_2007'], 'Netherlands')\n\nWhat’s happening here? (You might need to use Google.)\nAdd your favorite country too.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe function matplotlib.pyplot.text,\nmatplotlib.pyplot.text(x, y, s, fontdict=None, **kwargs)\nallows you to put text s in your plotting area at location x,y. x, y and s should be single values (not arrays or tables)\n\ndata_all.loc['United States','gdpPercap_2007'] selects the x value corresponding to the united states.\ndata_all.loc['United States','lifeExp_2007'] selects the y value corresponding to the united states.\n'United states' provides the label.\n\nThe result:\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nHow would you add labels for the top 10 GDP countries?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis would be very tedious with what you learned currently! In the next lessons (particularly lesson 12), we’ll learn how to automate your code. This will be very useful for this particular challenge.\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nSubselection and melting\n\n\n\n\n\n\nQuestion A\n\n\n\nUsing the kohela data, first create a new dataframe df_kohela_sel with only a selection of a few genes, and the condition and cell type columns.\nSelect the following genes: ['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA']. These are epicardial markers (WT1, TBX18), a transcription factor (TFAP2A), fibroblast markers (COL2A1, ACTA2), and fat markers (PPARG, CEBPA).\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndf_kohela_sel = \\\n    df_kohela.loc[:,['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA','Celltype','Condition']]\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nNow melt this dataframe using pd.melt(), and use cell type and condition as identifier variables.\n\nWhat will happen to the gene expression values?\nWhat is sensible input for the var_name and value_name parameters?\nWhy is this useful? (For answer, see next questions.)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ndf_kohela_melted = pd.melt(df_kohela_sel, \\\n    id_vars=['Celltype','Condition'], \\\n    var_name='Gene', value_name='Expression')\n\nWhat will happen to the gene expression values?\n\nExpression data that were spread out over multiple columns are now re-organized in one long column. A new column is created alongside the column with all data, that specifies from which column the data originated.\n\nWhat is sensible input for the var_name and value_name parameters?\n\nThe var_name is the name of the new column that specifies from which original columns the data came. In this case the original columns corresponded to different genes, and so “Gene” might be a sensible name.\nvalue_name is the name of the new column that holds all values previously spread out over multiple columns. The values all related to expression, and so “Expression” seems a good name.\n\nWhy is this useful? (For answer, see next questions.)\n\nThis is useful because now this dataframe can be easily used as input for the seaborn plots.\n\n\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nUse df_kohela_melted.head() to check whether the output is as expected.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe output was:\n  Celltype Condition Gene  Expression\n0  unknown    mutant  WT1           0\n1  unknown    mutant  WT1           0\n2  unknown    mutant  WT1           0\n3  unknown    mutant  WT1           0\n4  unknown    mutant  WT1           1\nand indeed, the columns Celltype and Condition were retained, and the expression data was re-organized into one column “Expression”, annotated by the new column “Gene” which lists from which original column that data originated.\n\n\n\n\n\n\n\n\n\nQuestion D\n\n\n\nMake a violin or stripplot, with as x-axis the genes, expression on the y-axis, and colored for condition. Is there an issue with this plot?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe code:\nsns.violinplot(df_kohela_melted, x='Gene', y='Expression', hue='Condition')\nplt.show()\nsns.stripplot(df_kohela_melted, x='Gene', y='Expression', hue='Condition', ax=ax, dodge=1)\nplt.show()\n\nFor the Violinplot\n\nAn issue you might have identified is that it is very hard to compare gene expression levels because some of these genes have rather high expression levels, thus forcing the expression levels of other genes to the very bottom of the plot, making it hard to distinguish differences.\nAlso some outliers appear to be affecting the scale to a large extend, making it hard to see trends.\nMoreover, you can probably not even see the Violins..\n\n\nFor the stripplot\n\nThis looks better, but it is still hard to see differences between wild type and mutant cell gene expression.\n\n\n\n\n\n\n\n\n\n\nQuestion E\n\n\n\nLook at the following example:\nimport numpy as np\n\n# A custom function, which normalizes a series by its mean\n# We'll learn more about functions in Lesson 16\ndef gene_normalization(X):\n    return X / np.mean(X)\n\n# Create a subset of the data\ncell_subset = ['mutant_rep1_cell174', 'WT_rep2_cell348', 'mutant_rep1_cell160',\n       'WT_rep1_cell022', 'mutant_rep1_cell069']\ngene_subset = ['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA']\n\n# Normalize gene expression\ndf_kohela_subset2 = df_kohela.loc[cell_subset, gene_subset]\ndf_kohela_subset2_normalized = df_kohela_subset2.transform(gene_normalization)\n\n# Print the result\nprint(df_kohela_subset2_normalized)\n\nCheck out what gene_normalization(df_kohela_subset2['WT1']) does.\nWhat does the transform method do in the above code?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n1. Check out what gene_normalization(df_kohela_subset2['WT1']) does.\nWhat gene_normalization() is described in the comment above the function, it normalizes a series by its mean. df_kohela_subset2['WT1'] is the series of expression values related to WT1.\ngene_normalization(df_kohela_subset2['WT1']) will thus return WT1 expression values normalized by their mean.\n\n\n2. What does the transform method do in the above code?\nTransform applies a function to each of the columns in a dataframe. So in this case it will go over each column, which correspond to df_kohela_subset2['WT1'], df_kohela_subset2['TBX18'], df_kohela_subset2['TFAP2A'] and so forth, and feed that column to the custom function, which in this case will return values that are normalized. Those normalized values are then put back in the column the function was applied to.\nThus, the each column will now correspond to mean-divided expression (ie will be normalized).\n\n\n\n\n\n\n\n\n\n\nQuestion F\n\n\n\nEdit the following code (replacing blanks by code) to normalize the gene expression by the total expression of each gene. Hint: look at exercise E.\ndf_kohela_grouped = df_kohela_melted.groupby(_______)\ndf_kohela_melted['Expression_normalized'] = df_kohela_grouped['Expression']._______(gene_normalization)\nThen, similar to D, plot the normalized gene expression using both the sns.barplot and sns.stripplot. For the stripplot, use the additional parameter dodge=True.\nThe barplot looks nice, but does it contain all information?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe normalization:\ndf_kohela_grouped = df_kohela_melted.groupby('Gene')\ndf_kohela_melted['Expression_normalized'] = df_kohela_grouped['Expression'].transform(gene_normalization)\nThe bar code hides the information about the distribution of the single data points. So perhaps it is best to combine those two plots:\nsns.barplot(df_kohela_melted, x='Gene', y='Expression_normalized', \n            hue='Condition', ci=None)\nsns.stripplot(df_kohela_melted, x='Gene', y='Expression_normalized', \n              hue='Condition', dodge=True, color='black', alpha=.1, size=4, \n              legend=False)\nplt.ylim([0,15])\nplt.show()\nplt.close('all')\nThe plot now looks like:\n\nNote that some data points are now hidden due to the choice of y limits. For a paper, one might need to think more deeply about how to display this data.\n\n\n\n\n\n\n\n\n\nQuestion G\n\n\n\nFinally, you might want to change the order in these plots. Use the following line:\ndf_kohela_melted['Condition'] = pd.Categorical(df_kohela_melted['Condition'], categories=['WT', 'mutant'], ordered=True)\nAnd then run your plotting code again. What has happened?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis results in the condition becoming a categorical series, which has a specific order, that is translated to the plot. This is very useful if you want to control where the labels go on your axes. The order of ‘WT’ and ‘mutant’ on the x-axis now makes more sense.\nThe figure now looks like:\n\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nSaving your plot\nYou might want to save your plot. You can use the plt.savefig function for this.\n\n\n\n\n\n\nExercise\n\n\n\nCheck out this code with some additional convenient options. Change '/your/location/your-filename.pdf' to a convenient path where you save your figure.\nimport matplotlib.pyplot as plt\n\n# Bang Wong colorblind-friendly color scheme (https://www.nature.com/articles/nmeth.1618)\ncolors_bangwong = [\n    \"#E69F00\",  # Orange\n    \"#56B4E9\",  # Sky Blue\n    \"#009E73\",  # Bluish Green\n    \"#F0E442\",  # Yellow\n    \"#0072B2\",  # Blue\n    \"#D55E00\",  # Vermillion\n    \"#CC79A7\",  # Reddish Purple\n    \"#000000\"   # Black\n]\n\nplt.style.use('default')\nfig, ax = plt.subplots(1,1, figsize=(10/2.54,10/2.54))\nax.plot([1,2,3,4], [1,4,9,16], linestyle='--', color=colors_bangwong[1], label=r'$x^2$')\nax.plot([1,2,3,4], [1,5,11,19], linestyle=':', color=colors_bangwong[2], label=r'$x^2+(x-1)$')\nax.legend()\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_title('Sample Plot', fontsize=12)\nax.legend(fontsize=12)\nax.tick_params(axis='both', which='major', labelsize=12)\nplt.tight_layout()\n\nplt.savefig('/your/location/your-filename.pdf', dpi=300, bbox_inches='tight')\nplt.close(fig)\n\n# Use this command to show the figure when not using (Jupyter) notebooks.\n# plt.show()\n\n\n\n\nSource: bioDSC.\n\n\n\n\nCorrelations\n\n\n\n\n\n\nQuestion A\n\n\n\nModify the code from “Minima and Maxima” exercise to create a scatter plot showing the relationship between the minimum and maximum GDP per capita across the countries in Asia, with each point in the plot corresponding to a year. What relationship do you see (if any)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe code:\ndata_asia = pd.read_csv('data/gapminder_gdp_asia.csv', index_col='country')\ndata_asia_transposed = data_asia.T\n\ndata_asia_transposed['min'] = data_asia.min()\ndata_asia_transposed['max'] = data_asia.max()\ndata_asia_transposed['year'] = data_asia_transposed.index.str.replace(\"gdpPercap_\",\"\")\n\nsns.scatterplot(data_asia_transposed, x='min', y='max', hue='year', palette='viridis', legend=False)\nResults in the plot:\n\nIt can be seen that there is no correlation between the minimum GDP and maximum GDP for a specific year, indicating that GDPs across asia do not tend to rise and fall together.\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nYou might note that the variability in the maximum is much higher than that of the minimum. Take a look at the maximum over time and the max indexes:\ndata_asia = pd.read_csv('data/gapminder_gdp_asia.csv', index_col='country')\n\ndf_max_GDP = pd.DataFrame()\ndf_max_GDP['GDP_max'] = data_asia.max()\ndf_max_GDP['Year']    = data_asia.columns.str.replace('gdpPercap_','').astype(int)\n\nplt.plot(df_max_GDP['Year'], df_max_GDP['GDP_max'])\n\nprint(data_asia.idxmax())\nprint(data_asia.idxmin())\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe plot looks like:\n\nSeems the variability in this value is due to a sharp drop after 1972. Some geopolitics at play perhaps? Given the dominance of oil producing countries, maybe the Brent crude index would make an interesting comparison? Whilst Myanmar consistently has the lowest GDP, the highest GDP nation has varied more notably.\n\n\n\n\n\nSource: Carpentries workshop materials, edited by the bioDSC.\n\n\n\n\nNormalized dataframe\n\n\n\n\n\n\nQuestion\n\n\n\n\nIn the previous lesson about dataframes (in the additional exercises), we normalized the GDP data against the average trend. Plot the data from this normalized dataframe.\nIs this helpful in any way?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n# Copy the df to modify it and not interfer with other code\ndata_europe_relative_copy = data_europe_relative.copy()\n# Add the country as column\ndata_europe_relative_copy['Country'] = data_europe_relative_copy.index\n# Melt it \ndata_europe_relative_melted = data_europe_relative_copy.melt(id_vars='Country')\n# Add the year as number\ndata_europe_relative_melted['year'] = data_europe_relative_melted['variable'].str.replace('gdpPercap_','').astype(int)\n\n# And plot\nsns.lineplot(data_europe_relative_melted, x='year', y='value', hue='Country', legend=False)\nWe can now visualize the relative position of countries over time. Unfortunately, there are a little bit too many countries to clearly identify single ones. A solution to this issue is left for another time.\n\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nCrude oil\n\n\n\n\n\n\nExercise\n\n\n\nCrude oil prices can be found here.\nThis gives the data below:\nDecade  Year-0  Year-1  Year-2  Year-3  Year-4  Year-5  Year-6  Year-7  Year-8  Year-9\n  1850's                                        16.00\n  1860's    9.59    0.49    1.05    3.15    8.06    6.59    3.74    2.41    3.62    5.64\n  1870's    3.86    4.34    3.64    1.83    1.17    1.35    2.52    2.38    1.17    0.86\n  1880's    0.94    0.92    0.78    1.10    0.85    0.88    0.71    0.67    0.65    0.77\n  1890's    0.77    0.56    0.51    0.60    0.72    1.09    0.96    0.68    0.80    1.13\n  1900's    1.19    0.96    0.80    0.94    0.86    0.62    0.73    0.72    0.72    0.70\n  1910's    0.61    0.61    0.74    0.95    0.81    0.64    1.10    1.56    1.98    2.01\n  1920's    3.07    1.73    1.61    1.34    1.43    1.68    1.88    1.30    1.17    1.27\n  1930's    1.19    0.65    0.87    0.67    1.00    0.97    1.09    1.18    1.13    1.02\n  1940's    1.02    1.14    1.19    1.20    1.21    1.22    1.41    1.93    2.60    2.54\n  1950's    2.51    2.53    2.53    2.68    2.78    2.77    2.79    3.09    3.01    2.90\n  1960's    2.88    2.89    2.90    2.89    2.88    2.86    2.88    2.92    2.94    3.09\n  1970's    3.18    3.39    3.39    3.89    6.87    7.67    8.19    8.57    9.00    12.64\n  1980's    21.59   31.77   28.52   26.19   25.88   24.09   12.51   15.40   12.58   15.86\n  1990's    20.03   16.54   15.99   14.25   13.19   14.62   18.46   17.23   10.87   15.56\n  2000's    26.72   21.84   22.51   27.56   36.77   50.28   59.69   66.52   94.04   56.35\n  2010's    74.71   95.73   94.52   95.99   87.39   44.39   38.29   48.05   61.40   55.59\n  2020's    36.86   65.84   93.97   76.10                       \nSave that data to a .tsv file, and upload it.\nNow try to understand the code below:\nimport pandas as pd\n\n# Load the data\ndf_crudeoil = \\\n    pd.read_csv('data/crude_oil.tsv', sep='\\t')\n\n# reshape the data, such that it becomes a long list\ndf_crudeoil_melted = df_crudeoil.melt(id_vars='Decade', var_name='lastdigit')\n\n# now reformat the year information\n# search and replace first\ndf_crudeoil_melted.loc[:,'Decade'] = df_crudeoil_melted.loc[:,'Decade'].str.replace(\"0's\",'')\ndf_crudeoil_melted.loc[:,'lastdigit'] = df_crudeoil_melted.loc[:,'lastdigit'].str.replace('Year-','')\n# now combine information to create a new column \"Year\"\ndf_crudeoil_melted.loc[:,'Year'] = (df_crudeoil_melted.loc[:,'Decade'] + df_crudeoil_melted.loc[:,'lastdigit']).astype(int)\n# Inspect the result\nprint(df_crudeoil_melted.head())\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUsing your new plotting skills, compare this data against the trends in the Asian GDPs showed earlier.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe saved to a tsv file and ran the code above.\nThen, made a plot:\nsns.lineplot(df_crudeoil_melted, x='Year', y='value', label='price')\nplt.axvline(x=1972, color='red', linestyle='--', label='1972')\nplt.legend()\nplt.show()\nThe plot looks like:\n\nProbably quite some things happened to Arabian oil countries after ’72, related to oil and aformentioned drop in GDP.\n\n\n\n\n\nSource: bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/12-loops.html",
    "href": "workshop-materials/py-intro/12-loops.html",
    "title": "Exercises Lesson 12: Loops",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 12: Loops\n\n\nTracing Execution\n\n\n\n\n\n\nQuestion\n\n\n\nCreate a table showing the numbers of the lines that are executed when this program runs, and the values of the variables after each line is executed.\n1| total = 0\n2| for char in \"tin\":\n3|     total = total + 1\n\n\n\nStep\nLine Number\nVariable Values\n\n\n\n\n1\n1\ntotal = 0\n\n\n2\n2\ntotal = 0, char = ‘t’\n\n\n3\n3\n..\n\n\n..\n..\n..\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nStep\nLine Number\nVariable Values\n\n\n\n\n1\n1\ntotal = 0\n\n\n2\n2\ntotal = 0, char = ‘t’\n\n\n3\n3\ntotal = 1, char = ‘t’\n\n\n4\n2\ntotal = 1, char = ‘i’\n\n\n5\n3\ntotal = 2, char = ‘i’\n\n\n6\n2\ntotal = 2, char = ‘n’\n\n\n7\n3\ntotal = 3, char = ‘n’\n\n\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nPractice Accumulating\n\n\n\n\n\n\nQuestion A\n\n\n\nFill in the blanks in the program below to produce the indicated result.\n# Total length of the strings in the list: [\"red\", \"green\", \"blue\"] =&gt; 12\ntotal = 0\nfor word in [\"red\", \"green\", \"blue\"]:\n    ____ = ____ + len(word)\nprint(total)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ntotal = 0\nfor word in [\"red\", \"green\", \"blue\"]:\n    total = total + len(word)\nprint(total)\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nFill in the blanks in the program below to produce the indicated result.\n# List of word lengths: [\"red\", \"green\", \"blue\"] =&gt; [3, 5, 4]\nlengths = ____\nfor word in [\"red\", \"green\", \"blue\"]:\n    lengths.____(____)\nprint(lengths)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nlengths = []\nfor word in [\"red\", \"green\", \"blue\"]:\n    lengths.append(len(word))\nprint(lengths)\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nFill in the blanks in the program below to produce the indicated result.\n# Concatenate all words: [\"red\", \"green\", \"blue\"] =&gt; \"redgreenblue\"\nwords = [\"red\", \"green\", \"blue\"]\nresult = ____\nfor ____ in ____:\n    ____\nprint(result)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nwords = [\"red\", \"green\", \"blue\"]\nresult = \"\"\nfor word in words:\n    result = result + word\nprint(result)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\nAdditional Exercises\n\n\nPlotting automation\nRemember this code from Lesson 9?\ndf_all = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsns.scatterplot(df_all, x='gdpPercap_2007', y='lifeExp_2007', size='pop_2007', sizes=(1,40**2), legend=False)\nplt.text(df_all.loc['United States','gdpPercap_2007'], df_all.loc['United States','lifeExp_2007'], 'United States')\nplt.text(df_all.loc['Netherlands','gdpPercap_2007'], df_all.loc['Netherlands','lifeExp_2007'], 'Netherlands')\n\n\n\n\n\n\nQuestion\n\n\n\nTry to annotate 10 selected countries automatically.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n# Maybe get the top 3 and bottom 3, and some more\ntop3_life = df_all.loc[:,'lifeExp_2007'].nlargest(3).index\nbottom3_life = df_all.loc[:,'lifeExp_2007'].nsmallest(3).index\ncustom_countries = ['Netherlands','United States']\n# Merge those three lists\nmy_countries_to_select = list(top3_life) + list(bottom3_life) + custom_countries\n\n# Plot\nsns.scatterplot(df_all, x='gdpPercap_2007', y='lifeExp_2007', size='pop_2007', sizes=(1,40**2), legend=False)\n# Now add text automatically\nfor country in my_countries_to_select:\n    plt.text(df_all.loc[country,'gdpPercap_2007'], df_all.loc[country,'lifeExp_2007'], country, fontsize=5)\nplt.show()\nThis plot looks like:\n\nA bit messy with the overlapping text, but this worked great otherwise!\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nReversing a String\nFill in the blanks in the program below so that it prints “nit” (the reverse of the original character string “tin”).\n\n\n\n\n\n\nQuestion\n\n\n\noriginal = \"tin\"\nresult = ____\nfor char in original:\n    result = ____\nprint(result)\nHint: If this is challenging:\n\nTry to first reproduce the word tin in result, using this loop.\nUse a similar approach as the examples we used.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\noriginal = \"tin\"\nresult = \"\"\nfor char in original:\n    result = char + result\nprint(result)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nPractice Accumulating (again)\n\n\n\n\n\n\nQuestion\n\n\n\nCreate an acronym: Starting from the list [\"red\", \"green\", \"blue\"], create the acronym “RGB” using a for loop.\n\nRemark: Note the capitals in “RGB”! You may need to use a string method to properly format the acronym.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nacronym = \"\"\nfor word in [\"red\", \"green\", \"blue\"]:\n    acronym = acronym + word[0].upper()\nprint(acronym)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nIdentifying Item Errors\n\n\n\n\n\n\nQuestion\n\n\n\n\nRead the code below and try to identify what the errors are without running it.\nRun the code, and read the error message. What type of error is it?\nFix the error.\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[4])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis list has 4 elements and the index to access the last element in the list is 3.\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[3])\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nCumulative Sum (code puzzle)\n\n\n\n\n\n\nQuestion\n\n\n\nReorder and properly indent the lines of code below so that they print a list with the cumulative sum of data. The result should be [1, 3, 5, 10].\ncumulative.append(total)\nfor number in data:\ncumulative = []\ntotal = total + number\ntotal = 0\nprint(cumulative)\ndata = [1,2,2,5]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ntotal = 0\ndata = [1,2,2,5]\ncumulative = []\nfor number in data:\n    total = total + number\n    cumulative.append(total)\nprint(cumulative)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nIdentifying Variable Name Errors\n\n\n\n\n\n\nQuestion\n\n\n\nRead the code below and try to identify what the errors are without running it.\n\nRun the code and read the error message. What type of NameError do you think this is? Is it a string with no quotes, a misspelled variable, or a variable that should have been defined but was not?\nFix the error.\nRepeat steps 2 and 3, until you have fixed all the errors.\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (Number % 3) == 0:\n        message = message + a\n    else:\n        message = message + \"b\"\nprint(message)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nPython variable names are case sensitive: number and Number refer to different variables.\nThe variable message needs to be initialized as an empty string.\nWe want to add the string “a” to message, not the undefined variable a.\n\nmessage = \"\"\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + \"a\"\n    else:\n        message = message + \"b\"\nprint(message)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nClassifying Errors\n\n\n\n\n\n\nQuestion\n\n\n\nIs an indentation error a syntax error or a runtime error?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAn IndentationError is a syntax error. Programs with syntax errors cannot be started. A program with a runtime error will start but an error will be thrown under certain conditions.\n\n\n\n\n\nSource: Carpentries workshop materials."
  },
  {
    "objectID": "workshop-materials/py-intro/02-variables.html",
    "href": "workshop-materials/py-intro/02-variables.html",
    "title": "Exercises Lesson 2: Variables",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 2: Variables\n\n\nOrder of things\n\n\n\n\n\n\n\nQuestion A\n\n\n\nFill the table showing the values of the variables in this program after each statement is executed.\n# Command  # Value of x   # Value of y   # Value of swap #\nx = 1.0    #              #              #               #\ny = 3.0    #              #              #               #\nswap = x   #              #              #               #\nx = y      #              #              #               #\ny = swap   #              #              #               #\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n1\n-\n-\n\n\ny = 3.0\n1\n3\n-\n\n\nswap = x\n1\n3\n1\n\n\nx = y\n3\n3\n1\n\n\ny = swap\n3\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nWhat is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.)\ninitial = 'left'\nposition = initial\ninitial = 'right'\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n‘left’\n\n\n\n\n\n\nChallenge\n\n\n\n\n\n\nQuestion\n\n\n\nIf you assign a = 123, what happens if you try to get the second digit of a via a[1]?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNumbers are not strings or sequences and Python will raise an error if you try to perform an index operation on a number. In the next lesson on types and type conversion we will learn more about types and how to convert between different types. If you want the Nth digit of a number you can convert it into a string using the str built-in function and then perform an index operation on that string.\na = 123\nprint(a[1])\nTypeError: ‘int’ object is not subscriptable\na = str(123)\nprint(a[1])\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nSlicing\n\n\n\n\n\n\n\nQuestion A\n\n\n\nGiven the following string:\nspecies_name = \"Acacia buxifolia\"\nWhat would these expressions return?\n1. species_name[2:8]\n2. species_name[11:] (without a value after the colon)\n3. species_name[:4] (without a value before the colon)\n4. species_name[:] (just a colon)\n5. species_name[11:-3]\n6. species_name[-5:-3]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nspecies_name[2:8] returns the substring ‘acia b’\nspecies_name[11:] returns the substring ‘folia’, from position 11 until the end\nspecies_name[:4] returns the substring ‘Acac’, from the start up to but not including position 4\nspecies_name[:] returns the entire string ‘Acacia buxifolia’\nspecies_name[11:-3] returns the substring ‘fo’, from the 11th position to the third last position\nspecies_name[-5:-3] also returns the substring ‘fo’, from the fifth last position to the third last\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nWhat happens when you choose a stop value which is out of range? (i.e., try species_name[0:20] or species_name[:103])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf a part of the slice is out of range, the operation does not fail. species_name[0:20] gives the same result as species_name[0:], and species_name[:103] gives the same result as species_name[:].\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\nAdditional exercises\n\nNaming\n\n\n\n\n\n\nQuestion\n\n\n\nWhich is a better variable name, m, min, or minutes? Why? Hint: think about which code you would rather inherit from someone who is leaving the lab:\nts = m * 60 + s\ntot_sec = min * 60 + sec\ntotal_seconds = minutes * 60 + seconds\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nminutes is better because min might mean something like “minimum” (and actually is an existing built-in function in Python that we will cover later).\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nVery challenging additional exercises\n\n\n\n\n\n\nQuestion A\n\n\n\nTry to find out what the following code does. Use Google or chatGPT if you don’t know the answers.\nLists inside lists:\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ngreetings_strings is a list. Lists and strings can be accessed in a similar way.\nprint(greetings_strings[0]) prints the first element of the list greetings_strings, ie ‘hello’.\ngreetings_strings[1][2] will access the 2nd element of the list greetings_strings, ie ‘bye’. The 3rd element of ‘bye’ can then be accessed by [2]. Therefor, the answer is ‘e’.\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nUsing a loops will be covered latered, list comprehensions are even a bit more advanced. What does the following code do?\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n[greetings_strings[idx] for idx in [0, 2]] will generate a new list. It will make idx equal to 0 and 2, and create two corresponding entries in the new list. Those entries are defined by the code before the keyword for, in this case greetings_strings[idx]. The list will thus consist of greetings_strings[0] and greetings_strings[2], ie hello and later.\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]]) does the same, but the list now constists of greetings_strings[0][0], greetings_strings[0][2] and greetings_strings[0][4], so ['h', 'l', 'o'].\nrange(0,4,2) generates a range, and will result in idx to be set to the values 0, 2 (importantly, not including 4). The result thus is ['h', 'l'].\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nWhat does the following code do? (Types will be the topic of the next lesson.)\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nSimilar to above, in the first line, a list is created. **2 results in the square of number. number will take values 0..9, and so the list looks like [0, 1, 4, 9, 16, 25, 36, 49, 64, 81].\nstr() converts a number to a string. The list therefor now looks like ['0', '1', '4', '9', '16', '25', '36', '49', '64', '81'].\n\n\n\n\n\n\n\n\n\n\nQuestion D\n\n\n\nWhat do .join() and .replace() do? We will delve into this syntax later.\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\nprint(species_name.replace('Aca', 'Bole'))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n.join() pastes (joins) together elements of a list, and uses the string before the dot to put between the elements.\n.replace() performs replacement of text in the string given before the dot. Between the quotes are what to search for and what to replace it with, respectively.\n\n\n\n\n\n\n\n\n\n\nQuestion E\n\n\n\nNow combine the above to create a new list, where you remove all letters ‘e’ from the following list:\nlist_of_species = ['Homo sapiens', 'Escherichia coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can combine the above commands:\nnew_list = [X.replace('e','') for X in list_of_species]\nprint(new_list)\n# result: ['Homo sapins', 'Eschrichia coli', 'Pan troglodyts', 'Canis lupus', 'Flis catus']\n\n\n\n\n\nSource: bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/additional_exercises.html",
    "href": "workshop-materials/py-intro/additional_exercises.html",
    "title": "Additional exercises for Python Workshop",
    "section": "",
    "text": "Complementary to The Carpentries Python course\n\n\n\n\nWhat’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'"
  },
  {
    "objectID": "workshop-materials/py-intro/additional_exercises.html#lesson-2",
    "href": "workshop-materials/py-intro/additional_exercises.html#lesson-2",
    "title": "Additional exercises for Python Workshop",
    "section": "",
    "text": "What’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'"
  },
  {
    "objectID": "workshop-materials/py-intro/14-looping-over-data.html",
    "href": "workshop-materials/py-intro/14-looping-over-data.html",
    "title": "Exercises Lesson 14: Looping over data",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 14: Looping over data\n\n\nDetermining Matches\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of these files is not matched by the expression glob.glob('data/*as.csv')?\n1. data/gapminder_gdp_africa.csv\n2. data/gapminder_gdp_americas.csv\n3. data/gapminder_gdp_asia.csv\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1 is not matched by the glob.\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nMinimum File Size\n\n\n\n\n\n\nQuestion\n\n\n\nModify this program so that it prints the number of records in the file that has the fewest records.\nimport glob\nimport pandas as pd\nfewest = ____\nfor filename in glob.glob('data/*.csv'):\n    dataframe = pd.____(filename)\n    fewest = min(____, dataframe.shape[0])\nprint('smallest file has', fewest, 'records')\nNote that the DataFrame.shape() method returns a tuple with the number of rows and columns of the data frame.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport glob\nimport pandas as pd\nfewest = float('Inf')\nfor filename in glob.glob('data/*.csv'):\n    dataframe = pd.read_csv(filename)\n    fewest = min(fewest, dataframe.shape[0])\nprint('smallest file has', fewest, 'records')\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\nAdditional Exercises\n\n\nComparing Data\n\n\n\n\n\n\nQuestion\n\n\n\nWrite a program that reads in the regional data sets and plots the average GDP per capita for each region over time in a single chart.\nHint: Pandas will raise an error if it encounters non-numeric columns in a dataframe computation so you may need to either filter out those columns or tell pandas to ignore them.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,1, figsize=(10*cm_to_inch, 5*cm_to_inch))\nfor filename in glob.glob(LOCAL_PATH_GAPMINDER+'data/gapminder_gdp*.csv'):\n    # filename = glob.glob(LOCAL_PATH_GAPMINDER+'data/gapminder_gdp*.csv')[0]\n    dataframe = pd.read_csv(filename)\n    # extract &lt;region&gt; from the filename, expected to be in the format 'data/gapminder_gdp_&lt;region&gt;.csv'.\n    # we will split the string using the split method and `_` as our separator,\n    # retrieve the last string in the list that split returns (`&lt;region&gt;.csv`), \n    # and then remove the `.csv` extension from that string.\n    region = filename.split('_')[-1][:-4]\n\n    # pandas raises errors when it encounters non-numeric columns in a dataframe computation\n    # but we can tell pandas to ignore them with the `numeric_only` parameter\n    mean_values = dataframe.mean(numeric_only=True)\n    years       = pd.Series(mean_values.index.values).str.split('_').str[-1].astype(int)\n    \n    ax.plot(years, mean_values, label=region)\n    # NOTE: another way of doing this selects just the columns with gdp in their name using the filter method\n    # dataframe.filter(like=\"gdp\").mean().plot(ax=ax, label=region)\n# set the title and labels\nax.set_title('GDP Per Capita for Regions Over Time')\nax.set_xlabel('Year')\nplt.tight_layout()\nplt.legend()\n\n# Save the plot\nfig.savefig('plots_pyworkshop/L14_meanGDP_regions.png', dpi=600)\n\n\n\n\n\n\nSource: Carpentries workshop materials, adapted by the bioDSC."
  },
  {
    "objectID": "workshop-materials/py-intro/03-data-types.html",
    "href": "workshop-materials/py-intro/03-data-types.html",
    "title": "Exercises Lesson 3: Types",
    "section": "",
    "text": "Credits & copyright\n\n\n\nMany questions originate from the carpentry lesson “Plotting and Programming in Python”, some were authored by the bioDSC. Carpentries website materials are licenced under the CC BY 4.0, which also applies to the contents of this website.\n\n\n\n\nExercises Lesson 3: Types\n\n\nFractions\n\n\n\n\n\n\nQuestion\n\n\n\nWhat type of value is 3.4? How can you find out?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a floating-point number (often abbreviated “float”). It is possible to find out by using the built-in function type().\nprint(type(3.4))\n&lt;class 'float'&gt;\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nAutomatic type conversion\n\n\n\n\n\n\nQuestion\n\n\n\nWhat type of value is 3.25 + 4?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a float: integers are automatically converted to floats as necessary.\nresult = 3.25 + 4\nprint(result, 'is', type(result))\n7.25 is &lt;class 'float'&gt;\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nChoose a type\n\n\n\n\n\n\nQuestion\n\n\n\nWhat type of value (integer, floating point number, or character string) would you use to represent each of the following? Try to come up with more than one good answer for each problem. For example, in # 1, when would counting days with a floating point variable make more sense than using an integer?\n\nNumber of days since the start of the year.\nTime elapsed from the start of the year until now in days.\nSerial number of a piece of lab equipment.\nA lab specimen’s age\nCurrent population of a city.\nAverage population of a city over time.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answers to the questions are:\n\nInteger, since the number of days would lie between 1 and 365.\nFloating point, since fractional days are required\nCharacter string if serial number contains letters and numbers, otherwise integer if the serial number consists only of numerals\nThis will vary! How do you define a specimen’s age? whole days since collection (integer)? date and time (string)?\nChoose floating point to represent population as large aggregates (eg millions), or integer to represent population in units of individuals.\nFloating point number, since an average is likely to have a fractional part.\n\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nFloat or integer?\n\n\n\n\n\n\nQuestion\n\n\n\nWhy wouldn’t you always use floats, and never use integers?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBoth are designed for specific use cases.\nA computer can work more efficiently with integer numbers, so if you know you only need integer numbers, use the integer type.\nSometimes a Python functionality also requires you to work with integers, mostly because those are compatible with specific tasks the computer will perform, whilst floats are not.\nFor many/most practical purposes, this is dealt with automatically by Python.\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nStrings to numbers\nWhere reasonable, float() will convert a string to a floating point number, and int() will convert a floating point number to an integer:\nprint(\"string to float:\", float(\"3.4\"))\nprint(\"float to int:\", int(3.4))\nOUTPUT:\nstring to float: 3.4\nfloat to int: 3\nIf the conversion doesn’t make sense, however, an error message will occur.\nprint(\"string to float:\", float(\"Hello world!\"))\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat do you expect the following program to do?\nWhat does it actually do?\nWhy do you think it does that?\n\nprint(\"fractional string to int:\", int(\"3.4\"))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWhat do you expect this program to do? It would not be so unreasonable to expect the Python 3 int command to convert the string “3.4” to 3.4 and an additional type conversion to 3. After all, Python 3 performs a lot of other magic - isn’t that part of its charm?\nHowever, Python 3 throws an error.\n\nprint(int(\"3.4\"))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: invalid literal for int() with base 10: '3.4'\n\nWhy? To be consistent, possibly. If you ask Python to perform two consecutive typecasts, you must convert it explicitly in code.\n\nprint(int(float(\"3.4\")))\n3\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nArithmetic with Different Types\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of the following will return the floating point number 2.0? Note: there may be more than one right answer.\nfirst = 1.0\nsecond = \"1\"\nthird = \"1.1\"\n\nfirst + float(second)\nfloat(second) + float(third)\nfirst + int(third)\nfirst + int(float(third))\nint(first) + int(float(third))\n2.0 * second\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n2.0\n2.1\nValueError\n2.0\n2\nTypeError\n\nSo, 1 and 4\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\n\n\n\nAdditional exercises\n\n\nLists\nWe’ll cover lists later in lesson 11, but let’s already take a brief look.\nA list is a series of elements bound together, where each element can have a value. They are defined as follows:\nnumbers = [1,2,3]\nfruits = ['apples', 'pears', 'oranges']\nphysical_constants = ['pi', 3.14, 'c', 299_792_458, 'mole', 6.022e23]\nElements can be accessed the same way as we saw with strings.\n\n\n\n\n\n\nQuestion A\n\n\n\nWhat will numbers[1] return? And physical_constants[2:4]?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nnumbers[1] will return 2 because in Python indexing vectors start counting at 0.\nnumbers[2:4] returns ['c',299792458] because indexing starts at the 3rd position (=2+1) and elements are selected up to but not including the 5th element (which has index 4).\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nWhat is the type of\n\nnumbers?\nnumbers[1]?\nphysical_constants?\nphysical_constants[1]?\nphysical_constants[2]?\nphysical_constants[3]?\nfruits?\nfruits[1]?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntype(numbers) &lt;class 'list'&gt;\ntype(numbers[1]) &lt;class 'int'&gt;\ntype(physical_constants) &lt;class 'list'&gt;\ntype(physical_constants[1]) &lt;class 'float'&gt;\ntype(physical_constants[2]) &lt;class 'str'&gt;\ntype(physical_constants[3]) &lt;class 'int'&gt;\ntype(fruits) &lt;class 'list'&gt;\ntype(fruits[1]) &lt;class 'str'&gt;\n\nSo list, integer, list, float, string, integer, list and string, respectively.\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nCan the elements in a list have different types? (This can be seen from the previous answer.)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes, a single list may contain numbers, strings, and anything else.\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nnp.array\nLists can be a useful tool, but for example in image analysis, don’t offer the full mathematical options one might like. numpy arrays introduce a new type of series, in which you can do more manipulations. See some examples below:\nimport numpy as np\nmy_array = [1,2,3,5] # This is a normal Python list\nmy_array_np = np.array([1,2,3,5]) # This is a numpy array!\n\n\n\n\n\n\nQuestion A\n\n\n\nLet’s perform the same + 1 operation, to both the Python list and the numpy array. What’s the difference here?\n\nmy_array + 1\nmy_array_np + 1\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array+1, throws an error message because there is no operator defined that accepts a list type and an int type. If you want to concatenate the two, use my_array+[1]\nmy_array_np+1, returns array([2, 3, 4, 6]), an np.array where every element from my_array_np has been increased by 1.\n\nThe effect of the + operator thus changes with the type of the parameters it is used for. With numpy arrays, you can work more mathematically.\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nAgain, we perform the same operation, this time * 3, to the Python list and the numpy array. What’s the difference here?\n\nmy_array * 3\nmy_array_np * 3\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array * 3, returns [1, 2, 3, 5, 1, 2, 3, 5, 1, 2, 3, 5] because the * operator repeats the list 3 times\nmy_array_np * 3, returns array([3, 6, 9, 15]) because in numpy, the * multplies the value of each element 3 times\n\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nWhat will happen here?\n\nmy_array + [1,2,3,4]\nmy_array_np + np.array([1, 2, 3, 4])\nnp.sin(my_array_np)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array + [1,2,3,4], returns a concatenated list [1, 2, 3, 5, 1, 2, 3, 4]. Like in question B, the + operator concatenates two lists.\nmy_array_np + np.array([1, 2, 3, 4]), returns the element wise sum of both vectors, so array([2, 4 ,6 ,9])\nnp.sin(my_array_np), returns the result of applying the sin function to each element in my_array_np, array([ 0.84147098,  0.90929743,  0.14112001, -0.95892427])\n\n\n\n\n\n\n\n\n\n\nQuestion D\n\n\n\nWhat is happening in these numpy-specific operations?\n\nmy_array_np[range(1,4,2)]\nmy_array_np[my_array_np&gt;1]\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array_np[range(1,4,2)], returns array([2,5]), the second and fourth element of my_array_np. range(1, 4, 2) generates numbers from 1 to 4 (not including 4) with a stepsize of 2 (1 and 3 in this case).\nmy_array_np[my_array_np&gt;1], returns array([2,3,5]), the my_array_np&gt;1 clause allows you to select those elements for which this equation is true (we’ll delve into this more later). Respectively\n\n1&gt;1 is false,\n2&gt;1 is true,\n3&gt;1 is true,\n5&gt;1 is true,\nhence respectively elements 2 to 4 of my_array_np are selected.\n\n\n\n\n\n\n\n\n\n\n\nQuestion E\n\n\n\nCan numpy arrays have different types? What is the type of the elements in these two arrays?\n\nnp.array([1,2,3,'4'])\nnp.array([1,2,3,'hello'])\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nnp.array([1,2,3,'4']), returns array(['1', '2', '3', '4'], dtype='&lt;U21')\nnp.array([1,2,3,'hello']), returns array(['1', '2', '3', 'hello'], dtype='&lt;U21')\n\nThe elements of a NumPy array must all be of the same type, whereas the elements of a Python list can be of completely different types. Automatic conversion occurs to convert all elements to the same type (in this case string takes presedence).\nYou can access the type of elements in a numpy array as follows:\nmy_array = np.array([1,2,3,'hello'])\nmy_array.dtype\n'&lt;U21' refers to a specific kind of string.\nAlternatively:\ntype(my_array[0])\nwill also show it’s a numpy string type (numpy.str_).\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nDict(ionary)\nPython has more types. A dict is sometimes very convenient, and is also used later, when creating tables. We create a simple dict first:\nexperimental_replicate_list = {'WT': 12, 'mut': 32, 'WT.cond1': 10, 'mut.cond1': 12}\nprint(experimental_replicate_list)\nprint(experimental_replicate_list['WT'])\n\n\n\n\n\n\nQuestion A\n\n\n\nEdit line three of following code such that we get the associated numbers for conditions involving WT\nmy_keys = experimental_replicate_list.keys()\nprint(my_keys)\nmy_keys_of_interest = [the_key for the_key in my_keys if 'mut' in the_key] # Edit this line\nprint(my_keys_of_interest)\nprint([experimental_replicate_list[sel_key] for sel_key in my_keys_of_interest])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_keys_of_interest = [the_key for the_key in my_keys if 'WT' in the_key]\nprint(my_keys_of_interest), returns [‘WT’, ‘WT.cond1’]\nprint([experimental_replicate_list[sel_key] for sel_key in my_keys_of_interest]), returns [12,10]\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nThe above code uses several lines, can you do the same operations in one line?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nprint([experimental_replicate_list[key] for key in experimental_replicate_list.keys() if 'WT' in key])\n\n\n\n\n\nSource: bioDSC.\n\n\n\n\nSpecial maths\n\n\n\n\n\n\nQuestion\n\n\n\nIn Python 3, the // operator performs integer (whole-number) floor division, the / operator performs floating-point division, and the % (or modulo) operator calculates and returns the remainder from integer division:\nprint('5 // 3:', 5 // 3)\nprint('5 / 3:', 5 / 3)\nprint('5 % 3:', 5 % 3)\nOUTPUT:\n5 // 3: 1\n5 / 3: 1.6666666666666667\n5 % 3: 2\nIf num_subjects is the number of subjects taking part in a study, and num_per_survey is the number that can take part in a single survey, write an expression that calculates the number of surveys needed to reach everyone once.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe want the minimum number of surveys that reaches everyone once, which is the rounded up value of num_subjects/ num_per_survey. This is equivalent to performing a floor division with // and adding 1. Before the division we need to subtract 1 from the number of subjects to deal with the case where num_subjects is evenly divisible by num_per_survey.\nnum_subjects = 600\nnum_per_survey = 42\nnum_surveys = (num_subjects - 1) // num_per_survey + 1\n\nprint(num_subjects, 'subjects,', num_per_survey, 'per survey:', num_surveys)\n\n\n\n\n\nSource: Carpentries workshop materials.\n\n\n\n\nComplex numbers\n\n\n\n\n\n\nQuestion\n\n\n\nPython provides complex numbers, which are written as 1.0+2.0j. If val is a complex number, its real and imaginary parts can be accessed using dot notation as val.real and val.imag.\na_complex_number = 6 + 2j\nprint(a_complex_number.real)\nprint(a_complex_number.imag)\n\nWhy do you think Python uses j instead of i for the imaginary part?\nWhat do you expect 1 + 2j + 3 to produce?\nWhat do you expect 4j to be? What about 4 j or 4 + j?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nPython uses the symbol ‘j’ for the imaginary part of complex numbers instead of ‘i’ to prevent confusion with electric current, which is represented by ‘i’ in electrical engineering. This choice caters to the professional background of many Python developers.\n4 + 2j, the real parts can be added and the imaginary part remains the same\n4j is 4 times the complex number j (4*1j), 4 j will generate a syntax error, it’s missing an operator. 4 + j will treat j as a variable and try to take the sum of both but throws an error if j is not defined or of the wrong type.\n\n\n\n\n\n\nSource: Carpentries workshop materials."
  },
  {
    "objectID": "workshop-materials/py-intro-notes-hidden/lesson-07-dataframes-1.html",
    "href": "workshop-materials/py-intro-notes-hidden/lesson-07-dataframes-1.html",
    "title": "Lesson 7, Pandas dataframes (1)",
    "section": "",
    "text": "Copyright information\n\n\n\nThese are notes based on the Carpentries materials (licenced under the CC BY 4.0). These notes are intended to present the materials related to Dataframes (part 1). \n\n\n\nLesson 7, Pandas dataframes (1)\n# Pandas is statistics library\n    # works with tabular data\n    # very similar to R's dataframes\n        # (\"borrows features\")\n        \n# Dataframe\n    # 2D table\n        # columns have names\n        # columns might have different types\n        \n# we will read in a datafile from .csv format\n    # check out what's in the file\n    \nimport pandas as pd\n\n# data_oceania = pd.read_csv('../data/gapminder_gdp_oceania.csv')\ndata_oceania = pd.read_csv('/Users/m.wehrens/Data_UVA/2024_teaching/2025-03-gapminder/data/gapminder_gdp_oceania.csv')\nprint(data_oceania)\n    # Note: \"\\\" is used when too wide for screen\n    # Note: use convenient names, \n        # Prevent accidental mixups\n    \n# Typical for dataframe:\n    # Columns are observed variables\n    # Rows are observations\n\n# Currently, 1st column is country name\n    # want to make that row name\n    # set parameter \"index_col\" \n    \ndata_oceania_country = pd.read_csv('/Users/m.wehrens/Data_UVA/2024_teaching/2025-03-gapminder/data/gapminder_gdp_oceania.csv',\\\n     index_col='country')\n     \n     # index_col should be column name as a string\n     \nprint(data_oceania_country)\n\n# convenient methods:\n\ndata_oceania_country.info()\n    # Gives summary of df\n        # two rows; \"Australia\" and \"New Zealand\"\n        # twelve columns, \n            # also their types are listed\n            # (null count talked about later)\n            # memory usage also listed\n# DataFrame.columns\n    # stores information about columns\n    # this is a PARAMETER\n        # similar to math.pi\n\ndata_oceania_country.columns\n\n# don't do:\n# data_oceania_country.columns()\n\n# .columns is called \"member variable\" or \"member\"\n\n# Transpose\n    # what if we want to swap the x,y axes (columns and rows)?\n        # use:\n\ndata_oceania_country.T\n    # (for all intents and purposes) member variable\n# to get summary statistics\n\ndata_oceania_country.describe()\n    # includes all columns with numerical data\n    # not useful 2 rows, but very when many"
  },
  {
    "objectID": "workshop-materials/py-intro-adaptedlessons/lesson-09-plotting.html",
    "href": "workshop-materials/py-intro-adaptedlessons/lesson-09-plotting.html",
    "title": "Lesson 9, Plotting",
    "section": "",
    "text": "Copyright information\n\n\n\nThese are notes based on the Carpentries materials (licenced under the CC BY 4.0). These notes are intended to present the materials related to Dataframes (part2). The contents of the lessons were modified by Martijn Wehrens (bioDSC).\nThis material was modified to include matplotlib.pyplot and seaborn, two commonly used tools, and exclude the pandas.DataFrame.plot, which we think is less commonly used.\n\n\n\nLesson 9, Plotting\n# Plotting \n\n# matplotlib most widely used\n    # or sub library \"pyplot\"\n    # will render inline by default\n\nimport matplotlib.pyplot as plt\n\n# simple plot\n\ntime = [0, 1, 2, 3]\nposition = [0, 100, 200, 300]\n\nplt.plot(time, position)\nplt.xlabel('Time (hr)')\nplt.ylabel('Position (km)')\n# when running python differently\n    # e.g. terminal\n    # also need \"plt.show()\" \n        # and also plt.close()\n# Seaborn\n\n# Can also use \"seaborn\"\n    # Based on matplotlib\n        # Usually used together\n    # Additional features geared towards dfs\n\n# How to install seaborn in Jupyter\n%pip install seaborn\n    # only needed in online version\n    # done differently locally\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('/Users/m.wehrens/Data_UVA/2024_teaching/2025-03-gapminder/data/gapminder_gdp_oceania.csv', index_col='country')\n\ndata.head()\n\nsns.lineplot(x=data.columns, y = data.loc['Australia',:])\n# plt.show(); plt.close()\n\n# why do we see ugly axis?\n    # we'll do something about this later\n# more convenient:\n    # dfs as input directly\n    # certain expecations from the data\n    # series to plot should be in columns\n    \n    # re-arrange our df\n        \ndata_transposed = data.T\ndata_transposed['Year'] = data.columns\ndata_transposed.head()\n\nsns.lineplot(data_transposed, x='Year', y = 'Australia')\nplt.show(); plt.close()\n# more re-arrangements needed for full options\n\n# \"long format\"\n\n    # column = variable or condition\n    # row = measurement\n    \n    # Current format:\n    # country           Australia  New Zealand            Year\n    # gdpPercap_1952  10039.59564  10556.57566  gdpPercap_1952\n    # gdpPercap_1957  10949.64959  12247.39532  gdpPercap_1957\n    # gdpPercap_1962  12217.22686  13175.67800  gdpPercap_1962\n    # gdpPercap_1967  14526.12465  14463.91893  gdpPercap_1967\n    # gdpPercap_1972  16788.62948  16046.03728  gdpPercap_1972\n    # (..)\n    #\n    # Goal:\n    #             Year      Country          GDP\n    # 0   gdpPercap_1952    Australia  10039.59564\n    # 1   gdpPercap_1957    Australia  10949.64959\n    # 2   gdpPercap_1962    Australia  12217.22686\n    # 3   gdpPercap_1967    Australia  14526.12465\n    # 4   gdpPercap_1972    Australia  16788.62948\n    # 5   gdpPercap_1977    Australia  18334.19751\n    # 6   gdpPercap_1982    Australia  19477.00928\n    # 7   gdpPercap_1987    Australia  21888.88903\n    # 8   gdpPercap_1992    Australia  23424.76683\n    # 9   gdpPercap_1997    Australia  26997.93657\n    # 10  gdpPercap_2002    Australia  30687.75473\n    # 11  gdpPercap_2007    Australia  34435.36744\n    # 12  gdpPercap_1952  New Zealand  10556.57566\n    # (..)\n\ndata_long = data_transposed.melt(id_vars='Year', var_name='Country', value_name='GDP')\n    # id_vars: identifier variables\n    #   identifying a specific observation\n    #   keep those\n    #       new id vars will be added, in this case country\n    # var_name: name used the new id parameter\n        # here, column names Australia, New Zealand ---&gt; these are countries\n    # value_name: name given to the values, which are taken from multiple columns\n    # \n    #    - Identifier variables are called this because they uniquely **identify a specific observation** in the dataset. \n    #      These variables are not measured or calculated but instead serve to distinguish one observation from another.\n    #    - In this example, `Year` and `Country` are identifier variables because they uniquely identify each observation \n    #      (e.g., GDP of Australia in 1952).\n\nprint(data_long.head())\n# OPTIONAL ##########\n\ndata2 = data.copy()\ndata2['Country']=data2.index\ndata_long2 = data2.melt(id_vars='Country', var_name='Year', value_name='GDP')\n\n    # Let's not go into technicalities\n        # sometimes a = b\n            # a \"reference\" is made\n            # instead of new parameter\n    # list1 = [1,2,3]\n    # list2 = list1\n    # list2[1] = 44\n    # list1\n    # list2\n    \n# END OPTIONAL ##########\nsns.lineplot(data_long, x='Year', y = 'GDP', hue='Country')\n# plt.show(); plt.close()\n# Finally\n    # Fix years\n\n# search and replace\n    # keep year\n    # remove rest\n\nyears = data_long['Year'].str.replace('gdpPercap_', '')\n    # MW:\n    # \"str\" is a method which holds more methods \n        # related to string operations\n        # str is also part of python standard library\n        # stand-alone example: str.replace('hoi', 'oi', '') \n\n# convert to int\n# put back in dataframe\ndata_long['Year'] = years.astype(int)\n\n# Plot again\nsns.lineplot(data_long, x='Year', y = 'GDP', hue='Country')\nplt.show(); plt.close()\n# Change\n    # Plot types\n    # Style\n    # Seaborn can be manipulated by matplotlib    \n# More options\n\n# Example\nplt.style.use('ggplot')\n\n# Other type, bars\nsns.barplot(data_long, x='Year', y = 'GDP', hue='Country')\n\n# use google to tweak\n# for nice examples:\n    # https://matplotlib.org/stable/gallery/ \n    # https://seaborn.pydata.org/examples/index.html\n# Another plot\n\nplt.style.use('default')\nsns.violinplot(data_long, x='Country', y = 'GDP')\n# OPTIONAL:\n# sns.stripplot(data_long, x='Country', y = 'GDP', hue='Year')\nplt.xticks(rotation=45)\nplt.title('GDPs of different countries')\n# Calling matplotlib.pyplot directly\n\n# plt.plot(x, y) like above\n    # also can choose different plot styles and tweak features\n        # e.g.\n\nselected_rows = data_long['Country']=='Australia'\nyears = data_long.loc[selected_rows,'Year']\ngdp_australia = data_long.loc[selected_rows,'GDP']\n\nplt.plot(years, gdp_australia, linestyle='--', color='g')\n\n# we can also use a shorthand argument:\n# plt.plot(years, gdp_australia, fmt='g--')\n\n# more brief\n# plt.plot(years, gdp_australia, 'g--')\n# combining data\n\n# Select two countries' worth of data.\n\ngdp_nz = data_long.loc[data_long['Country']=='New Zealand','GDP']\n\n# Plot with differently-colored markers.\nplt.plot(years, gdp_australia, 'b-', label='Australia')\nplt.plot(years, gdp_nz, 'g-', label='New Zealand')\n\n# Create legend.\n#plt.legend()\nplt.legend(loc='lower right') \nplt.xlabel('Year')\nplt.ylabel('GDP per capita ($)')\n\n# about legend, \n    # 2 stages to create:\n        # \"label\" argument to label each set of data\n        # create legend\n            # plt.legend()\n    # legend placement\n        # can use \"loc\" argument \n            # per default, tries to find good position\n                # (show this)\n# scatter plot\n\n# other style\n# plt.scatter(gdp_australia, gdp_nz)\n\n# can be done directly from dataframe also\nsns.scatterplot(data_transposed, x = 'Australia', y = 'New Zealand')\n# To save plot\n# PARTS OF THIS ARE OPTIONAL\n\n# plt.savefig('my_figure.png')\n\n    # note MW: pdf or svg very convenient\n    # note MW: also convenient:\n        # plt.tight_layout() and \n        # plt.savefig('blabla.pdf', bbox_inches='tight')\n\n# plt.savefig will use latest figure that was last displayed\n\n    # either make sure you have the right one\n    # or\n    # data.plot(kind='bar')\n    # fig = plt.gcf() # get current figure\n    # fig.savefig('my_figure.png')\n\n# good practice\n\n    # make text large enough in powerpoints\n         # use  fontsize parameter in xlabel, ylabel, title, and legend, and tick_params with labelsize\n    # make symbols readable\n        # ie large enough; \"s\" parameter to increase size\n\n    # be careful with colors (only) to distinguish lines\n        # use color-blind friendly palette \n            # color-blind emulators:\n                # https://www.color-blindness.com/coblis-color-blindness-simulator/\n                # https://colororacle.org/\n        # linestyle to counter black-white printing, or marker for scatter plots\n# Good practice example:\n\nimport matplotlib.pyplot as plt\n\n# Bang Wong colorblind-friendly color scheme (https://www.nature.com/articles/nmeth.1618)\ncolors_bangwong = [\n    \"#E69F00\",  # Orange\n    \"#56B4E9\",  # Sky Blue\n    \"#009E73\",  # Bluish Green\n    \"#F0E442\",  # Yellow\n    \"#0072B2\",  # Blue\n    \"#D55E00\",  # Vermillion\n    \"#CC79A7\",  # Reddish Purple\n    \"#000000\"   # Black\n]\n\nplt.style.use('default')\nfig, ax = plt.subplots(1,1, figsize=(10/2.54,10/2.54))\nax.plot([1,2,3,4], [1,4,9,16], linestyle='--', color=colors_bangwong[1], label=r'$x^2$')\nax.plot([1,2,3,4], [1,5,11,19], linestyle=':', color=colors_bangwong[2], label=r'$x^2+(x-1)$')\nax.legend()\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_title('Sample Plot', fontsize=12)\nax.legend(fontsize=12)\nax.tick_params(axis='both', which='major', labelsize=12)\nplt.tight_layout()\n# plt.show()\n# plt.savefig('/Users/m.wehrens/Desktop/202503_Python-Gapminder-testingcode.pdf', dpi=300, bbox_inches='tight')\n# plt.close(fig)"
  },
  {
    "objectID": "posts/Ath_synteny.html",
    "href": "posts/Ath_synteny.html",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "",
    "text": "Interesting phenotypes can be encoded in the genome of organisms. Therefore, it is often interesting to visualize differences between genomes of different species, or of individuals within the same species. These differences could be small, such as single nucleotide polymorphisms, but can also be structural rearrangements of entire chromosomes.\nIn a recent publication, Lian et al 2024 Nature Genetics investigated structural rearrangements in a panel of 69 Arabidopsis thaliana accessions (Fig 3). In addition, they published a chromosome level assembly of Arabidopsis accession Oy-0. This accession is used in several research groups of the University of Amsterdam, so it is interesting to show genome rearrangements between this accession, and the model accession Col-0. The paper uses plotsr software to plot synteny across chromosomes. The syntenic regions are first identified using syri.\nSoftware used:\n\nsyri, a software package to identify structural rearrangements.\nplotsr, a software package to plot the rearrangements detected by syri.\nminimap2, a fast aligner, used here to align the two genomes of interest."
  },
  {
    "objectID": "posts/Ath_synteny.html#introduction",
    "href": "posts/Ath_synteny.html#introduction",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "",
    "text": "Interesting phenotypes can be encoded in the genome of organisms. Therefore, it is often interesting to visualize differences between genomes of different species, or of individuals within the same species. These differences could be small, such as single nucleotide polymorphisms, but can also be structural rearrangements of entire chromosomes.\nIn a recent publication, Lian et al 2024 Nature Genetics investigated structural rearrangements in a panel of 69 Arabidopsis thaliana accessions (Fig 3). In addition, they published a chromosome level assembly of Arabidopsis accession Oy-0. This accession is used in several research groups of the University of Amsterdam, so it is interesting to show genome rearrangements between this accession, and the model accession Col-0. The paper uses plotsr software to plot synteny across chromosomes. The syntenic regions are first identified using syri.\nSoftware used:\n\nsyri, a software package to identify structural rearrangements.\nplotsr, a software package to plot the rearrangements detected by syri.\nminimap2, a fast aligner, used here to align the two genomes of interest."
  },
  {
    "objectID": "posts/Ath_synteny.html#the-data",
    "href": "posts/Ath_synteny.html#the-data",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "The data",
    "text": "The data\nI downloaded the Arabidopsis Col-0 TAIR10.1 and Oy-0 genome assemblies with the following script, moved them to a folder called data, and unzipped them:\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat download_genoomes.sh \nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/036/927/085/GCA_036927085.1_ASM3692708v1/GCA_036927085.1_ASM3692708v1_genomic.fna.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.fna.gz\n\nmv GC* data/\ncd data/\ngunzip *.gz\nThe synteny software expects that homologous chromosomes in the two genomes have exactly the same chromosome id. Unfortunately, this is almost never the case if you download genomes from different organisms and from different sources. To rename the fasta headers, I used the following sed commands. In addition, I removed the mitochondrial and chromosomal DNA sequences.\n(syri) [mpaauw@omics-h0 data]$ cat chromosome_renamer.sh \nsed -i 's/CM072627.1.*chromosome 1.*/Chr1/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072628.1.*chromosome 2.*/Chr2/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072629.1.*chromosome 3.*/Chr3/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072630.1.*chromosome 4.*/Chr4/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072631.1.*chromosome 5.*/Chr5/' GCA_036927085.1_ASM3692708v1_genomic.fna\n\nsed -i 's/NC_003070.9.*chromosome 1.*/Chr1/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003071.7.*chromosome 2.*/Chr2/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003074.8.*chromosome 3.*/Chr3/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003075.7.*chromosome 4.*/Chr4/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003076.8.*chromosome 5.*/Chr5/' GCF_000001735.4_TAIR10.1_genomic.fna\n\n(syri) [mpaauw@omics-h0 data]$ cat contig_remover.sh \nsed -i '/&gt;JAWQTX010000006.1/,/^&gt;/d' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i '/&gt;JAWQTX010000007.1/,/^&gt;/d' GCA_036927085.1_ASM3692708v1_genomic.fna\n\nsed -i '/&gt;NC_037304.1/,/^&gt;/d' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i '/&gt;NC_000932.1/,/^&gt;/d' GCF_000001735.4_TAIR10.1_genomic.fna\nLet’s do sanity check: did this work? We do this by using grep, a sophisticated command line search tool, to search for the character &gt;, in all files that are called G*.fna. Note that the * is used as a ‘wildcard’ and we match both genomes with this statement.\n(syri) [mpaauw@omics-h0 data]$ grep \"&gt;\" G*.fna\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr1\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr2\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr3\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr4\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr5\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr1\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr2\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr3\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr4\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr5\nThat looks good. In both genomes, the chromosomes are called Chr1, and so forth."
  },
  {
    "objectID": "posts/Ath_synteny.html#aligning-the-genomes",
    "href": "posts/Ath_synteny.html#aligning-the-genomes",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Aligning the genomes",
    "text": "Aligning the genomes\nThe first step is to do a whole-genome alignment between the two genomes. I followed the instructions at plotsr github to do this. We use the software minimap2. This is a quick sequence alignment program and is in installed in base environment on Crunchomics. The alignments are sorted and indexed using samtools. Then syri is used to call structural variants based on the alignment between the two genomes.\nminimap2 -ax asm5 -t 16 --eqx data/GCF_000001735.4_TAIR10.1_genomic.fna data/GCA_036927085.1_ASM3692708v1_genomic.fna | samtools sort -O BAM &gt; Col_Oy.bam\nsamtools index Col_Oy.bam\nsyri -c Col_Oy.bam -r data/GCF_000001735.4_TAIR10.1_genomic.fna -q data/GCA_036927085.1_ASM3692708v1_genomic.fna -F B --prefix Col_Oy\nThis produces several output files:\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ ls Col_Oysyri.*\nCol_Oysyri.log  Col_Oysyri.out  Col_Oysyri.summary  Col_Oysyri.vcf\nLet’s have a look at the summary.\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat Col_Oysyri.summary \n#Structural annotations\n#Variation_type Count   Length_ref  Length_qry\nSyntenic regions    95  107247075   107294458\nInversions  14  1504073 2295146\nTranslocations  110 566651  566464\nDuplications (reference)    62  267859  -\nDuplications (query)    528 -   898113\nNot aligned (reference) 250 9765159 -\nNot aligned (query) 696 -   20371617\n\n\n#Sequence annotations\n#Variation_type Count   Length_ref  Length_qry\nSNPs    405790  405790  405790\nInsertions  42410   -   904020\nDeletions   41893   867402  -\nCopygains   26  -   485440\nCopylosses  11  15494   -\nHighly diverged 3961    18041314    18376872\nTandem repeats  2   518 599\nOk, so there are quite a few structural variants between Col-0 and Oy-0! The syri.out file contains the details of all the variants individually."
  },
  {
    "objectID": "posts/Ath_synteny.html#plotting-the-structural-variants",
    "href": "posts/Ath_synteny.html#plotting-the-structural-variants",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Plotting the structural variants",
    "text": "Plotting the structural variants\nWe can then go ahead and visualize the structural variants using the plotsr software. First we create the genomes.txt file containing info about where the software can find both genomes, how to call them, and specify how they should be plotted.\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat genomes.txt \n#file   name    tags\ndata/GCF_000001735.4_TAIR10.1_genomic.fna   Col lw:1.5\ndata/GCA_036927085.1_ASM3692708v1_genomic.fna   Oy  lw:1.5\n\n# then we run the software: all chromosomes\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ plotsr --sr Col_Oysyri.out --genomes genomes.txt -o all_chromosomes.png\n\n# or just chromosome 4\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ plotsr --sr Col_Oysyri.out --genomes genomes.txt --chr Chr4 -o chr4.png\n\nSo, while most of the Chr 4 of the two accessions are syntenic, we can find two inversions, a translocation, and some parts that don’t have a match at all at the chromosome of the other accession."
  },
  {
    "objectID": "posts/Ath_synteny.html#adding-tracks-with-other-data",
    "href": "posts/Ath_synteny.html#adding-tracks-with-other-data",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Adding tracks with other data",
    "text": "Adding tracks with other data\nYou can add tracks of data, such as gene annotations, or SNP density, across the genomes. Perhaps you can then discover interesting genomic features that colocalize with the structural variant breakpoints. Here, we have a dataset of short sequences (k-mers) that were mapped to the Col-0 reference genome using the mapping software bowtie. We need to transform the .bam file with the mappings to a .bed file. Then, we need to add “Chr” to the chromosome identifiers in the bed file.\nbedtools bamtobed -i mapping.sorted.bam &gt; mapping.sorted.bed\nawk '{print \"Chr\"$1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $4}' mapping.sorted.bed &gt; mapping.sorted.chr.bed\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ head mapping.sorted.chr.bed \nChr1    753215  753246  sequence_1703\nChr1    753216  753247  sequence_761\nChr1    753217  753248  sequence_1521\nLet’s continue by generating a tracks.txt file, similar to the genomes.txt file to tell plotsr where to find the track data, how to call the track, and some graphical settings of the track.\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat tracks.txt \n#file   name    tags\nmapping.sorted.chr.bed    sig_kmers   bw:10000;nc:black;ns:8;lc:sienna;lw:1;bc:peachpuff;ba:0.7\n\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ plotsr --sr Col_Oysyri.out --genomes genomes.txt --tracks tracks.txt --chr Chr5 -W 6 -o Chr5_kmertrack.png\n\nCool! In this case, there is no clear association between our mapped k-mers and structural variation between Col-0 and Oy-0 on this chromosome."
  },
  {
    "objectID": "posts/Ath_synteny.html#further-reading",
    "href": "posts/Ath_synteny.html#further-reading",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Further reading",
    "text": "Further reading\n\nsyri, a software package to identify structural rearrangements.\nplotsr, a software package to plot the rearrangements detected by syri.\nLian et al 2024 Nature Genetics.\nclinker, a versatile synteny visualization tool that works on smaller scales (gene clusters)."
  },
  {
    "objectID": "posts/introduction.html",
    "href": "posts/introduction.html",
    "title": "The bioDSC blog: first post",
    "section": "",
    "text": "We want to use this blog to share, every now and then, what’s on our minds. For example, let’s say we read a few interesting papers on a new bioinformatics technique. Sometimes we install the tools, to see if we can get it to work on our own data, or on a small test dataset. It might not develop into a full tutorial or protocol, but instead of keeping it to ourselves, we will share it here on this blog. Who knows, maybe it’s useful for someone in the future!\nTopics we aim to cover:\n\nBioinformatics, from RNA-seq to comparative genomics and anything beyond that\nData visualization, focussing on the ggplot universe in R\nCrunchomics, tips and tricks to use the high-performance compute cluster of the University of Amsterdam\nData management and archiving\n\nDo not hesitate to send us an email if you have ideas for new topics, or if you want to contribute to the blog by co-authoring a post!"
  },
  {
    "objectID": "posts/introduction.html#why-a-blog",
    "href": "posts/introduction.html#why-a-blog",
    "title": "The bioDSC blog: first post",
    "section": "",
    "text": "We want to use this blog to share, every now and then, what’s on our minds. For example, let’s say we read a few interesting papers on a new bioinformatics technique. Sometimes we install the tools, to see if we can get it to work on our own data, or on a small test dataset. It might not develop into a full tutorial or protocol, but instead of keeping it to ourselves, we will share it here on this blog. Who knows, maybe it’s useful for someone in the future!\nTopics we aim to cover:\n\nBioinformatics, from RNA-seq to comparative genomics and anything beyond that\nData visualization, focussing on the ggplot universe in R\nCrunchomics, tips and tricks to use the high-performance compute cluster of the University of Amsterdam\nData management and archiving\n\nDo not hesitate to send us an email if you have ideas for new topics, or if you want to contribute to the blog by co-authoring a post!"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops & events",
    "section": "",
    "text": "Get introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nMay 19, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html#upcoming-workshops",
    "href": "workshops.html#upcoming-workshops",
    "title": "Workshops & events",
    "section": "",
    "text": "Get introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nMay 19, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html#stay-tuned",
    "href": "workshops.html#stay-tuned",
    "title": "Workshops & events",
    "section": "Stay tuned",
    "text": "Stay tuned\n\n\n\n\n\nList of past and upcoming workshops\n\n\n\n\n\nWorkshops we are currently developing that are coming soon.\n\n\n\n\n\nMar 1, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html#past-workshops",
    "href": "workshops.html#past-workshops",
    "title": "Workshops & events",
    "section": "Past workshops",
    "text": "Past workshops\n\n\n\n\n\nApril 2 and 4 (2025): Introduction to Python\n\n\n\n\n\nGet introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nApr 2, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nMarch 12 and 14 (2025): Introduction to Python\n\n\n\n\n\nGet introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nMar 12, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\n24 February 2025: Bring Your Own Data\n\n\n\n\n\nBring your own dataset and make publication-quality figures.\n\n\n\n\n\nFeb 24, 2025\n\n\nMisha Paauw\n\n\n\n\n\n\n\n15 and 17 January 2025: Introduction to R\n\n\n\n\n\nLearn the basics of data analysis and visualisation in R in this workshop.\n\n\n\n\n\nJan 15, 2025\n\n\nMisha Paauw\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "bioDSC core team",
    "section": "",
    "text": "Our core team consists of three members; Frans van der Kloet, Martijn Wehrens and Misha Paauw."
  },
  {
    "objectID": "people.html#frans-van-der-kloet",
    "href": "people.html#frans-van-der-kloet",
    "title": "bioDSC core team",
    "section": "Frans van der Kloet",
    "text": "Frans van der Kloet\n\n\nInfo\nI obtained my BSc at the Noordelijke Hogeschool (NHL) in Leeuwarden majoring in analytical chemistry and got my MSc in Computational Chemistry at the Radboud University in Nijmegen. After several jobs in commercial companies I continued my academic career in 2009 and obtained my PhD at the Leiden University on quantitative aspects in/of high resolution mass spectrometry data in metabolomics in 2014. In that same year I started as a post-doc at the BDA group working on aspects of multi-block/view solutions like JIVE, DISCO and OnPLS and incorporating these types of methods in the prediction/classification of in-vivo transcriptome data. After another PD position at the Amsterdam Medical Center I started as a data scientist in the BDA group in 2019.\nCurrent activities concern administration and implementation of a local Galaxy Server. Development of a database to store meta-data on samples/measurements (Metatree). Deployment of DL tools in the deconvolution of HRMS data and learning from DL models in general.\nMy main interest is in (pre)processing of large data-sets (with a preference for high resolution mass spectrometry data) and the development and implementation of data-analysis and pre-processing tools that is often further complicated because of these large data sizes.\n\n\nKeywords:\nPython, C++, C, Matlab, (R), Java, Galaxy(project), Metabolomics, MS, (multivariate) data analysis, (learning from) DL models.\n\n\nContact:\nGroup: Biosystems Data Analysis\nOffice: SP C2-205\nEmail: f.m.vanderkloet@uva.nl"
  },
  {
    "objectID": "people.html#martijn-wehrens",
    "href": "people.html#martijn-wehrens",
    "title": "bioDSC core team",
    "section": "Martijn Wehrens",
    "text": "Martijn Wehrens\n\nAfter studying biology and chemistry in Nijmegen (BSc, RU), I obtained a MSc degree in theoretical chemistry at the University of Amsterdam, during which I focussed on simulations of biomolecular networks. Remaining in Amsterdam, I then obtained my PhD at AMOLF, where I worked on tracking single bacteria using time lapse microscopy to understand gene regulation at the single cell. After finishing my PhD, I moved to the Hubrecht Institute in Utrecht where I used RNA-sequencing to investigate the pathogenesis of heart disease as a postdoc.\nAt the bioDSC, I have fun writing scripts and lending my various computational and quantitative expertise to further understand all the amazing biological processes that occur in cells.\n\nKeywords:\nImage analysis, quantitative biology, RNA-seq and sequencing pipelines, data analysis in Python and R.\n\n\nContact:\nGroup: Molecular Cytology\nOffice: SP C2.267a\nEmail: m.wehrens@uva.nl"
  },
  {
    "objectID": "people.html#misha-paauw",
    "href": "people.html#misha-paauw",
    "title": "bioDSC core team",
    "section": "Misha Paauw",
    "text": "Misha Paauw\n\n\nInfo\nAfter a broad training in biology during my BSc and MSc, ranging from ecology to bioinformatics to molecular plant biology, I started my PhD in the Molecular Plant Pathology group of the University of Amsterdam in 2019. In my PhD research, I worked on the molecular interactions between plants and a pathogenic bacterium, called Xanthomonas campestris pv. campestris (Xcc). By studying the ‘pangenome’ of Xcc, I reconstructed the evolutionary history of Xcc and pinpointed the genes required for the specific infection strategy of Xcc.\nIn my current role as data scientist I support molecular biologists in their data-heavy projects by providing individual consultancy and organizing workshops via the bioDSC.\n\n\nKeywords:\nPython, R, bash, data visualisation, comparative genomics, microbial genomics, molecular (plant) biology.\n\n\nContact:\nGroup: Plant Physiology, Green Life Sciences\nOffice: SP C2-207\nEmail: m.m.paauw@uva.nl"
  },
  {
    "objectID": "people.html#other-contributors",
    "href": "people.html#other-contributors",
    "title": "bioDSC core team",
    "section": "Other contributors",
    "text": "Other contributors\n\nAnna Heintz-Buschart (BDA, SILS)\nTijs Bliek (PDEG, SILS)\nNina Dombrowski (IBED)\nWim de Leeuw (MAD, SILS)"
  },
  {
    "objectID": "further-reading.html",
    "href": "further-reading.html",
    "title": "*bio*DSC",
    "section": "",
    "text": "Here, we will provide links to other useful pages on the intersection between biology and data science. Send us an email if you maintain a page or blog with useful biology-related programming tips and tricks, and wish to be included in these lists.\n\n\n\nCrunchomics documentation\nThe IBED bioinformatics support page\n\n\n\n\n\nDataViz protocols by Joachim Goedhart\nFundamentals of Data Visualization by Claus Wilke\nAn introduction to good color schemes\n\n\n\n\n\nMeta-Omics tutorials by Anna Heintz-Buschart"
  },
  {
    "objectID": "further-reading.html#further-reading",
    "href": "further-reading.html#further-reading",
    "title": "*bio*DSC",
    "section": "",
    "text": "Here, we will provide links to other useful pages on the intersection between biology and data science. Send us an email if you maintain a page or blog with useful biology-related programming tips and tricks, and wish to be included in these lists.\n\n\n\nCrunchomics documentation\nThe IBED bioinformatics support page\n\n\n\n\n\nDataViz protocols by Joachim Goedhart\nFundamentals of Data Visualization by Claus Wilke\nAn introduction to good color schemes\n\n\n\n\n\nMeta-Omics tutorials by Anna Heintz-Buschart"
  }
]