[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The bioDSC blog",
    "section": "",
    "text": "Installing Python Software\n\n\n\n\n\n\npython\n\n\nsoftware\n\n\ninstallation\n\n\ntutorial\n\n\n\nInstalling software that allows you to easily work with Python.\n\n\n\n\n\nMar 11, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nShould I learn Python or R?\n\n\n\n\n\n\ngeneral\n\n\nworkshop-related\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nVisualizing Arabidopsis whole-genome alignments\n\n\n\n\n\n\ncomparative genomics\n\n\narabidopsis\n\n\ncrunchomics\n\n\n\nLearn how to plot genome rearrangements in this blog.\n\n\n\n\n\nNov 4, 2024\n\n\nMisha Paauw\n\n\n\n\n\n\n\nThe bioDSC blog: first post\n\n\n\n\n\n\nintro\n\n\ngeneral\n\n\n\nAn introduction to the blog and its purpose.\n\n\n\n\n\nOct 30, 2024\n\n\nMisha Paauw\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "*bio*DSC",
    "section": "",
    "text": "Tutorial pages will be uploaded to this section of the site! Stay tuned!"
  },
  {
    "objectID": "tutorials.html#tutorials",
    "href": "tutorials.html#tutorials",
    "title": "*bio*DSC",
    "section": "",
    "text": "Tutorial pages will be uploaded to this section of the site! Stay tuned!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The biological Data Science Center",
    "section": "",
    "text": "As a biologist at the Science Park of the University of Amsterdam, it’s likely that you will generate large amounts of data. The biological Data Science Center (bioDSC) was initiated to help you analyse that data.\nOur aim is to enable you to perform analyses yourself, which is why we offer workshops, on-line tutorials and in-person mentoring. We have an open-door policy: we encourage you to send an email or drop by one of our desks when you need help or input with your data analysis problem. For quick questions, check out our Slack channel!\nSend an email Drop by Slack"
  },
  {
    "objectID": "index.html#our-approach",
    "href": "index.html#our-approach",
    "title": "The biological Data Science Center",
    "section": "Our approach",
    "text": "Our approach\n\n\nThe bioDSC was initiated in September ’24, and we are currently finding out what works best to optimally support people with their data analyses.\nOur core members are embedded in several research groups at the Swammerdam Institute for Life Sciences (SILS), to be integrated in the community of researchers at Science Park. In addition, other researchers at SILS sometimes volunteer to further support data analyses and good practices."
  },
  {
    "objectID": "index.html#check-out-our-online-resources",
    "href": "index.html#check-out-our-online-resources",
    "title": "The biological Data Science Center",
    "section": "Check out our online resources",
    "text": "Check out our online resources\nCheck out our blog for quick reads on coding, which maybe inspiring for your research. Check out our tutorials for full protocols on analyses that are often performed by biologists at the UvA Science Park, such as RNA-seq.\nBlog Tutorials Further reading"
  },
  {
    "objectID": "index.html#check-out-our-workshops",
    "href": "index.html#check-out-our-workshops",
    "title": "The biological Data Science Center",
    "section": "Check out our workshops",
    "text": "Check out our workshops\nOur aim is to organize workshops on topics researchers are interested in and that will help them. We’re currently working on composing a list of workshops and will start to organize them soon. Check out our workshop page to see whether we already have a workshop that is of interest to you. If you’re interested in a topic that’s not on the list, don’t hesitate to send us an email or drop by. We can either help you out with your specific problem, or organize a workshop if it’s a topic of potential interest to many researchers.\nWorkshops"
  },
  {
    "objectID": "index.html#history",
    "href": "index.html#history",
    "title": "The biological Data Science Center",
    "section": "History",
    "text": "History\n\n\nAt the University of Amsterdam Science Park, enthusiastic researchers started the Science Park Study Group in 2017. The idea was to create a local culture of connecting and sharing knowledge with regard to coding in the biological sciences. Having started in September ’24 with the bioDSC, we aim to further boast this initiative and spirit.\nThe original website of the Science Park Study group can be found here."
  },
  {
    "objectID": "posts/python-or-R.html",
    "href": "posts/python-or-R.html",
    "title": "Should I learn Python or R?",
    "section": "",
    "text": "When you haven’t had any exposure to scripting languages, but know some type of scripting language might be convenient to analyze your data, it might be hard to determine which language (R, Python, Javascript, ..) you should look into.\nIn this blog post I list some typical use cases of Python and R such that you have an impression which language you use for what."
  },
  {
    "objectID": "posts/python-or-R.html#motivation-for-writing-this-blog-post",
    "href": "posts/python-or-R.html#motivation-for-writing-this-blog-post",
    "title": "Should I learn Python or R?",
    "section": "",
    "text": "When you haven’t had any exposure to scripting languages, but know some type of scripting language might be convenient to analyze your data, it might be hard to determine which language (R, Python, Javascript, ..) you should look into.\nIn this blog post I list some typical use cases of Python and R such that you have an impression which language you use for what."
  },
  {
    "objectID": "posts/python-or-R.html#python-versus-r",
    "href": "posts/python-or-R.html#python-versus-r",
    "title": "Should I learn Python or R?",
    "section": "Python versus R",
    "text": "Python versus R\nWhen analyzing scientific data, there are multiple tools available. For simple analyses, Excel, GraphPad or FIJI/ImageJ may suffice. When handling large amounts of data, or when you demand more specialized, custom types of analyses, it becomes convenient to use scripting languages such as Python or R. Many programming and scripting languages exist, but the main ones many researchers use are R and Python, on which I’ll focus here.\nBelow a list of what people typically use R and Python for.\n\nR\nTypical use cases for R include:\n\nbioinformatics, including specialized packages for\n\ngenomics, transcriptomics, proteomics, e.g.\n\nGene annotation (GO terms)\nEnrichment analyses, differential gene expression\nSeurat package for single cell analysis\n\n\nextensive data visualization\n\n(through the ggplot library)\ndisplaying high-dimensional data e.g. in tSNE or UMAPs, PCA analysis etc\n\ncomplex statistical modeling, specialized e.g. for\n\nEcology\nBioinformatics (as mentioned above)\n\n\n\n\nPython\n\nImage analysis\n\nAlthough MATLAB is a go-to tool for image analysis, Python has become the open-source almost on-par alternative\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images, e.g. to\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nE.g. SciPy, OpenCV, scikit, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\nThis offers similar capabilities as R\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nE.g. Keras and PyTorch\n\n\nhigh-throughput data analysis and automation\n\npython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nbioinformatics\n\nseems there’s less tools available, however offers packages to:\nPerform single cell analysis, such as SCANPY\nCustom tasks in bioinformatics pipelines (specialized mapping, e.g.) can use python\n\n\nWhat ChatGPT had to say about it: “R excels at statistical and visualization tasks, particularly for biostatistics and bioinformatics, while Python is preferred for machine learning, automation, and handling diverse data types in scalable pipelines. Many researchers use both, leveraging each for its strengths.”"
  },
  {
    "objectID": "posts/python-or-R.html#so-what-should-i-choose",
    "href": "posts/python-or-R.html#so-what-should-i-choose",
    "title": "Should I learn Python or R?",
    "section": "So, what should I choose?",
    "text": "So, what should I choose?\nIf you’re uncertain what you should learn, feel free to drop us an email, walk by our desks, or contact us in another way to have a chat about what is most suitable for you!\nSend an email Drop by Slack"
  },
  {
    "objectID": "posts/installing_conda_python.html",
    "href": "posts/installing_conda_python.html",
    "title": "Installing Python Software",
    "section": "",
    "text": "This tutorial explains how to install software on your computer that allows you to write and run Python scripts."
  },
  {
    "objectID": "posts/installing_conda_python.html#python-itself",
    "href": "posts/installing_conda_python.html#python-itself",
    "title": "Installing Python Software",
    "section": "1: Python itself",
    "text": "1: Python itself\nAs explained during our workshops, Python itself is a software that interprets written computer code, and uses that to make the computer do things. Sometimes Python is already installed on your computer, but often you need to install it.\nPython makes use of “libraries” (pre-written code with functions for specific tasks), that you likely need to download/install as well."
  },
  {
    "objectID": "posts/installing_conda_python.html#a-text-editor-dedicated-to-writing-python-code",
    "href": "posts/installing_conda_python.html#a-text-editor-dedicated-to-writing-python-code",
    "title": "Installing Python Software",
    "section": "2: A text editor dedicated to writing Python code",
    "text": "2: A text editor dedicated to writing Python code\nIt is convenient to also install another piece of software, in which you can write Python code, and which also allows you to “send” the script towards the actual Python software.\nThis way you can write Python code in a pleasant way, and also run it immediately. In practice, you’ll only interact with this editor."
  },
  {
    "objectID": "posts/installing_conda_python.html#software-installing-software.",
    "href": "posts/installing_conda_python.html#software-installing-software.",
    "title": "Installing Python Software",
    "section": "3: Software installing-software.",
    "text": "3: Software installing-software.\nInstalling software to manage software installation sounds perhaps a bit complicated, but it’ll make your life easier.\nInstalling Python, its libraries, and code editors, comes challenges, such as:\n\nIt is tedious to install all these things manually.\nYou might need a set of libraries for a specific task, which have installation requirements that exclude you from installing other libraries that you might need for another task.\n\nThese challenges are adressed by software that manages such “dependencies” and “conflicts”."
  },
  {
    "objectID": "posts/installing_conda_python.html#how-to-install-python-software-using-miniconda",
    "href": "posts/installing_conda_python.html#how-to-install-python-software-using-miniconda",
    "title": "Installing Python Software",
    "section": "How to install Python software using Miniconda",
    "text": "How to install Python software using Miniconda\n\n1. Open a terminal with Miniconda activated\n\nIn windows, after installing Miniconda, open the “Anaconda Powershell Prompt (miniconda3)”.\nOn a Macbook, if you have followed all the instructions, (close and re-)open the terminal. It should now display “(base)” in front of your username (see screenshot from my terminal above).\n\n\n\n2. Install python and libraries.\nPython will be automatically installed in step 3. However, libraries often need to be installed separately. You can do this at any point you like, whenever it turns out you need a library that you haven’t installed yet.\nThis can be done easily using miniconda. To install something using Conda, simply open the terminal and type the following command in the terminal, and press enter.\nconda install XXXX\nWhere XXXX is the library you want.\nYou sometimes need to specifiy on which online “database” (“channel”) software is found. You also need to know the very specific name of the package.\nTo address both these questions, it is easiest to just google for “conda install XXX”, where XXX is the package you want. For example, to install pandas, after Googling, we end up at this page, and see the installation command is:\nconda install anaconda::pandas\nThe part anaconda:: specifies further where to find pandas installation files.\nSimply copy that code into your terminal, and press enter. You will see something like this:\n\n\n\nA 2nd screenshot of my terminal, using conda.\n\n\nType a “y” and press enter to proceed. This means all the listed libraries will be installed. They are needed because in this case Pandas requires these other libraries to be installed in order to run. This is usually the case.\n\n\n3. Choose the editor software you want and install it\nTo work with Python, there are multiple options for software to write scripts and run them using Python. I’ve listed them below.\n\n\n\n\n\n\n\nSoftware\nDescription\n\n\n\n\nJupyter \n(+) Write code and text in blocks, and see the output directly below in one document. (-) Not suitable for large amounts of code (-) Adds extra layer complexity.\n\n\nSpyder\n(+) Easily write simply .py files, inspect parameters. (+) Rstudio or MATLAB like IDE. (-) Lacks some modern features. (-) No co-pilot integration.\n\n\nVisual Studio Code\n(+) ±Industry standard, has most advanced features. (+) Understands different conda environments. (+) Plugins for many features. (-) Very counter-intuitive to use. Makes it difficult to use.\n\n\nPyCharm\n(We don’t have much experience with this, but it’s also good and often used.)\n\n\n\nFor the workshop, we used Jupyter notebooks. We also recommend Spyder.\nThe installation of both of these can be done through Conda. See below.\n\nInstalling Jupyter using Conda\nJupyter can be installed using Conda. (On the official website they say use something called “pip” (link), but we recommend conda.)\nInstall jupyter similar as to how you installed other python libraries (see section 2 above):\nconda install conda-forge::jupyterlab\n(Source.). Copy and paste this into a terminal which has conda activated, and press enter to run. (See above.)\nTo run Jupyter, simply type the following command in your terminal and press enter:\njupyter lab\nThis will start Jupyter running on your local computer.\n\n\nInstalling spyder using Conda\nSpyder can be installed using Conda.\nInstall Spyder similar as to how you installed other python libraries (see section 2 above). It is also described in detail at: https://docs.spyder-ide.org/current/installation.html\nPaste the following command in your terminal (and hit enter) for an extended installation:\nconda install -c conda-forge spyder numpy scipy pandas matplotlib sympy cython\nPaste the following command in your terminal (and hit enter) for an minimal installation:\nconda install -c conda-forge spyder\n(Note: the official installation instructions use “environments”, if you use the official instructions, read carefully about using environments.)\nTo run, simply open a terminal, and type (+ hit enter afterwards):\nspyder\n\n\nInstalling VS Code\nFollow the instructions at: https://code.visualstudio.com/\n\n\nInstalling PyCharm\nFollow the instructions at: https://www.jetbrains.com/pycharm/"
  },
  {
    "objectID": "posts/installing_conda_python.html#a-final-note-about-this-blog",
    "href": "posts/installing_conda_python.html#a-final-note-about-this-blog",
    "title": "Installing Python Software",
    "section": "A final note about this blog",
    "text": "A final note about this blog\nThese installation instructions are intended for relative novices to using Python. For intermediates and experts, we recommend to use different conda environments when working with Python. However, for novices, we think it’s better to do everything in the base environment.\nWe do recommend people reading up on how to use conda environments. And while I’m at it, I also recommend people reading up on version tracking using Git and Github. But that’s another topic."
  },
  {
    "objectID": "workshop-materials/py-intro/additional_exercises.html",
    "href": "workshop-materials/py-intro/additional_exercises.html",
    "title": "Additional exercises for Python Workshop",
    "section": "",
    "text": "Complementary to The Carpentries Python course\n\n\n\n\nWhat’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'"
  },
  {
    "objectID": "workshop-materials/py-intro/additional_exercises.html#lesson-2",
    "href": "workshop-materials/py-intro/additional_exercises.html#lesson-2",
    "title": "Additional exercises for Python Workshop",
    "section": "",
    "text": "What’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'"
  },
  {
    "objectID": "workshop-materials/py-intro/plot.html",
    "href": "workshop-materials/py-intro/plot.html",
    "title": "Workshop image",
    "section": "",
    "text": "Workshop image\n\n\n\nGDP of European countries 1952-2007."
  },
  {
    "objectID": "workshops/2025-introduction-R.html",
    "href": "workshops/2025-introduction-R.html",
    "title": "15 and 17 January 2025: Introduction to R",
    "section": "",
    "text": "The bioDSC organizes an introductory course on data analysis and visualisation in R. R is a programming language for statistical programming and data visualation, and is very popular in the field of biology. Chances are high that those nice figures you see in papers were generated in R! In addition, understanding the basics of R sets you up for more complex data-heavy analysis such as the analysis of RNA-seq data.\nThis workshop assumes no prior knowledge on R or programming!"
  },
  {
    "objectID": "workshops/2025-introduction-R.html#should-i-learn-python-or-r",
    "href": "workshops/2025-introduction-R.html#should-i-learn-python-or-r",
    "title": "15 and 17 January 2025: Introduction to R",
    "section": "Should I learn Python or R?",
    "text": "Should I learn Python or R?\nSee the blog post about Python vs. R in case you’re wondering whether you should start learning: R, or Python. We have more workshops about Python and R coming soon.\n\nSign up\nSign up here. Sign up deadline: 8th of january!"
  },
  {
    "objectID": "workshops/2025-BYOD-1.html",
    "href": "workshops/2025-BYOD-1.html",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "",
    "text": "The bioDSC organizes a “Bring Your Own Data” data visualization workshop. In this workshop, we will offer help and guidance to turn your own dataset into insightful visualizations using ggplot2 in R. You are expected to bring your own table with data to work on, ranging from a small pilot experiment to large-scale experimental assays. We can help improving your existing plotting scripts, or build one from scratch.\nThe aims of this workshop are to help you with:\n\nMaking (publication-quality) plots of your dataset\nWriting reusable scripts to generate more plots later with new datasets\nBecoming more experienced and confident in using R and ggplot2\n\n\n\n\nBoxplots, barplots, and lineplots made in ggplot2, taken from Paauw et al, 2024."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#bring-your-own-data-data-visualization-workshop",
    "href": "workshops/2025-BYOD-1.html#bring-your-own-data-data-visualization-workshop",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "",
    "text": "The bioDSC organizes a “Bring Your Own Data” data visualization workshop. In this workshop, we will offer help and guidance to turn your own dataset into insightful visualizations using ggplot2 in R. You are expected to bring your own table with data to work on, ranging from a small pilot experiment to large-scale experimental assays. We can help improving your existing plotting scripts, or build one from scratch.\nThe aims of this workshop are to help you with:\n\nMaking (publication-quality) plots of your dataset\nWriting reusable scripts to generate more plots later with new datasets\nBecoming more experienced and confident in using R and ggplot2\n\n\n\n\nBoxplots, barplots, and lineplots made in ggplot2, taken from Paauw et al, 2024."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#prior-experience",
    "href": "workshops/2025-BYOD-1.html#prior-experience",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Prior experience",
    "text": "Prior experience\nFor this workshop, we assume that you have some experience with data manipulation and visualisation in R, using the tidyverse packages dplyr, tidyr and ggplot2. For example, you have followed our recent workshop Introduction to R where we covered the materials of R for Social Scientists. Alternatively, you have similar experience from other courses or your own explorations."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#workshop-logistics",
    "href": "workshops/2025-BYOD-1.html#workshop-logistics",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Workshop logistics",
    "text": "Workshop logistics\nIn this workshop, you will work by yourself on your own dataset. Martijn Wehrens, Misha Paauw, and Frans van der Kloet from the bioDSC will be available to discuss your approach and help you with all the problems you encounter along the way. There is room for 4-12 participants. You have to bring your own laptop, with and R and RStudio installed, including the tidyverse package."
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#workshop-schedule",
    "href": "workshops/2025-BYOD-1.html#workshop-schedule",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Workshop schedule",
    "text": "Workshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\nMonday 24 february\n13:00 - 16:00\nScience Park B0.206\nVisualisation of datasets"
  },
  {
    "objectID": "workshops/2025-BYOD-1.html#sign-up",
    "href": "workshops/2025-BYOD-1.html#sign-up",
    "title": "24 February 2025: Bring Your Own Data",
    "section": "Sign up",
    "text": "Sign up\nSign up by sending an email to info@biodsc.nl.\nIn your email, please include the following:\n\nYour research group\nYour dataset in comma separated, tab separated, or excel table format (.csv, .tsv, .xlsx).\nA brief explanation of your dataset and the plots you want to make\nIdeally, a quick sketch of the plot you want to make\n\nSign up deadline: 19th of february!"
  },
  {
    "objectID": "workshops/2025-introduction-python-2.html",
    "href": "workshops/2025-introduction-python-2.html",
    "title": "April 2 and 4 (2025): Introduction to Python",
    "section": "",
    "text": "This workshop is full.\n\n\n\n12 people have registered (surprisingly fast), we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\nAn introduction to Python\nThe bioDSC organizes an introductory workshop that will cover basic functionalities of Python.\nPython is a computer script language that has become a workhorse of data analysis throughout many fields and disciplines. It can process and analyze big data sets, perform image analysis, and create great visualizations.\nSee below for small overview of what types of tasks can be done using Python.\n\n2nd session\nThis workshop will be exactly the same as the previous one. We are simply organizing another session since the previous workshop was booked completely full.\n\n\n\nWorkshop goals\nAfter this workshop, you’ll be familiar with Python basics. We’ll discuss and let you practice on:\n\n\nUsing Python (Jupyter notebooks, VS Studio code, Spyder)\n\nThroughout the workshop, we’ll use an online version of Jupyter notebook to avoid installation issues)\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib).\nUsing loops and conditionals.\nWriting functions.\nGood practices in programming and software engineering.\n\n\nThis workshop will largely (&gt;95%) follow the Carpentries training material “Plotting and programming in Python”. All training material can be found at:\n\nhttps://swcarpentry.github.io/python-novice-gapminder/\n\nSee this workshop page for a summary of desired learning results. The materials we’ll use do not use examples specific to biology, but instead use the “Gapminder” dataset (see [1] and [2]). We think this offers thorough insights into how Python works. Follow-up courses, e.g. into image analysis, will be delve into more biological datasets.\nIf you have any questions regarding whether this course is relevant for you, please send us an email (info@biodsc.nl) or walk by our desks.\n\n\nRequirements\nThis workshop assumes no prior knowledge on Python or programming!\n\n\nWorkshop logistics\nThe course will be given by Misha Paauw, Martijn Wehrens and Frans van der Kloet from the bioDSC. There is room for 4-12 participants. You have to bring your own laptop, we’ll use an online version of Python, so there are no software installation requirements.\nAs mentioned, the contents of this workshops are almost completely based on: https://swcarpentry.github.io/python-novice-gapminder/\n\n\nWorkshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\nApril 2\n13:00 - 17:30\nSP L1.13\nIntroduction to Python, episodes 1-9\n\n\nApril 4\n12:30 - 17:30\nSP L2.06\nIntroduction to Python, episodes 11-18\n\n\n\n\n\nSign up\n\n\n\n\n\n\n\n\nThis workshop is full.\n\n\n\n12 people have registered, we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\n\n\nTypical use cases for Python\nPython is a scripting language, meaning that your write a file with commands for the computer to execute. This allows you to perform complicated tasks, such as:\n\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\n\nImage analysis\n\nPython has become a great open-source tool for image analysis\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images. You can:\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nExtract summary parameters relevant for your analysis.\nRelated libraries: scikit-image, SciPy, OpenCV, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nLibraries: E.g. Keras and PyTorch\n\n\nHigh-throughput data analysis and automation\n\nPython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nBioinformatics\n\nCan be used to handle, manipulate, process, quantify raw sequence data or similar.\nPerform single cell analysis, e.g. using the SCANPY lirbary.\nNot the tool for RNA-seq statistics and gene expression analysis, for which R is superior.\n\nMuch more ..\n\n\nSee also the post I wrote earlier to decide whether you should Python or R."
  },
  {
    "objectID": "workshops/2025-introduction-python.html",
    "href": "workshops/2025-introduction-python.html",
    "title": "March 12 and 14 (2025): Introduction to Python",
    "section": "",
    "text": "This workshop is full.\n\n\n\n12 people have registered, we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\nAn introduction to Python\nThe bioDSC organizes an introductory workshop that will cover basic functionalities of Python.\nPython is a computer script language that has become a workhorse of data analysis throughout many fields and disciplines. It can process and analyze big data sets, perform image analysis, and create great visualizations.\nSee below for small overview of what types of tasks can be done using Python.\n\n\nWorkshop goals\nAfter this workshop, you’ll be familiar with Python basics. We’ll discuss and let you practice on:\n\n\nUsing Python (Jupyter notebooks, VS Studio code, Spyder)\n\nThroughout the workshop, we’ll use an online version of Jupyter notebook to avoid installation issues)\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib).\nUsing loops and conditionals.\nWriting functions.\nGood practices in programming and software engineering.\n\n\nThis workshop will largely (&gt;95%) follow the Carpentries training material “Plotting and programming in Python”. All training material can be found at:\n\nhttps://swcarpentry.github.io/python-novice-gapminder/\n\nSee this workshop page for a summary of desired learning results. The materials we’ll use do not use examples specific to biology, but instead use the “Gapminder” dataset (see [1] and [2]). We think this offers thorough insights into how Python works. Follow-up courses, e.g. into image analysis, will be delve into more biological datasets.\nIf you have any questions regarding whether this course is relevant for you, please send us an email (info@biodsc.nl) or walk by our desks.\n\n\nRequirements\nThis workshop assumes no prior knowledge on Python or programming!\n\n\nWorkshop logistics\nThe course will be given by Martijn Wehrens and Misha Paauw from the bioDSC. There is room for 4-12 participants. You have to bring your own laptop, we’ll use an online version of Python, so there are no software installation requirements.\nAs mentioned, the contents of this workshops are almost completely based on: https://swcarpentry.github.io/python-novice-gapminder/\n\n\nWorkshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\n12 March\n12:30 - 17:30\nSP A1.06\nIntroduction to Python, episodes 1-9\n\n\n14 March\n12:30 - 17:30\nSP B0.209\nIntroduction to Python, episodes 11-18\n\n\n\n\n\nSign up\n\n\n\n\n\n\n\nThis workshop is full.\n\n\n\n12 people have registered, we do not have place for more participants. Unfortunately, you cannot register any more.\nPlease send us an email in case you were interested in signing up, such that we know to organize more workshops soon.\n\n\n\n\n\nTypical use cases for Python\nPython is a scripting language, meaning that your write a file with commands for the computer to execute. This allows you to perform complicated tasks, such as:\n\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\n\nImage analysis\n\nPython has become a great open-source tool for image analysis\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images. You can:\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nExtract summary parameters relevant for your analysis.\nRelated libraries: scikit-image, SciPy, OpenCV, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nLibraries: E.g. Keras and PyTorch\n\n\nHigh-throughput data analysis and automation\n\nPython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nBioinformatics\n\nCan be used to handle, manipulate, process, quantify raw sequence data or similar.\nPerform single cell analysis, e.g. using the SCANPY lirbary.\nNot the tool for RNA-seq statistics and gene expression analysis, for which R is superior.\n\nMuch more ..\n\n\nSee also the post I wrote earlier to decide whether you should Python or R."
  },
  {
    "objectID": "workshops/2025-introduction-python-3.html",
    "href": "workshops/2025-introduction-python-3.html",
    "title": "May 19, 21 and 26 (2025): Introduction to Python",
    "section": "",
    "text": "An introduction to Python\nThe bioDSC organizes an introductory workshop that will cover basic functionalities of Python.\nPython is a computer script language that has become a workhorse of data analysis throughout many fields and disciplines. It can process and analyze big data sets, perform image analysis, and create great visualizations.\nSee below for small overview of what types of tasks can be done using Python.\n\n\nWorkshop goals\nAfter this workshop, you’ll be familiar with Python basics. We’ll discuss and let you practice on:\n\n\nUsing Python (Jupyter notebooks, VS Studio code, Spyder)\n\nThroughout the workshop, we’ll use an online version of Jupyter notebook to avoid installation issues)\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib and Seaborn).\nUsing loops and conditionals.\nWriting functions.\nGood practices in programming and software engineering.\n\n\n\n\nWorkshop materials\n\nContents\nThis workshop will largely (&gt;80%) follow the Carpentries training material “Plotting and programming in Python”. We customized materials for lessons 8 and 9 (dataframes and plotting, see below) to include biological examples. We are working to publish these materials online too. The course does not address specific programming solutions for common biological challenges. Follow-up courses, e.g. about RNA-seq analysis and image analysis, will delve into more biological programming topics.\n\n\n\nDatasets\nThese materials use the “Gapminder” dataset (see [1] and [2]). Programming examples from this dataset will not relate to biological problems, but the examples offer good insights in how Python works. Additionally, we included custom materials based on single cell sequencing data from Kohela et al. (GSE149331), which will highlight Python functionalities through biological examples.\n\n\nChanges compared previous workshops\nThis workshop will be similar to previous Python workshops [1], [2]. We changed the scheduling (3 shorter timeslots instead of 2), and edited the materials to include more biologically relevant examples (and plotting with Seaborn).\n\n\nQuestions\nIf you have any questions regarding whether this course is relevant for you, please send us an email (info@biodsc.nl) or walk by our desks.\n\n\n\nRequirements\n\nThis workshop assumes no prior knowledge on Python or programming!\nYou have to bring your own laptop.\n\n\n\nWorkshop logistics\nThe course will be given by Misha Paauw, Martijn Wehrens and Frans van der Kloet from the bioDSC. There is room for 4-20 participants. You have to bring your own laptop, we’ll use an online version of Python, so there are no software installation requirements.\n\n\nWorkshop schedule\n\n\n\n\n\n\n\n\n\nDate\nTime\nLocation\nTopic\n\n\n\n\nMay 19 (Mon)\n13:00 - 17:00\nSP L1.13*\nThe basics, episodes 1-6\n\n\nMay 21 (Wed)\n13:00 - 17:00\nSP L1.13*\nDataframes and plotting, episodes 7-9\n\n\nMay 26 (Mon)\n13:00 - 17:00\nSP L1.12*\nProgramming concepts, episodes 11-16\n\n\n\n*) These rooms are in the “Lab 42” building!\n\n\nSign up\nSign up using this link. Sign up deadline: 28th of April. First come, first serve.\n\n\n\n\nTypical use cases for Python\nPython is a scripting language, meaning that your write a file with commands for the computer to execute. This allows you to perform complicated tasks, such as:\n\n\nPlotting and analyzing large data sets\n\nWith the pandas, matplotlib, and seaborn library, large datasets can be imported, manipulated, and visualized\n\nImage analysis\n\nPython has become a great open-source tool for image analysis\nMultiple libraries are now available that can do pretty much any “classical” image operations to analyze images. You can:\n\nSegment your image (identify cells, or other parts of your image that are of interest)\nExtract summary parameters relevant for your analysis.\nRelated libraries: scikit-image, SciPy, OpenCV, ..\n\nMultiple tools exist to allow smooth user-interaction, such as Napari\n\nMachine learning, neural networks, LLM, AI\n\nPython has become the go-to tool for working with machine learning related tech\n\nLibraries: E.g. Keras and PyTorch\n\n\nHigh-throughput data analysis and automation\n\nPython is often used to process large amounts of data for further processing (see bio-informatics below)\n\nBioinformatics\n\nNot the tool for RNA-seq statistics and gene expression analysis, for which R is superior.\nCan be used to handle, manipulate, process, quantify raw sequence data or similar.\nPerform single cell analysis, e.g. using the SCANPY lirbary.\n\nMuch more ..\n\n\nSee also the post I wrote earlier to decide whether you should Python or R.\n\n\nIdentifier\nThis meeting’s identifier: 2025-05-19-UvA-bioDSC."
  },
  {
    "objectID": "workshops/2025-working-on.html",
    "href": "workshops/2025-working-on.html",
    "title": "List of past and upcoming workshops",
    "section": "",
    "text": "Below a list of planned workshops:\n\nAn introduction to R (Jan 2025)\n\nBasics of R\nImporting, manipulating and analyzing data (mostly using dataframes).\nPlotting using ggplot.\n\nBring your own data (February 25th, 2025)\n\nBring your own dataset and make publication-quality figures.\n\nAn introduction to Python (March 12 & 14, 2025).\n\nVariables, types, basic commands, functions, libraries.\nWorking with tabular data (Pandas).\nPlotting (matplotlib).\nUsing loops and conditionals, writing functions.\nGood practices in programming and software engineering.\n\nImage analysis with Python (planning: April/May 2025).\n\nBasics of working with images\nImage manipulation, cellular segmentation, and other often-used concepts.\n(..)\n\nAnalysis of RNA-sequencing data (planning: April/May 2025)\n\nRead quality control\nMapping reads to a reference genome\nDifferentially expressed gene analysis using DEseq2 in R.\n\n\nWe aim to give workshops based on demand. So if there’s enough people interested, workshops will be recurring."
  },
  {
    "objectID": "workshops/2025-working-on.html#should-i-learn-python-or-r",
    "href": "workshops/2025-working-on.html#should-i-learn-python-or-r",
    "title": "List of past and upcoming workshops",
    "section": "Should I learn Python or R?",
    "text": "Should I learn Python or R?\nSee the blog post about Python vs. R in case you’re wondering whether you should start learning: R, or Python."
  },
  {
    "objectID": "workshop-materials/index.html",
    "href": "workshop-materials/index.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\n\nFor the python workshop exercises, see here: exercises.\nPython workshop exercises:\n\nLesson 2 - variables\nLesson 3 - data-types\nLesson 4 - built-in functions\nLesson 6 - libraries\nLesson 7 - dataframes part-1\nLesson 8 - dataframes part-2\nLesson 9 - plotting\nLesson 11 - lists\nLesson 12 - loops\nLesson 13 - conditionals\nLesson 14 - looping over data\nLesson 16 - writing functions\n\nFor the RNA-seq dataset from Kohela et al. (GSE149331), click this link.\nFor the example plot, see here: link."
  },
  {
    "objectID": "workshop-materials/py-intro/exercises.html",
    "href": "workshop-materials/py-intro/exercises.html",
    "title": "Exercises Lesson 2, variables",
    "section": "",
    "text": "Exercises Lesson 2, variables\n\n1. Order of things\n\nA\nFill the table showing the values of the variables in this program after each statement is executed.\n# Command  # Value of x   # Value of y   # Value of swap #\nx = 1.0    #              #              #               #\ny = 3.0    #              #              #               #\nswap = x   #              #              #               #\nx = y      #              #              #               #\ny = swap   #              #              #               #\n\n\nB\n\n\n\n\n\n\nBWA alignment options\n\n\n\nBWA consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the other two are for sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate.\n\n\nWhat is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.)\ninitial = 'left'\nposition = initial\ninitial = 'right'\n\n\n\n\n\n\nSolution\n\n\n\n\n\n$ grep -v \"#\" results/vcf/SRR2584866_final_variants.vcf | wc -l\n765\nThere are 765 variants in this file.\n\n\n\n\n\n\n“Challenge”\nIf you assign a = 123, what happens if you try to get the second digit of a via a[1]?\n\n\nSlicing\n\nGiven the following string:\nspecies_name = \"Acacia buxifolia\"\nwhat would these expressions return?\nspecies_name[2:8]\nspecies_name[11:] (without a value after the colon)\nspecies_name[:4] (without a value before the colon)\nspecies_name[:] (just a colon)\nspecies_name[11:-3]\nspecies_name[-5:-3]\nWhat happens when you choose a stop value which is out of range? (i.e., try species_name[0:20] or species_name[:103])\n\n\n\n\n\n\n\nLesson 2, additional exercises\n\nNaming (Carpentries)\nWhich is a better variable name, m, min, or minutes? Why? Hint: think about which code you would rather inherit from someone who is leaving the lab:\nts = m * 60 + s\ntot_sec = min * 60 + sec\ntotal_seconds = minutes * 60 + seconds\n\n\nA very challenging set of additional exercises (bioDSC)\nWhat’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'\n\n\n\n\n\n\n\nExercises Lesson 3, types\n\nTypes\n\nA\nWhat type of value is 3.4? How can you find out?\n\n\nB\nWhat type of value is 3.25 + 4?\n\n\nC\nWhat type of value (integer, floating point number, or character string) would you use to represent each of the following? Try to come up with more than one good answer for each problem. For example, in # 1, when would counting days with a floating point variable make more sense than using an integer?\n\nNumber of days since the start of the year.\nTime elapsed from the start of the year until now in days.\nSerial number of a piece of lab equipment.\nA lab specimen’s age\nCurrent population of a city.\nAverage population of a city over time.\n\n\n\nD (added bioDSC)\nWhy wouldn’t you always use floats, and never use integers?\n\n\n\nStrings to numbers\nWhere reasonable, float() will convert a string to a floating point number, and int() will convert a floating point number to an integer:\nprint(\"string to float:\", float(\"3.4\"))\nprint(\"float to int:\", int(3.4))\nOUTPUT:\nstring to float: 3.4\nfloat to int: 3\nIf the conversion doesn’t make sense, however, an error message will occur.\nprint(\"string to float:\", float(\"Hello world!\"))\nGiven this information, what do you expect the following program to do?\nWhat does it actually do?\nWhy do you think it does that?\nprint(\"fractional string to int:\", int(\"3.4\"))\nWhich of the following will return the floating point number 2.0? Note: there may be more than one right answer.\nfirst = 1.0\nsecond = \"1\"\nthird = \"1.1\"\n1. first + float(second)\n2. float(second) + float(third)\n3. first + int(third)\n4. first + int(float(third))\n5. int(first) + int(float(third))\n6. 2.0 * second\n\n\n\n\n\n\n\nLesson 3, additional exercises\n\nLists\nWe’ll cover lists later in lesson 11, but let’s already take a brief look.\nA list is a series of elements bound together, where each element can have a value. They are defined as follows:\nnumbers = [1,2,3]\nfruits = ['apples', 'pears', 'oranges']\nphysical_constants = ['pi', 3.14, 'c', 299_792_458, 'mole', 6.022e23]\nElements can be accessed the same way as we saw with strings.\n\nA\nWhat will numbers[1] return? And physical_constants[2:4]?\n\n\nB\nWhat is the type of\n\nnumbers?\nnumbers[1]?\nphysical_constants?\nphysical_constants[1]?\nphysical_constants[2]?\nphysical_constants[3]?\nfruits?\nfruits[1]?\n\n\n\nC\nCan the elements in a list have different types? (This can be seen from the previous answer.)\n\n\n\nnp.array\nLists can be a useful tool, but for example in image analysis, don’t offer the full mathematical options one might like. numpy arrays introduce a new type of series, in which you can do more manipulations. See some examples below:\nimport numpy as np\nmy_array = [1,2,3,5]\nmy_array_np = np.array([1,2,3,5])\n\n# what's the difference here:\n# my_array+1 # commented because gives error, why?\nmy_array_np+1\n\n# what's the difference here:\nmy_array * 3\nmy_array_np * 3\n\n# more:\nmy_array + [1,2,3,4]\nmy_array_np + np.array([1,2,3,4])\nnp.sin(my_array_np)\n\n# numpy-specific things, what is happening?\nmy_array_np[range(1,4,2)]\nmy_array_np[my_array_np&gt;1]\n\n# Can numpy arrays have different types?\n# What is the type of the elements in these two arrays?\nnp.array([1,2,3,'4'])\nnp.array([1,2,3,'hello'])\n\n\nDict\nPython has more types. A dict is sometimes very convenient, and is also used later with creating tables.\nexperimental_replicate_list = {'WT': 12, 'mut': 32, 'WT.cond1': 10, 'mut.cond1': 12}\nprint(experimental_replicte_list)\n\nprint(experimental_replicate_list['WT'])\n\n# Exercise:\n# Edit the following code such that we get replicate numbers for conditions involving WT\nmy_keys = experimental_replicate_list.keys()\nprint(my_keys)\nmy_keys_of_interest = [the_key for the_key in my_keys if 'mut' in the_key] # edit this line\nprint(my_keys_of_interest)\nprint([experimental_replicate_list[sel_key] for sel_key in my_keys_of_interest])\n\n# The above code uses several lines, can you do this in one line?\nprint(...) # edit this line\n\n\nSpecial maths (Carpentries)\nIn Python 3, the // operator performs integer (whole-number) floor division, the / operator performs floating-point division, and the % (or modulo) operator calculates and returns the remainder from integer division:\nprint('5 // 3:', 5 // 3)\nprint('5 / 3:', 5 / 3)\nprint('5 % 3:', 5 % 3)\nOUTPUT:\n5 // 3: 1\n5 / 3: 1.6666666666666667\n5 % 3: 2\nIf num_subjects is the number of subjects taking part in a study, and num_per_survey is the number that can take part in a single survey, write an expression that calculates the number of surveys needed to reach everyone once.\n\n\nComplex numbers (Carpentries)\nPython provides complex numbers, which are written as 1.0+2.0j. If val is a complex number, its real and imaginary parts can be accessed using dot notation as val.real and val.imag.\na_complex_number = 6 + 2j\nprint(a_complex_number.real)\nprint(a_complex_number.imag)\n\nQuestions:\n\nWhy do you think Python uses j instead of i for the imaginary part?\nWhat do you expect 1 + 2j + 3 to produce?\nWhat do you expect 4j to be? What about 4 j or 4 + j?\n\n\n\n\n\n\n\n\n\nExercises Lesson 4, functions (1)\n\nOrder\n\nExplain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc.\nWhat is the final value of radiance?\n\nradiance = 1.0\nradiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))\n\n\nLast string character\nIf Python starts counting from zero, and len returns the number of characters in a string, what index expression will get the last character in the string name? (Note: we will see a simpler way to do this in a later episode.)\n\n\n\n\n\n\n\nAdditional exercises Lesson 4\n\nWhy not?\nWhy is it that max and min do not return None when they are called with no arguments?\n\n\nSpot the difference\nPredict what each of the print statements in the program below will print. Does max(len(rich), poor) run or produce an error message? If it runs, does its result make any sense?\neasy_string = \"abc\"\nprint(max(easy_string))\nrich = \"gold\"\npoor = \"tin\"\nprint(max(rich, poor))\nprint(max(len(rich), len(poor)))\n\n\n\n(Lesson 5 is a break.)\n\n\n\n\n\n\n\n\nExercises Lesson 6, libraries\n\nExplore\n\nWhat function from the math module can you use to calculate a square root without using sqrt?\nSince the library contains this function, why does sqrt exist?\n\n\n\nFind the right module\nYou want to select a random character from a string:\nbases = 'ACTTGCTTGAC'\n\nWhich standard library module could help you?\nWhich function would you select from that module? Are there alternatives?\nTry to write a program that uses the function.\n\n\n\nHelp!\nWhen a colleague of yours types help(math), Python reports an error:\nNameError: name 'math' is not defined\nWhat has your colleague forgotten to do?\n\n\nImporting with aliases\n\nFill in the blanks so that the program below prints 90.0.\nRewrite the program so that it uses import without as.\nWhich form do you find easier to read?\n\nimport math as m\nangle = ____.degrees(____.pi / 2)\nprint(____)\n\n\nMultiple ways of importing\nMatch the following print statements with the appropriate library calls.\nPrint commands:\n\nprint(\"sin(pi/2) =\", sin(pi/2))\nprint(\"sin(pi/2) =\", m.sin(m.pi/2))\nprint(\"sin(pi/2) =\", math.sin(math.pi/2))\n\nLibrary calls:\n\nfrom math import sin, pi\nimport math\nimport math as m\nfrom math import *\n\n\n\n\n\n\n\n\nAdditional Exercises Lesson 6, Libraries\n\n“Jigsaw”: progamming example\nRearrange the following statements so that a random DNA base is printed and its index in the string. Not all statements may be needed. Feel free to use/add intermediate variables.\nbases=\"ACTTGCTTGAC\"\nimport math\nimport random\n___ = random.randrange(n_bases)\n___ = len(bases)\nprint(\"random base \", bases[___], \"base index\", ___)\n\n\nImporting specific items\n\nFill in the blanks so that the program below prints 90.0.\nDo you find this version easier to read than preceding ones?\nWhy wouldn’t programmers always use this form of import?\n\n____ math import ____, ____\nangle = degrees(pi / 2)\nprint(angle)\n\n\nReading error messages\nRead the code below and try to identify what the errors are without running it. Run the code, and read the error message. What type of error is it?\nfrom math import log\nlog(0)\n\n\nWrite your own function and import it (bioDSC)\nWhen you have a notebook file, you can also create another file, with a .py extension, and write functions in that file. The .py file can be imported like a library, and the functions in the file can be used as if they came from a library.\n\nExercise\nUsing the information below, try to - create two files, one .ipynb (notebook) file and one .py (python plain text code) file. - rename the myfunctionname functions in the .py file and use them in the notebook. - create a third function, which returns C when you provide A and B, assuming A^2+B^2 = C^2, and use it in your notebook.\nUseful things to know: - You can also make .py files. Unlike notebooks, every text here is assumed to be python code. - For Jypiter notebooks: - You can make a .py file with file &gt; new &gt; python file.\n- Save the file to myfilename.py (replacing myfilename with your own favorite name). - For Google colabs: - To create a .py file, right click in the file overview (where you also put the gapminder .csv files), and select ‘new file’. Then create a file ‘myfilename.py’ and double click to edit it. - You can import your file in a python notebook using: - import myfilename where myfilename.py should exist and hold your code. You can also put your file in a directory, but then you need to import it like import mydirectoryname.myfilename.\nYou can write a function using the following template:\n\ndef myfunctionname():\n    print(\"hello world\")\n    \ndef myfunctionname2(input1, input2):\n    print(\"input 1 = \", input1, ', input 2 = ', input2)\n\n\n\n\n\n\n\n\nExercises lesson 7, dataframes (1)\n\nReading other data\nRead the data in gapminder_gdp_americas.csv (which should be in the same directory as gapminder_gdp_oceania.csv) into a variable called data_americas and display its summary statistics.\n\n\nInspecting data\nAfter reading the data for the Americas, use help(data_americas.head) and help(data_americas.tail) to find out what DataFrame.head and DataFrame.tail do.\n\nWhat method call will display the first three rows of this data?\nWhat method call will display the last three columns of this data? (Hint: you may need to change your view of the data.)\n\n\n\nNavigating directories\nThe data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis:\nyour_home_directory\n+-- field_data/\n|   +-- microbes.csv\n+-- thesis/\n    +-- analysis.ipynb\nWhat value(s) should you pass to read_csv to read microbes.csv in analysis.ipynb?\n\n\nWriting data\nAs well as the read_csv function for reading data from a file, Pandas provides a to_csv function to write dataframes to files. Applying what you’ve learned about reading from files, write one of your dataframes to a file called processed.csv. You can use help to get information on how to use to_csv.\n\n\n\n\n\n\n\nAdditional exercises for Lesson 7, dataframes (1)\n\nCreate a plain text file on your computer, and give it the extension .csv.\nFind out what the comma-separated format looks like.\nUse your imagination to complete the following table and put it in the .csv file.\n\n\n\n\nreplicate\ncond1\ncond2\n\n\n\n\n\n10\n\n\n\n\n11\n\n\n\n\n10\n\n\n\n\n12\n\n\n\n\n13\n\n\n\n\n13\n\n\n\n\n\nNow try to read in that table in your python notebook.\nGet the following code to run on your dataframe (referred to as df below):\n\nfrom scipy.stats import ttest_ind\nt_stat, p_value = ttest_ind(df['cond1'], df['cond2'])\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nWhat does that code do?\nLet’s do some unethical data massaging, and tweak the csv such that you get a significant p-val.\n\n\n\n\n\n\n\nExercises for lesson 8, dataframes (2)\n\nSelection individual values\nImport data for europe:\nimport pandas as pd\ndata_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\nFind the GDP Capita of Serbia in 2007.\n\n\nExtent of slicing\n\nDo the two statements below produce the same output?\nBased on this, what rule governs what is included (or not) in numerical slices and named slices in Pandas?\n\nprint(data_europe.iloc[0:2, 0:2])\nprint(data_europe.loc['Albania':'Belgium', 'gdpPercap_1952':'gdpPercap_1962'])\n\n\nReconstructing Data\nExplain what each line in the following short program does: what is in first, second, etc.?\nfirst = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsecond = first[first['continent'] == 'Americas']\nthird = second.drop('Puerto Rico')\nfourth = third.drop('continent', axis = 1)\nfourth.to_csv('result.csv')\n\n\nSelecting Indices\nExplain in simple terms what idxmin and idxmax do in the short program below. When would you use these methods?\ndata = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\nprint(data.idxmin())\nprint(data.idxmax())\n\n\nPractice with Selection\nAssume Pandas has been imported and the Gapminder GDP data for Europe has been loaded. Write an expression to select each of the following:\n\nGDP per capita for all countries in 1982.\nGDP per capita for Denmark for all years.\nGDP per capita for all countries for years after 1985.\nGDP per capita for each country in 2007 as a multiple of GDP per capita for that country in 1952.\n\n\n\nMany Ways of Access\nThere are at least two ways of accessing a value or slice of a DataFrame: by name or index. However, there are many others. For example, a single column or row can be accessed either as a DataFrame or a Series object.\nSuggest different ways of doing the following operations on a DataFrame:\n\nAccess a single column\nAccess a single row\nAccess an individual DataFrame element\nAccess several columns\nAccess several rows\nAccess a subset of specific rows and columns\nAccess a subset of row and column ranges\n\n\n\n\n\n\n\n\nAdditional exercises Lesson 8, dataframes (2)\n\nEpicardial cells\n\nA\nIn the RNA-seq data, we can create another column that reflects the condition of the cells, WT or mutant. Fill in the blanks to achieve this:\ndf_cells_kohela2['Condition'] = ____\n\ndf_cells_kohela2.loc[df_cells_kohela2.index.str.contains('WT_'), 'Condition'] = ____\ndf_cells_kohela2.loc[df_cells_kohela2.index._______, _________] = ______\n\n\n\nB\nWhat is the difference between str.contains and str.match?\n\n\nC\nNow again calculate the mean value of TFAP2A expression in WT cells vs. mutant cells. Does there appear to be more TFAP2A expression in the mutant cells?\n\n\n\n\nGene expression (bioDSC)\n\nConvert the data below to a file you can import (e.g.: csv, tsv), import it to a pandas df, and determine the following:\n\nThe average CRP gene expression per condion.\nThe corresponding standard deviations.\nThe log2-fold change between WT, condition A, and condition B.\nDo the same for ACTA1.\nNormalize all gene expression levels to their average respective wild type levels.\n\n\ngene    expression  condition\nCRP 873 WT\nCRP 324 WT\nCRP 214 WT\nCRP 151 WT\nCRP 1220    A\nCRP 450 A\nCRP 300 A\nCRP 210 A\nCRP 800 B\nCRP 200 B\nCRP 200 B\nCRP 130 B\nACTA1   7457    WT\nACTA1   2342    WT\nACTA1   8000    WT\nACTA1   9000    WT\nACTA1   6500    A\nACTA1   2200    A\nACTA1   7500    A\nACTA1   8000    A\nACTA1   1000    B\nACTA1   1123    B\nACTA1   3211    B\nACTA1   1231    B\n\n\nGDPs (bioDSC)\n\nBetween ’87 and ’92 the GDP of most countries took a hit. Are there any countries which had a positive increase between those two years? Which ones?\nCalculate the average GDP between all European countries per year.\n\nNormalize the dataframe by this trend.\n\n\n\n\nExploring available methods using the dir() function\nPython includes a dir() function that can be used to display all of the available methods (functions) that are built into a data object. In Episode 4, we used some methods with a string. But we can see many more are available by using dir():\nmy_string = 'Hello world!'   # creation of a string object \ndir(my_string)\nThis command returns:\n['__add__',\n...\n'__subclasshook__',\n'capitalize',\n'casefold',\n'center',\n...\n'upper',\n'zfill']\nYou can use help() or Shift+Tab to get more information about what these methods do.\nAssume Pandas has been imported and the Gapminder GDP data for Europe has been loaded as data. Then, use dir() to find the function that prints out the median per-capita GDP across all European countries for each year that information is available.\n\n\nInterpretation\nPoland’s borders have been stable since 1945, but changed several times in the years before then. How would you handle this if you were creating a table of GDP per capita for Poland for the entire twentieth century?\n\n\n\n\n\n\n\nExercises Lesson 9, plotting\nModified by bioDSC\n\nMinima and Maxima (Carpentries, modified)\nFill in the blanks below to plot the minimum GDP per capita over time for all the countries in Europe.\nModify it again to plot the maximum GDP per capita over time for Europe, you need to edit the code beyond the ___ for this.\ndata_europe = pd.read_csv('/Users/m.wehrens/Desktop/python_course/data/gapminder_gdp_europe.csv', index_col='country')\ndata_europe_transposed = data_europe.T\n\ndata_europe_transposed['min'] = data_europe.____\ndata_europe_transposed['max'] = ____\ndata_europe_transposed['year'] = ____\n\nsns.lineplot(data_europe_transposed, x='year', y='min')\nsns.lineplot(data_europe_transposed, x='year', y='max')\nplt.legend(loc='best')\nplt.xticks(rotation=90)\nbioDSC HINT: if you don’t see the solution, take it step by step. Break down the task in subtasks, and adress the first step towards the solution first. Try that first. Running code is free.\n\n\nMean gene expression (bioDSC)\nUse the kohela data again:\n# Load data, note the \".T\" at the end here\ndf_kohela = pd.read_csv('data/kohela-et-al.csv', index_col=0).T\n# create new 'masks'\nepicardial_cells = df_kohela['WT1']&gt;3\nfibroblast_cells = df_kohela['COL2A1']&gt;30\nfat_cells = df_kohela['PPARG']&gt;2\n# Add cell type\ndf_kohela['Celltype'] = 'unknown'\ndf_kohela.loc[epicardial_cells,'Celltype'] = 'epicardial'\ndf_kohela.loc[fibroblast_cells, 'Celltype'] = 'fibroblast'\ndf_kohela.loc[fat_cells, 'Celltype'] = 'fat'\n# Add conditions\ndf_kohela['Condition'] = 'unknown'\ndf_kohela.loc[df_kohela.index.str.contains('WT_'), 'Condition'] = 'WT'\ndf_kohela.loc[df_kohela.index.str.contains('mutant_'), 'Condition'] = 'mutant'\n\n\nA\nUse seaborn to create a ‘stripplot’ plot for WT1 expression per cell type. Then create a similar plot for TBX18. (Epicardial cell markers.) What information can be extracted from this plot?\n\n\n\nB\nNow create a scatter plot, showing WT1 expression vs. TBX18 expression across all cells. What does this tell us?\n\n\n\nC\nColor the scatter plot per cell type. What does this tell us?\n\n\nD\nCalculate the total RNA-seq reads per cell, and make a violin plot per condition for these total reads.\nSome code to start with:\ndf_kohela['Total_reads'] = df_kohela.loc[:,'A1BG':'ZZZ3']._.____\nWhat does the violin plot tell us?\n\n\nMore Correlations (Carpentries, modified)\nThis short program creates a plot showing the correlation between GDP and life expectancy for 2007, normalizing marker size by population:\ndata_all = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsns.scatterplot(data_all, x='gdpPercap_2007', y='lifeExp_2007', s=data_all['pop_2007']/1e6)\nUsing online help and other resources, explain what each argument to plot does.\n\n\nEven more correlations (bioDSC)\n\nA\nUse the code from the “More Correlations” exercise, and try to add the following lines to the plotting code:\nplt.text(data_all.loc['United States','gdpPercap_2007'], data_all.loc['United States','lifeExp_2007'], 'United States')\nplt.text(data_all.loc['Netherlands','gdpPercap_2007'], data_all.loc['Netherlands','lifeExp_2007'], 'Netherlands')\n\nWhat’s happening here? (You might need to use Google.)\nAdd your favorite country too.\n\n\n\nB\nHow would you add labels for the top 10 GDP countries?\nAnswer: this would be very tedious with what you learned currently! In the next lessons (particularly lesson 12), we’ll learn how to automate your code. This will be very useful for this particular challenge.\n\n\n\n\n\n\n\n\nAdditional Exercises Lesson 9, plotting\nModified by bioDSC\n\nSubselection and melting\n\nA\nUsing the kohela data, first create a new dataframe df_kohela_sel with only a selection of a few genes, and the condition and cell type columns.\nSelect the following genes: ['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA']. These are epicardial markers (WT1, TBX18), a transcription factor (TFAP2A), fibroblast markers (COL2A1, ACTA2), and fat markers (PPARG, CEBPA).\n\n\n\nB\nNow melt this dataframe using pd.melt(), and use cell type and condition as identifier variables. What will happen to the gene expression values? What is sensible input for the var_name and value_name parameters? Why is this useful? (For answer, see next questions.)\n\n\n\nC\nUse df_kohela_melted.head() to check whether the output is as expected.\n\n\nD\nNow make a violin or stripplot, with as x-axis the genes, expression on the y-axis, and colored for condition. Is there an issue with this plot?\n\n\n\nE\nLook at the following example:\nimport numpy as np\n\n# A custom function, which normalizes a series by its mean\n# We'll learn more about functions in Lesson 16\ndef gene_normalization(X):\n    return X / np.mean(X)\n\n# Create a subset of the data\ncell_subset = ['mutant_rep1_cell174', 'WT_rep2_cell348', 'mutant_rep1_cell160',\n       'WT_rep1_cell022', 'mutant_rep1_cell069']\ngene_subset = ['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA']\n\n# Normalize gene expression\ndf_kohela_subset2 = df_kohela.loc[cell_subset, gene_subset]\ndf_kohela_subset2_normalized = df_kohela_subset2.transform(gene_normalization)\n\n# Print the result\nprint(df_kohela_subset2_normalized)\n\nCheck out what gene_normalization(df_kohela_subset2['WT1']) does.\nWhat does the transform method do in the above code?\n\n\n\nF\nEdit the following code (replacing blanks by code) to normalize the gene expression by the total expression of each gene. Hint: look at exercise E.\ndf_kohela_grouped = df_kohela_melted.groupby(_______)\ndf_kohela_melted['Expression_normalized'] = df_kohela_grouped['Expression']._______(gene_normalization)\nThen, similar to D, plot the normalized gene expression using both the sns.barplot and sns.stripplot. For the stripplot, use the additional parameter dodge=True.\nThe barplot looks nice, but does it contain all information?\n\n\n\nG\nFinally, you might want to change the order in these plots. Use the following line:\ndf_kohela_melted['Condition'] = pd.Categorical(df_kohela_melted['Condition'], categories=['WT', 'mutant'], ordered=True)\nAnd then run your plotting code again. What has happened?\n\n\n\nSaving your plot\nYou might want to save your plot. You can use the plt.savefig function for this.\nCheck out this code with some additional convenient options. Change '/your/location/your-filename.pdf' to a convenient path where you save your figure.\nimport matplotlib.pyplot as plt\n\n# Bang Wong colorblind-friendly color scheme (https://www.nature.com/articles/nmeth.1618)\ncolors_bangwong = [\n    \"#E69F00\",  # Orange\n    \"#56B4E9\",  # Sky Blue\n    \"#009E73\",  # Bluish Green\n    \"#F0E442\",  # Yellow\n    \"#0072B2\",  # Blue\n    \"#D55E00\",  # Vermillion\n    \"#CC79A7\",  # Reddish Purple\n    \"#000000\"   # Black\n]\n\nplt.style.use('default')\nfig, ax = plt.subplots(1,1, figsize=(10/2.54,10/2.54))\nax.plot([1,2,3,4], [1,4,9,16], linestyle='--', color=colors_bangwong[1], label=r'$x^2$')\nax.plot([1,2,3,4], [1,5,11,19], linestyle=':', color=colors_bangwong[2], label=r'$x^2+(x-1)$')\nax.legend()\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_title('Sample Plot', fontsize=12)\nax.legend(fontsize=12)\nax.tick_params(axis='both', which='major', labelsize=12)\nplt.tight_layout()\n\nplt.savefig('/your/location/your-filename.pdf', dpi=300, bbox_inches='tight')\nplt.close(fig)\n\n# Use this command to show the figure when not using (Jupyter) notebooks.\n# plt.show()\n\n\nCorrelations (Carpentries, modified)\nModify the code from “Minima and Maxima” exercise to create a scatter plot showing the relationship between the minimum and maximum GDP per capita across the countries in Asia, with each point in the plot corresponding to a year. What relationship do you see (if any)?\n\n\nCorrelations (continued) (Carpentries, modified)\nYou might note that the variability in the maximum is much higher than that of the minimum. Take a look at the maximum and the max indexes:\ndata_asia = pd.read_csv('data/gapminder_gdp_asia.csv', index_col='country')\n\ndf_max_GDP = pd.DataFrame()\ndf_max_GDP['GDP_max'] = data_asia.max()\ndf_max_GDP['Year']    = data_asia.columns.str.replace('gdpPercap_','').astype(int)\n\nplt.plot(df_max_GDP['Year'], df_max_GDP['GDP_max'])\n\nprint(data_asia.idxmax())\nprint(data_asia.idxmin())\n\n\nNormalized dataframe (bioDSC)\n\nIn the previous lesson about dataframes (in the additional exercises), we normalized the GDP data against the average trend. Plot the data from this normalized dataframe.\n\nIs this helpful in any way?\n\n\n\n\nCrude oil (bioDSC)\nCrude oil prices can be found at: https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=F000000__3&f=A\nThis gives the data below:\nDecade  Year-0  Year-1  Year-2  Year-3  Year-4  Year-5  Year-6  Year-7  Year-8  Year-9\n  1850's                                        16.00\n  1860's    9.59    0.49    1.05    3.15    8.06    6.59    3.74    2.41    3.62    5.64\n  1870's    3.86    4.34    3.64    1.83    1.17    1.35    2.52    2.38    1.17    0.86\n  1880's    0.94    0.92    0.78    1.10    0.85    0.88    0.71    0.67    0.65    0.77\n  1890's    0.77    0.56    0.51    0.60    0.72    1.09    0.96    0.68    0.80    1.13\n  1900's    1.19    0.96    0.80    0.94    0.86    0.62    0.73    0.72    0.72    0.70\n  1910's    0.61    0.61    0.74    0.95    0.81    0.64    1.10    1.56    1.98    2.01\n  1920's    3.07    1.73    1.61    1.34    1.43    1.68    1.88    1.30    1.17    1.27\n  1930's    1.19    0.65    0.87    0.67    1.00    0.97    1.09    1.18    1.13    1.02\n  1940's    1.02    1.14    1.19    1.20    1.21    1.22    1.41    1.93    2.60    2.54\n  1950's    2.51    2.53    2.53    2.68    2.78    2.77    2.79    3.09    3.01    2.90\n  1960's    2.88    2.89    2.90    2.89    2.88    2.86    2.88    2.92    2.94    3.09\n  1970's    3.18    3.39    3.39    3.89    6.87    7.67    8.19    8.57    9.00    12.64\n  1980's    21.59   31.77   28.52   26.19   25.88   24.09   12.51   15.40   12.58   15.86\n  1990's    20.03   16.54   15.99   14.25   13.19   14.62   18.46   17.23   10.87   15.56\n  2000's    26.72   21.84   22.51   27.56   36.77   50.28   59.69   66.52   94.04   56.35\n  2010's    74.71   95.73   94.52   95.99   87.39   44.39   38.29   48.05   61.40   55.59\n  2020's    36.86   65.84   93.97   76.10                       \nSave that data to a .tsv file, and upload it.\nNow try to understand the code below:\nimport pandas as pd\n\n# Load the data\ndf_crudeoil = \\\n    pd.read_csv('/Users/m.wehrens/Data_UVA/2024_teaching/2025-03-gapminder/crude_oil/crude_oil_prices.tsv', sep='\\t')\n\n# reshape the data, such that it becomes a long list\ndf_crudeoil_melted = df_crudeoil.melt(id_vars='Decade', var_name='lastdigit')\n\n# now reformat the year information\n# search and replace first\ndf_crudeoil_melted.loc[:,'Decade'] = df_crudeoil_melted.loc[:,'Decade'].str.replace(\"0's\",'')\ndf_crudeoil_melted.loc[:,'lastdigit'] = df_crudeoil_melted.loc[:,'lastdigit'].str.replace('Year-','')\n# now combine information to create a new column \"Year\"\ndf_crudeoil_melted.loc[:,'Year'] = df_crudeoil_melted.loc[:,'Decade'] + df_crudeoil_melted.loc[:,'lastdigit']\n# Inspect the result\nprint(df_crudeoil_melted.head())\nUsing your new plotting skills, compare this data against the trends in the Asian GDPs showed earlier.\n\n\n\n(Lesson 10 is a break.)\n\n\n\n\n\n\n\n\nExercises Lesson 11, Lists\n\nFill in the blanks\nFill in the blanks so that the program below produces the output shown.\nvalues = ____\nvalues.____(1)\nvalues.____(3)\nvalues.____(5)\nprint('first time:', values)\nvalues = values[____]\nprint('second time:', values)\nOutput:\nfirst time: [1, 3, 5]\nsecond time: [3, 5]\n\n\nHow Large is a Slice?\nIf start and stop are both non-negative integers, how long is the list values[start:stop]?\n\n\nFrom Strings to Lists and Back\nGiven this:\nprint('string to list:', list('tin'))\nprint('list to string:', ''.join(['g', 'o', 'l', 'd']))\nOUTPUT:\nstring to list: ['t', 'i', 'n']\nlist to string: gold\n\nWhat does list(‘some string’) do?\nWhat does ‘-’.join([‘x’, ‘y’, ‘z’]) generate?\n\n\n\nWorking With the End\nWhat does the following program print?\nelement = 'helium'\nprint(element[-1])\n\nHow does Python interpret a negative index?\nIf a list or string has N elements, what is the most negative index that can safely be used with it, and what location does that index represent?\nIf values is a list, what does del values[-1] do?\nHow can you display all elements but the last one without changing values? (Hint: you will need to combine slicing and negative indexing.)\n\n\n\nStepping Through a List\nWhat does the following program print?\nelement = 'fluorine'\nprint(element[::2])\nprint(element[::-1])\n\nIf we write a slice as low:high:stride, what does stride do?\nWhat expression would select all of the even-numbered items from a collection?\n\n\n\n\n\n\n\n\nAdditional Exercises Lesson 11, Lists\n\nSlice Bounds\nWhat does the following program print?\nelement = 'lithium'\nprint(element[0:20])\nprint(element[-1:3])\n\n\nSort and Sorted\nWhat do these two programs print? In simple terms, explain the difference between sorted(letters) and letters.sort().\n# Program A\nletters = list('gold')\nresult = sorted(letters)\nprint('letters is', letters, 'and result is', result)\n# Program B\nletters = list('gold')\nresult = letters.sort()\nprint('letters is', letters, 'and result is', result)\n\n\nCopying (or Not)\nWhat do these two programs print? In simple terms, explain the difference between new = old and new = old[:].\n# Program A\nold = list('gold')\nnew = old      # simple assignment\nnew[0] = 'D'\nprint('new is', new, 'and old is', old)\n# Program B\nold = list('gold')\nnew = old[:]   # assigning a slice\nnew[0] = 'D'\nprint('new is', new, 'and old is', old)\n\n\nGo to lesson 3 (bioDSC)\n\nDo the additional exercises of lesson 3 if you haven’t already.\n\n\n\nNegative slicing (bioDSC)\nelement = 'lithium'\nWhat does element[-7:3] print and why?\nExplanation: https://www.geeksforgeeks.org/slicing-with-negative-numbers-in-python/\n\n\nMore list comprehensions and filtering (bioDSC)\n\nEdit the following code such that the list consists of values 2x+x^2-1 where x is the index of the list element.\n\n[x for x in range(10)]\n\nNow from that list, select values that are &gt; 10, by modifying the following code:\n\n[x for x in your_list if ______]\n\nConvert your list to a np.array, and do the same in a more elegant way.\nGiven: list_withtop = [1000+-10*(x-7)**2 for x in range(20)]\n\nFind the position of the maximum value in this array.\nEdit the code above such that the maximum value shifts to an index of your choice.\n\nCheck whether you succeedded by finding the maximum value.\n\nMultiply your list with -1, and put the result in another list.\n\nWhere are now the maximum and minimum values?\nDoes this make sense?\n\nlist_line = [70*x-1000 for x in range(20)]\n\nWhat’s the biggest value, either negative or positive, in this list?\nAnd the index of that number?\nWhat’s the standard deviation?\nCan you calculate the correlation between list_withtop and list_line?\nCan you make a scatter plot of list_withtop versus list_line?\n\n\n\n\n\n\n\n\n\n\nExercises Lesson 12, Loops\n\nTracing Execution\nCreate a table showing the numbers of the lines that are executed when this program runs, and the values of the variables after each line is executed.\ntotal = 0\nfor char in \"tin\":\n    total = total + 1\n\n\n\nStep\nLine Number\nVariable Values\n\n\n\n\n1\n1\ntotal = 0\n\n\n2\n..\ntotal = 0, char = ‘t’\n\n\n3\n..\n..\n\n\n4\n..\n..\n\n\n5\n..\n..\n\n\n..\n..\n..\n\n\n\n\n\nPractice Accumulating\nFill in the blanks in each of the programs below to produce the indicated result.\n# Total length of the strings in the list: [\"red\", \"green\", \"blue\"] =&gt; 12\ntotal = 0\nfor word in [\"red\", \"green\", \"blue\"]:\n    ____ = ____ + len(word)\nprint(total)\n\n\nPractice Accumulating 2 (continued)\n# List of word lengths: [\"red\", \"green\", \"blue\"] =&gt; [3, 5, 4]\nlengths = ____\nfor word in [\"red\", \"green\", \"blue\"]:\n    lengths.____(____)\nprint(lengths)\n\n\nPractice Accumulating 3 (continued)\n# Concatenate all words: [\"red\", \"green\", \"blue\"] =&gt; \"redgreenblue\"\nwords = [\"red\", \"green\", \"blue\"]\nresult = ____\nfor ____ in ____:\n    ____\nprint(result)\n\n\n\n\n\n\n\nAdditional Exercises Lesson 12, Loops\n\nPlotting automation\nRemember this code from Lesson 9?\ndata_all = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsns.scatterplot(data_all, x='gdpPercap_2007', y='lifeExp_2007', s=data_all['pop_2007']/1e6)\nplt.text(df_all.loc['United States','gdpPercap_2007'], df_all.loc['United States','lifeExp_2007'], 'United States')\nplt.text(df_all.loc['Netherlands','gdpPercap_2007'], df_all.loc['Netherlands','lifeExp_2007'], 'Netherlands')\nTry to annotate 10 selected countries automatically.\n\n\nReversing a String\nFill in the blanks in the program below so that it prints “nit” (the reverse of the original character string “tin”).\noriginal = \"tin\"\nresult = ____\nfor char in original:\n    result = ____\nprint(result)\nbioDSC hint If this is challenging: - try to first reproduce the word tin in result, using this loop. - Use a similar approach as the examples we used.\n\n\nPractice Accumulating (continued from above)\nCreate an acronym: Starting from the list [“red”, “green”, “blue”], create the acronym “RGB” using a for loop.\n\nbioDSC remark: Note the capitals in “RGB”!\nHint: You may need to use a string method to properly format the acronym.\n\n\n\nIdentifying Item Errors\n\nRead the code below and try to identify what the errors are without running it.\nRun the code, and read the error message. What type of error is it?\nFix the error.\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[4])\n\n\nCumulative Sum (code puzzle)\nReorder and properly indent the lines of code below so that they print a list with the cumulative sum of data. The result should be [1, 3, 5, 10].\ncumulative.append(total)\nfor number in data:\ncumulative = []\ntotal = total + number\ntotal = 0\nprint(cumulative)\ndata = [1,2,2,5]\n\n\nIdentifying Variable Name Errors\n\nRead the code below and try to identify what the errors are without running it.\nRun the code and read the error message. What type of NameError do you think this is? Is it a string with no quotes, a misspelled variable, or a variable that should have been defined but was not?\nFix the error.\nRepeat steps 2 and 3, until you have fixed all the errors.\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (Number % 3) == 0:\n        message = message + a\n    else:\n        message = message + \"b\"\nprint(message)\n\n\nClassifying Errors\nIs an indentation error a syntax error or a runtime error?\n\n\n\n\n\n\n\nExercises Lesson 13, Conditionals\n\nTracing Execution\nWhat does this program print?\npressure = 71.9\nif pressure &gt; 50.0:\n    pressure = 25.0\nelif pressure &lt;= 50.0:\n    pressure = 0.0\nprint(pressure)\n\n\nTrimming Values\nFill in the blanks so that this program creates a new list containing zeroes where the original list’s values were negative and ones where the original list’s values were positive.\noriginal = [-1.5, 0.2, 0.4, 0.0, -1.3, 0.4]\nresult = ____\nfor value in original:\n    if ____:\n        result.append(0)\n    else:\n        ____\nprint(result)\nDesired output:\n[0, 1, 1, 1, 0, 1]\n\n\nProcessing Small Files\nModify this program so that it only processes files with fewer than 50 records.\nimport glob\nimport pandas as pd\nfor filename in glob.glob('data/*.csv'):\n    contents = pd.read_csv(filename)\n    ____:\n        print(filename, len(contents))\n\n\n\n\n\n\n\nAdditional Exercises Lesson 13, Conditionals\n\nList comprehension (bioDSC)\n\nA\nAdapt the following code to select only positive values:\nexample_list = [1,2,3,4,-5,1,34,6,-10, 39]\nexample_list_pos = [___ for item in example_list if ___]\nprint(example_list_pos)\n\n\nB\nUse the same code, but: - select values between 30 and 40 - select items &lt;0 or &gt;10\n\n\nC\nUse a np.array (see additional exercises Lesson 3) to do the same more elegantly.\n\n\n\nEnumerate, zip (bioDSC)\nThese exercises introduce two new concepts. You might need google.\n\nA\nWhat does the following code do? What is the meaning of the output?\nfor idx, item in enumerate([1,2,3,4,-5,1,34,6,-10]):\n    \n    if item&gt;5:\n        print(idx)\n\n\nB\nModify the following code such that it will compare each item i in apples with each item i in pears, and tell you which one is heavier. You need to edit the code.\napples = [123, 436, 123, 654, 117, 193, 120]\npears  = [543, 163, 178, 165, 123, 187, 190]\n\nfor apple_weight, pear_weight in zip(apples, pears):\n    print('='*10)\n    print('weigth apple: ', apple_weight)\n    print('weigth pear: ',pear_weight)\n    \n    print('the XXX is heavier')\n\n\n\nInitializing\nModify this program so that it finds the largest and smallest values in the list no matter what the range of values originally is.\nvalues = [...some test data...]\nsmallest, largest = None, None\nfor v in values:\n    if ____:\n        smallest, largest = v, v\n    ____:\n        smallest = min(____, v)\n        largest = max(____, v)\nprint(smallest, largest)\nWhat are the advantages and disadvantages of using this method to find the range of the data?\nbioDSC hints\nThe loop could also look as follows:\nvalues = [...some test data...]\nsmallest, largest = None, None\nfor v in values:\n    smallest = min(____, v)\n    largest = max(____, v)\nWhy wouldn’t this work?\nThis is why the if statement is needed.\nHow can we test whether we are in the first iteration?\n\n\n\n\n\n\n\nExercises Lesson 14, Looping over data\n\nDetermining Matches\nWhich of these files is not matched by the expression glob.glob(‘data/as.csv’)?\ndata/gapminder_gdp_africa.csv\ndata/gapminder_gdp_americas.csv\ndata/gapminder_gdp_asia.csv\n\n\nMinimum File Size\nModify this program so that it prints the number of records in the file that has the fewest records.\nimport glob\nimport pandas as pd\nfewest = ____\nfor filename in glob.glob('data/*.csv'):\n    dataframe = pd.____(filename)\n    fewest = min(____, dataframe.shape[0])\nprint('smallest file has', fewest, 'records')\nNote that the DataFrame.shape() method returns a tuple with the number of rows and columns of the data frame.\n\n\n\n\n\n\n\nAdditional Exercises Lesson 14, Looping over data\n\nComparing Data\nWrite a program that reads in the regional data sets and plots the average GDP per capita for each region over time in a single chart.\nPandas will raise an error if it encounters non-numeric columns in a dataframe computation so you may need to either filter out those columns or tell pandas to ignore them.\n\n\n\n(Lesson 15 is a break.)\n\n\n\n\n\n\n\n\nExercises Lesson 16, Functions\n\nIdentifying Syntax Errors\nRead the code below and try to identify what the errors are without running it. Run the code and read the error message. Is it a SyntaxError or an IndentationError? Fix the error.\ndef another_function\n  print(\"Syntax errors are annoying.\")\n   print(\"But at least python tells us about them!\")\n  print(\"So they are usually not too hard to fix.\")\n\n\nWhat does the following program print?\ndef report(pressure):\n    print('pressure is', pressure)\n\nprint('calling', report, 22.5)\n\n\nEncapsulation\nFill in the blanks to create a function that takes a single filename as an argument, loads the data in the file named by the argument, and returns the minimum value in that data.\nimport pandas as pd\n\ndef min_in_data(____):\n    data = ____\n    return ____\n\n\n\n\n\n\n\nAdditional Exercises Lesson 16, Functions\n\nOrder of Operations\nWhat’s wrong in this example?\nresult = print_time(11, 37, 59)\n\ndef print_time(hour, minute, second):\n   time_string = str(hour) + ':' + str(minute) + ':' + str(second)\n   print(time_string)\nAfter fixing the problem above, explain why running this example code:\nresult = print_time(11, 37, 59)\nprint('result of call is:', result)\ngives this output:\n11:37:59\nresult of call is: None\nWhy is the result of the call None?\n\n\nFind the First\nFill in the blanks to create a function that takes a list of numbers as an argument and returns the first negative value in the list. What does your function do if the list is empty? What if the list has no negative numbers?\ndef first_negative(values):\n    for v in ____:\n        if ____:\n            return ____\n\n\nCalling by Name\nEarlier we saw this function:\ndef print_date(year, month, day):\n    joined = str(year) + '/' + str(month) + '/' + str(day)\n    print(joined)\nWe saw that we can call the function using named arguments, like this:\nprint_date(day=1, month=2, year=2003)\n\nWhat does print_date(day=1, month=2, year=2003) print?\nWhen have you seen a function call like this before?\nWhen and why is it useful to call functions this way?\n\n\n\nWrite your own function\nSee the exercise “Write your own function and import it” from the additional exercises in lesson 6.\n\n\nOmitted exercises\nContinue with more exercises at: https://swcarpentry.github.io/python-novice-gapminder/instructor/16-writing-functions.html\n(See: “Encapsulation of an If/Print Block”.)\n\n\nPrimes\nWrite a function that looks as follows:\ndef calculate_primes(N):\n    ...\nthat returns an array with prime numbers between 0 and N.\nHints: - Start with writing a function is_number_prime(X), that checks whether \\(X\\) is a prime number. - You probably need the following ingredients for that function: - How to test if a number is divisible by any number? - Use a for loop to test whether \\(X\\) can be divided by all numbers \\(&lt;X\\). - Could be convenient to make smart use of the return function.\ndef is_number_prime(X):\n    \n    # &lt;insert explanatory comment&gt;\n    for y in range(X):\n    \n        # &lt;insert explanatory comment&gt;\n        if ...:\n            return False\n    \n    # &lt;insert explanatory comment&gt;\n    return True\n\ndef calculate_primes(N):\n    ..."
  },
  {
    "objectID": "posts/Ath_synteny.html",
    "href": "posts/Ath_synteny.html",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "",
    "text": "Interesting phenotypes can be encoded in the genome of organisms. Therefore, it is often interesting to visualize differences between genomes of different species, or of individuals within the same species. These differences could be small, such as single nucleotide polymorphisms, but can also be structural rearrangements of entire chromosomes.\nIn a recent publication, Lian et al 2024 Nature Genetics investigated structural rearrangements in a panel of 69 Arabidopsis thaliana accessions (Fig 3). In addition, they published a chromosome level assembly of Arabidopsis accession Oy-0. This accession is used in several research groups of the University of Amsterdam, so it is interesting to show genome rearrangements between this accession, and the model accession Col-0. The paper uses plotsr software to plot synteny across chromosomes. The syntenic regions are first identified using syri.\nSoftware used:\n\nsyri, a software package to identify structural rearrangements.\nplotsr, a software package to plot the rearrangements detected by syri.\nminimap2, a fast aligner, used here to align the two genomes of interest."
  },
  {
    "objectID": "posts/Ath_synteny.html#introduction",
    "href": "posts/Ath_synteny.html#introduction",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "",
    "text": "Interesting phenotypes can be encoded in the genome of organisms. Therefore, it is often interesting to visualize differences between genomes of different species, or of individuals within the same species. These differences could be small, such as single nucleotide polymorphisms, but can also be structural rearrangements of entire chromosomes.\nIn a recent publication, Lian et al 2024 Nature Genetics investigated structural rearrangements in a panel of 69 Arabidopsis thaliana accessions (Fig 3). In addition, they published a chromosome level assembly of Arabidopsis accession Oy-0. This accession is used in several research groups of the University of Amsterdam, so it is interesting to show genome rearrangements between this accession, and the model accession Col-0. The paper uses plotsr software to plot synteny across chromosomes. The syntenic regions are first identified using syri.\nSoftware used:\n\nsyri, a software package to identify structural rearrangements.\nplotsr, a software package to plot the rearrangements detected by syri.\nminimap2, a fast aligner, used here to align the two genomes of interest."
  },
  {
    "objectID": "posts/Ath_synteny.html#the-data",
    "href": "posts/Ath_synteny.html#the-data",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "The data",
    "text": "The data\nI downloaded the Arabidopsis Col-0 TAIR10.1 and Oy-0 genome assemblies with the following script, moved them to a folder called data, and unzipped them:\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat download_genoomes.sh \nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/036/927/085/GCA_036927085.1_ASM3692708v1/GCA_036927085.1_ASM3692708v1_genomic.fna.gz\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.fna.gz\n\nmv GC* data/\ncd data/\ngunzip *.gz\nThe synteny software expects that homologous chromosomes in the two genomes have exactly the same chromosome id. Unfortunately, this is almost never the case if you download genomes from different organisms and from different sources. To rename the fasta headers, I used the following sed commands. In addition, I removed the mitochondrial and chromosomal DNA sequences.\n(syri) [mpaauw@omics-h0 data]$ cat chromosome_renamer.sh \nsed -i 's/CM072627.1.*chromosome 1.*/Chr1/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072628.1.*chromosome 2.*/Chr2/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072629.1.*chromosome 3.*/Chr3/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072630.1.*chromosome 4.*/Chr4/' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i 's/CM072631.1.*chromosome 5.*/Chr5/' GCA_036927085.1_ASM3692708v1_genomic.fna\n\nsed -i 's/NC_003070.9.*chromosome 1.*/Chr1/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003071.7.*chromosome 2.*/Chr2/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003074.8.*chromosome 3.*/Chr3/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003075.7.*chromosome 4.*/Chr4/' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i 's/NC_003076.8.*chromosome 5.*/Chr5/' GCF_000001735.4_TAIR10.1_genomic.fna\n\n(syri) [mpaauw@omics-h0 data]$ cat contig_remover.sh \nsed -i '/&gt;JAWQTX010000006.1/,/^&gt;/d' GCA_036927085.1_ASM3692708v1_genomic.fna\nsed -i '/&gt;JAWQTX010000007.1/,/^&gt;/d' GCA_036927085.1_ASM3692708v1_genomic.fna\n\nsed -i '/&gt;NC_037304.1/,/^&gt;/d' GCF_000001735.4_TAIR10.1_genomic.fna\nsed -i '/&gt;NC_000932.1/,/^&gt;/d' GCF_000001735.4_TAIR10.1_genomic.fna\nLet’s do sanity check: did this work? We do this by using grep, a sophisticated command line search tool, to search for the character &gt;, in all files that are called G*.fna. Note that the * is used as a ‘wildcard’ and we match both genomes with this statement.\n(syri) [mpaauw@omics-h0 data]$ grep \"&gt;\" G*.fna\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr1\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr2\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr3\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr4\nGCA_036927085.1_ASM3692708v1_genomic.fna:&gt;Chr5\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr1\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr2\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr3\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr4\nGCF_000001735.4_TAIR10.1_genomic.fna:&gt;Chr5\nThat looks good. In both genomes, the chromosomes are called Chr1, and so forth."
  },
  {
    "objectID": "posts/Ath_synteny.html#aligning-the-genomes",
    "href": "posts/Ath_synteny.html#aligning-the-genomes",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Aligning the genomes",
    "text": "Aligning the genomes\nThe first step is to do a whole-genome alignment between the two genomes. I followed the instructions at plotsr github to do this. We use the software minimap2. This is a quick sequence alignment program and is in installed in base environment on Crunchomics. The alignments are sorted and indexed using samtools. Then syri is used to call structural variants based on the alignment between the two genomes.\nminimap2 -ax asm5 -t 16 --eqx data/GCF_000001735.4_TAIR10.1_genomic.fna data/GCA_036927085.1_ASM3692708v1_genomic.fna | samtools sort -O BAM &gt; Col_Oy.bam\nsamtools index Col_Oy.bam\nsyri -c Col_Oy.bam -r data/GCF_000001735.4_TAIR10.1_genomic.fna -q data/GCA_036927085.1_ASM3692708v1_genomic.fna -F B --prefix Col_Oy\nThis produces several output files:\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ ls Col_Oysyri.*\nCol_Oysyri.log  Col_Oysyri.out  Col_Oysyri.summary  Col_Oysyri.vcf\nLet’s have a look at the summary.\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat Col_Oysyri.summary \n#Structural annotations\n#Variation_type Count   Length_ref  Length_qry\nSyntenic regions    95  107247075   107294458\nInversions  14  1504073 2295146\nTranslocations  110 566651  566464\nDuplications (reference)    62  267859  -\nDuplications (query)    528 -   898113\nNot aligned (reference) 250 9765159 -\nNot aligned (query) 696 -   20371617\n\n\n#Sequence annotations\n#Variation_type Count   Length_ref  Length_qry\nSNPs    405790  405790  405790\nInsertions  42410   -   904020\nDeletions   41893   867402  -\nCopygains   26  -   485440\nCopylosses  11  15494   -\nHighly diverged 3961    18041314    18376872\nTandem repeats  2   518 599\nOk, so there are quite a few structural variants between Col-0 and Oy-0! The syri.out file contains the details of all the variants individually."
  },
  {
    "objectID": "posts/Ath_synteny.html#plotting-the-structural-variants",
    "href": "posts/Ath_synteny.html#plotting-the-structural-variants",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Plotting the structural variants",
    "text": "Plotting the structural variants\nWe can then go ahead and visualize the structural variants using the plotsr software. First we create the genomes.txt file containing info about where the software can find both genomes, how to call them, and specify how they should be plotted.\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat genomes.txt \n#file   name    tags\ndata/GCF_000001735.4_TAIR10.1_genomic.fna   Col lw:1.5\ndata/GCA_036927085.1_ASM3692708v1_genomic.fna   Oy  lw:1.5\n\n# then we run the software: all chromosomes\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ plotsr --sr Col_Oysyri.out --genomes genomes.txt -o all_chromosomes.png\n\n# or just chromosome 4\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ plotsr --sr Col_Oysyri.out --genomes genomes.txt --chr Chr4 -o chr4.png\n\nSo, while most of the Chr 4 of the two accessions are syntenic, we can find two inversions, a translocation, and some parts that don’t have a match at all at the chromosome of the other accession."
  },
  {
    "objectID": "posts/Ath_synteny.html#adding-tracks-with-other-data",
    "href": "posts/Ath_synteny.html#adding-tracks-with-other-data",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Adding tracks with other data",
    "text": "Adding tracks with other data\nYou can add tracks of data, such as gene annotations, or SNP density, across the genomes. Perhaps you can then discover interesting genomic features that colocalize with the structural variant breakpoints. Here, we have a dataset of short sequences (k-mers) that were mapped to the Col-0 reference genome using the mapping software bowtie. We need to transform the .bam file with the mappings to a .bed file. Then, we need to add “Chr” to the chromosome identifiers in the bed file.\nbedtools bamtobed -i mapping.sorted.bam &gt; mapping.sorted.bed\nawk '{print \"Chr\"$1 \"\\t\" $2 \"\\t\" $3 \"\\t\" $4}' mapping.sorted.bed &gt; mapping.sorted.chr.bed\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ head mapping.sorted.chr.bed \nChr1    753215  753246  sequence_1703\nChr1    753216  753247  sequence_761\nChr1    753217  753248  sequence_1521\nLet’s continue by generating a tracks.txt file, similar to the genomes.txt file to tell plotsr where to find the track data, how to call the track, and some graphical settings of the track.\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ cat tracks.txt \n#file   name    tags\nmapping.sorted.chr.bed    sig_kmers   bw:10000;nc:black;ns:8;lc:sienna;lw:1;bc:peachpuff;ba:0.7\n\n(syri) [mpaauw@omics-h0 02_Ath_synteny]$ plotsr --sr Col_Oysyri.out --genomes genomes.txt --tracks tracks.txt --chr Chr5 -W 6 -o Chr5_kmertrack.png\n\nCool! In this case, there is no clear association between our mapped k-mers and structural variation between Col-0 and Oy-0 on this chromosome."
  },
  {
    "objectID": "posts/Ath_synteny.html#further-reading",
    "href": "posts/Ath_synteny.html#further-reading",
    "title": "Visualizing Arabidopsis whole-genome alignments",
    "section": "Further reading",
    "text": "Further reading\n\nsyri, a software package to identify structural rearrangements.\nplotsr, a software package to plot the rearrangements detected by syri.\nLian et al 2024 Nature Genetics.\nclinker, a versatile synteny visualization tool that works on smaller scales (gene clusters)."
  },
  {
    "objectID": "posts/introduction.html",
    "href": "posts/introduction.html",
    "title": "The bioDSC blog: first post",
    "section": "",
    "text": "We want to use this blog to share, every now and then, what’s on our minds. For example, let’s say we read a few interesting papers on a new bioinformatics technique. Sometimes we install the tools, to see if we can get it to work on our own data, or on a small test dataset. It might not develop into a full tutorial or protocol, but instead of keeping it to ourselves, we will share it here on this blog. Who knows, maybe it’s useful for someone in the future!\nTopics we aim to cover:\n\nBioinformatics, from RNA-seq to comparative genomics and anything beyond that\nData visualization, focussing on the ggplot universe in R\nCrunchomics, tips and tricks to use the high-performance compute cluster of the University of Amsterdam\nData management and archiving\n\nDo not hesitate to send us an email if you have ideas for new topics, or if you want to contribute to the blog by co-authoring a post!"
  },
  {
    "objectID": "posts/introduction.html#why-a-blog",
    "href": "posts/introduction.html#why-a-blog",
    "title": "The bioDSC blog: first post",
    "section": "",
    "text": "We want to use this blog to share, every now and then, what’s on our minds. For example, let’s say we read a few interesting papers on a new bioinformatics technique. Sometimes we install the tools, to see if we can get it to work on our own data, or on a small test dataset. It might not develop into a full tutorial or protocol, but instead of keeping it to ourselves, we will share it here on this blog. Who knows, maybe it’s useful for someone in the future!\nTopics we aim to cover:\n\nBioinformatics, from RNA-seq to comparative genomics and anything beyond that\nData visualization, focussing on the ggplot universe in R\nCrunchomics, tips and tricks to use the high-performance compute cluster of the University of Amsterdam\nData management and archiving\n\nDo not hesitate to send us an email if you have ideas for new topics, or if you want to contribute to the blog by co-authoring a post!"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops & events",
    "section": "",
    "text": "Get introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nMay 19, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html#upcoming-workshops",
    "href": "workshops.html#upcoming-workshops",
    "title": "Workshops & events",
    "section": "",
    "text": "Get introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nMay 19, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html#stay-tuned",
    "href": "workshops.html#stay-tuned",
    "title": "Workshops & events",
    "section": "Stay tuned",
    "text": "Stay tuned\n\n\n\n\n\nList of past and upcoming workshops\n\n\n\n\n\nWorkshops we are currently developing that are coming soon.\n\n\n\n\n\nMar 1, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workshops.html#past-workshops",
    "href": "workshops.html#past-workshops",
    "title": "Workshops & events",
    "section": "Past workshops",
    "text": "Past workshops\n\n\n\n\n\nApril 2 and 4 (2025): Introduction to Python\n\n\n\n\n\nGet introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nApr 2, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\nMarch 12 and 14 (2025): Introduction to Python\n\n\n\n\n\nGet introduced to the Python from simple basics to functions, dataframes and plotting.\n\n\n\n\n\nMar 12, 2025\n\n\nMartijn Wehrens\n\n\n\n\n\n\n\n24 February 2025: Bring Your Own Data\n\n\n\n\n\nBring your own dataset and make publication-quality figures.\n\n\n\n\n\nFeb 24, 2025\n\n\nMisha Paauw\n\n\n\n\n\n\n\n15 and 17 January 2025: Introduction to R\n\n\n\n\n\nLearn the basics of data analysis and visualisation in R in this workshop.\n\n\n\n\n\nJan 15, 2025\n\n\nMisha Paauw\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "bioDSC core team",
    "section": "",
    "text": "Our core team consists of three members; Frans van der Kloet, Martijn Wehrens and Misha Paauw."
  },
  {
    "objectID": "people.html#frans-van-der-kloet",
    "href": "people.html#frans-van-der-kloet",
    "title": "bioDSC core team",
    "section": "Frans van der Kloet",
    "text": "Frans van der Kloet\n\n\nInfo\nI obtained my BSc at the Noordelijke Hogeschool (NHL) in Leeuwarden majoring in analytical chemistry and got my MSc in Computational Chemistry at the Radboud University in Nijmegen. After several jobs in commercial companies I continued my academic career in 2009 and obtained my PhD at the Leiden University on quantitative aspects in/of high resolution mass spectrometry data in metabolomics in 2014. In that same year I started as a post-doc at the BDA group working on aspects of multi-block/view solutions like JIVE, DISCO and OnPLS and incorporating these types of methods in the prediction/classification of in-vivo transcriptome data. After another PD position at the Amsterdam Medical Center I started as a data scientist in the BDA group in 2019.\nCurrent activities concern administration and implementation of a local Galaxy Server. Development of a database to store meta-data on samples/measurements (Metatree). Deployment of DL tools in the deconvolution of HRMS data and learning from DL models in general.\nMy main interest is in (pre)processing of large data-sets (with a preference for high resolution mass spectrometry data) and the development and implementation of data-analysis and pre-processing tools that is often further complicated because of these large data sizes.\n\n\nKeywords:\nPython, C++, C, Matlab, (R), Java, Galaxy(project), Metabolomics, MS, (multivariate) data analysis, (learning from) DL models.\n\n\nContact:\nGroup: Biosystems Data Analysis\nOffice: SP C2-205\nEmail: f.m.vanderkloet@uva.nl"
  },
  {
    "objectID": "people.html#martijn-wehrens",
    "href": "people.html#martijn-wehrens",
    "title": "bioDSC core team",
    "section": "Martijn Wehrens",
    "text": "Martijn Wehrens\n\nAfter studying biology and chemistry in Nijmegen (BSc, RU), I obtained a MSc degree in theoretical chemistry at the University of Amsterdam, during which I focussed on simulations of biomolecular networks. Remaining in Amsterdam, I then obtained my PhD at AMOLF, where I worked on tracking single bacteria using time lapse microscopy to understand gene regulation at the single cell. After finishing my PhD, I moved to the Hubrecht Institute in Utrecht where I used RNA-sequencing to investigate the pathogenesis of heart disease as a postdoc.\nAt the bioDSC, I have fun writing scripts and lending my various computational and quantitative expertise to further understand all the amazing biological processes that occur in cells.\n\nKeywords:\nImage analysis, quantitative biology, RNA-seq and sequencing pipelines, data analysis in Python and R.\n\n\nContact:\nGroup: Molecular Cytology\nOffice: SP C2.267a\nEmail: m.wehrens@uva.nl"
  },
  {
    "objectID": "people.html#misha-paauw",
    "href": "people.html#misha-paauw",
    "title": "bioDSC core team",
    "section": "Misha Paauw",
    "text": "Misha Paauw\n\n\nInfo\nAfter a broad training in biology during my BSc and MSc, ranging from ecology to bioinformatics to molecular plant biology, I started my PhD in the Molecular Plant Pathology group of the University of Amsterdam in 2019. In my PhD research, I worked on the molecular interactions between plants and a pathogenic bacterium, called Xanthomonas campestris pv. campestris (Xcc). By studying the ‘pangenome’ of Xcc, I reconstructed the evolutionary history of Xcc and pinpointed the genes required for the specific infection strategy of Xcc.\nIn my current role as data scientist I support molecular biologists in their data-heavy projects by providing individual consultancy and organizing workshops via the bioDSC.\n\n\nKeywords:\nPython, R, bash, data visualisation, comparative genomics, microbial genomics, molecular (plant) biology.\n\n\nContact:\nGroup: Plant Physiology, Green Life Sciences\nOffice: SP C2-207\nEmail: m.m.paauw@uva.nl"
  },
  {
    "objectID": "people.html#other-contributors",
    "href": "people.html#other-contributors",
    "title": "bioDSC core team",
    "section": "Other contributors",
    "text": "Other contributors\n\nAnna Heintz-Buschart (BDA, SILS)\nTijs Bliek (PDEG, SILS)\nNina Dombrowski (IBED)\nWim de Leeuw (MAD, SILS)"
  },
  {
    "objectID": "further-reading.html",
    "href": "further-reading.html",
    "title": "*bio*DSC",
    "section": "",
    "text": "Here, we will provide links to other useful pages on the intersection between biology and data science. Send us an email if you maintain a page or blog with useful biology-related programming tips and tricks, and wish to be included in these lists.\n\n\n\nCrunchomics documentation\nThe IBED bioinformatics support page\n\n\n\n\n\nDataViz protocols by Joachim Goedhart\nFundamentals of Data Visualization by Claus Wilke\nAn introduction to good color schemes\n\n\n\n\n\nMeta-Omics tutorials by Anna Heintz-Buschart"
  },
  {
    "objectID": "further-reading.html#further-reading",
    "href": "further-reading.html#further-reading",
    "title": "*bio*DSC",
    "section": "",
    "text": "Here, we will provide links to other useful pages on the intersection between biology and data science. Send us an email if you maintain a page or blog with useful biology-related programming tips and tricks, and wish to be included in these lists.\n\n\n\nCrunchomics documentation\nThe IBED bioinformatics support page\n\n\n\n\n\nDataViz protocols by Joachim Goedhart\nFundamentals of Data Visualization by Claus Wilke\nAn introduction to good color schemes\n\n\n\n\n\nMeta-Omics tutorials by Anna Heintz-Buschart"
  },
  {
    "objectID": "workshop-materials/py-intro/02-variables.html",
    "href": "workshop-materials/py-intro/02-variables.html",
    "title": "Exercises Lesson 2, variables",
    "section": "",
    "text": "disclaimer\n\n\n\nMost questions are copied from the carpentry lesson “Plotting and Programming in Python”\n\n\n\n\nExercises Lesson 2, variables\n\n1. Order of things\n\n\n\n\n\n\nQuestion A\n\n\n\nFill the table showing the values of the variables in this program after each statement is executed.\n# Command  # Value of x   # Value of y   # Value of swap #\nx = 1.0    #              #              #               #\ny = 3.0    #              #              #               #\nswap = x   #              #              #               #\nx = y      #              #              #               #\ny = swap   #              #              #               #\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n1\nNone\nNone\n\n\ny = 3.0\n1\n3\nNone\n\n\nswap = x\n1\n3\n1\n\n\nx = y\n3\n3\n1\n\n\ny = swap\n3\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nWhat is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.)\ninitial = 'left'\nposition = initial\ninitial = 'right'\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n‘left’\n\n\n\n\n\n\n\n\n\nChallenge question\n\n\n\nIf you assign a = 123, what happens if you try to get the second digit of a via a[1]?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNumbers are not strings or sequences and Python will raise an error if you try to perform an index operation on a number. In the next lesson on types and type conversion we will learn more about types and how to convert between different types. If you want the Nth digit of a number you can convert it into a string using the str built-in function and then perform an index operation on that string.\na = 123\nprint(a[1])\nTypeError: ‘int’ object is not subscriptable\n\n\n\n\n\n2. Slicing\n\n\n\n\n\n\n\nQuestion C\n\n\n\nGiven the following string:\nspecies_name = \"Acacia buxifolia\"\nWhat would these expressions return?\n1. species_name[2:8]\n2. species_name[11:] (without a value after the colon)\n3. species_name[:4] (without a value before the colon)\n4. species_name[:] (just a colon)\n5. species_name[11:-3]\n6. species_name[-5:-3]\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nspecies_name[2:8] returns the substring 'acia b'\nspecies_name[11:] returns the substring 'folia', from position 11 until the end\nspecies_name[:4] returns the substring 'Acac', from the start up to but not including position 4\nspecies_name[:] returns the entire string 'Acacia buxifolia'\nspecies_name[11:-3] returns the substring 'fo', from the 11th position to the third last position\nspecies_name[-5:-3] also returns the substring 'fo', from the fifth last position to the third last\n\n\n\n\n\n\n\n\n\n\nQuestion D\n\n\n\nWhat happens when you choose a stop value which is out of range? (i.e., try species_name[0:20] or species_name[:103])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf a part of the slice is out of range, the operation does not fail. species_name[0:20] gives the same result as species_name[0:], and species_name[:103] gives the same result as species_name[:]\n\n\n\n\n\n\n\n\n\n\nAdditional exercises\n\nNaming (Carpentries)\n\n\n\n\n\n\nQuestion E\n\n\n\nWhich is a better variable name, m, min, or minutes? Why? Hint: think about which code you would rather inherit from someone who is leaving the lab:\nts = m * 60 + s\ntot_sec = min * 60 + sec\ntotal_seconds = minutes * 60 + seconds\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nminutes is better because min might mean something like “minimum” (and actually is an existing built-in function in Python that we will cover later).\n\n\n\n\n\nA very challenging set of additional exercises (bioDSC)\n\n\n\n\n\n\nQuestion F\n\n\n\nWhat’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'\n\n\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "workshop-materials/py-intro/02-variables.html#command-value-of-x-value-of-y-value-of-swap",
    "href": "workshop-materials/py-intro/02-variables.html#command-value-of-x-value-of-y-value-of-swap",
    "title": "Exercises Lesson 2, variables",
    "section": "| Command | Value of x | Value of y | Value of swap |",
    "text": "| Command | Value of x | Value of y | Value of swap |\nx=1.0 | 1 | None | None |\ny = 3 | 1 | 3 | None |\nswap = x | 1 | 3 | 1 |\nx= y | 3|3|1|\ny=swap | 3 | 1 | 1|\n\nB\n\n\n\n\n\n\nCaution\n\n\n\nWhat is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.)\ninitial = 'left'\nposition = initial\ninitial = 'right'\n\n\n\n\n\n\nBWA alignment options\n\n\n\nBWA consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the other two are for sequences ranging from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nVisualize the alignment of the reads for our SRR2584866 sample. What variant is present at position 4377265? What is the canonical nucleotide in that position?\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nBioinformatic command line tools are collections of commands that can be used to carry out bioinformatic analyses.\nTo use most powerful bioinformatic tools, you will need to use the command line.\nThere are many different file formats for storing genomics data. It is important to understand what type of information is contained in each file, and how it was derived.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways read the manual page for any tool before using and make sure the options you use are appropriate for your data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n$ grep -v \"#\" results/vcf/SRR2584866_final_variants.vcf | wc -l\n765\nThere are 765 variants in this file.\n\n\n\n\n“Challenge”\nIf you assign a = 123, what happens if you try to get the second digit of a via a[1]?\n\n\nSlicing\n\nGiven the following string:\nspecies_name = \"Acacia buxifolia\"\nwhat would these expressions return?\nspecies_name[2:8]\nspecies_name[11:] (without a value after the colon)\nspecies_name[:4] (without a value before the colon)\nspecies_name[:] (just a colon)\nspecies_name[11:-3]\nspecies_name[-5:-3]\nWhat happens when you choose a stop value which is out of range? (i.e., try species_name[0:20] or species_name[:103])\n\n\n\n\n\n\nLesson 2, additional exercises\n\nNaming (Carpentries)\nWhich is a better variable name, m, min, or minutes? Why? Hint: think about which code you would rather inherit from someone who is leaving the lab:\nts = m * 60 + s\ntot_sec = min * 60 + sec\ntotal_seconds = minutes * 60 + seconds\n\n\nA very challenging set of additional exercises (bioDSC)\nWhat’s happening here:\n\n# use Google or chatGPT if you don't know the answers\n# if you're at the end, try to play around more\n\ngreetings_strings = ['hello', 'bye', 'later']\nprint(greetings_strings[0])\nprint(greetings_strings[1][2])\n    # (list of lists)\n\nprint([greetings_strings[idx] for idx in [0, 2]])\nprint([greetings_strings[0][idx] for idx in [0, 2, 4]])\nprint([greetings_strings[0][idx] for idx in range(0,4,2)])\n    # using a loop (will be covered later) / list comprehension\n\nsquare_values = [number**2 for number in range(10)]\nsquare_values_string = [str(number**2) for number in range(10)]\nprint(square_values)\nprint(square_values_string)\n    # types will be the topic of the next lesson\n\nspecies_name = \"Acacia buxifolia\"\nprint(\"\".join([species_name[i] for i in range(10, 2, -1)]))\n    # convenient command when working with strings\n\nprint(species_name.replace('Aca', 'Bole'))\n    # another convenient command\n\n# Exercise:\nlist_of_species = ['','','','']\nlist_of_species = ['Homo sapiens', 'Escherichia Coli', 'Pan troglodytes', 'Canis lupus', 'Felis catus']\n# Create a new lists, where you remove all letters 'e'\n\n\n\n\n\n\n\nExercises Lesson 3, types\n\nTypes\n\nA\nWhat type of value is 3.4? How can you find out?\n\n\nB\nWhat type of value is 3.25 + 4?\n\n\nC\nWhat type of value (integer, floating point number, or character string) would you use to represent each of the following? Try to come up with more than one good answer for each problem. For example, in # 1, when would counting days with a floating point variable make more sense than using an integer?\n\nNumber of days since the start of the year.\nTime elapsed from the start of the year until now in days.\nSerial number of a piece of lab equipment.\nA lab specimen’s age\nCurrent population of a city.\nAverage population of a city over time.\n\n\n\nD (added bioDSC)\nWhy wouldn’t you always use floats, and never use integers?\n\n\n\nStrings to numbers\nWhere reasonable, float() will convert a string to a floating point number, and int() will convert a floating point number to an integer:\nprint(\"string to float:\", float(\"3.4\"))\nprint(\"float to int:\", int(3.4))\nOUTPUT:\nstring to float: 3.4\nfloat to int: 3\nIf the conversion doesn’t make sense, however, an error message will occur.\nprint(\"string to float:\", float(\"Hello world!\"))\nGiven this information, what do you expect the following program to do?\nWhat does it actually do?\nWhy do you think it does that?\nprint(\"fractional string to int:\", int(\"3.4\"))\nWhich of the following will return the floating point number 2.0? Note: there may be more than one right answer.\nfirst = 1.0\nsecond = \"1\"\nthird = \"1.1\"\n1. first + float(second)\n2. float(second) + float(third)\n3. first + int(third)\n4. first + int(float(third))\n5. int(first) + int(float(third))\n6. 2.0 * second\n\n\n\n\n\n\n\nLesson 3, additional exercises\n\nLists\nWe’ll cover lists later in lesson 11, but let’s already take a brief look.\nA list is a series of elements bound together, where each element can have a value. They are defined as follows:\nnumbers = [1,2,3]\nfruits = ['apples', 'pears', 'oranges']\nphysical_constants = ['pi', 3.14, 'c', 299_792_458, 'mole', 6.022e23]\nElements can be accessed the same way as we saw with strings.\n\nA\nWhat will numbers[1] return? And physical_constants[2:4]?\n\n\nB\nWhat is the type of\n\nnumbers?\nnumbers[1]?\nphysical_constants?\nphysical_constants[1]?\nphysical_constants[2]?\nphysical_constants[3]?\nfruits?\nfruits[1]?\n\n\n\nC\nCan the elements in a list have different types? (This can be seen from the previous answer.)\n\n\n\nnp.array\nLists can be a useful tool, but for example in image analysis, don’t offer the full mathematical options one might like. numpy arrays introduce a new type of series, in which you can do more manipulations. See some examples below:\nimport numpy as np\nmy_array = [1,2,3,5]\nmy_array_np = np.array([1,2,3,5])\n\n# what's the difference here:\n# my_array+1 # commented because gives error, why?\nmy_array_np+1\n\n# what's the difference here:\nmy_array * 3\nmy_array_np * 3\n\n# more:\nmy_array + [1,2,3,4]\nmy_array_np + np.array([1,2,3,4])\nnp.sin(my_array_np)\n\n# numpy-specific things, what is happening?\nmy_array_np[range(1,4,2)]\nmy_array_np[my_array_np&gt;1]\n\n# Can numpy arrays have different types?\n# What is the type of the elements in these two arrays?\nnp.array([1,2,3,'4'])\nnp.array([1,2,3,'hello'])\n\n\nDict\nPython has more types. A dict is sometimes very convenient, and is also used later with creating tables.\nexperimental_replicate_list = {'WT': 12, 'mut': 32, 'WT.cond1': 10, 'mut.cond1': 12}\nprint(experimental_replicte_list)\n\nprint(experimental_replicate_list['WT'])\n\n# Exercise:\n# Edit the following code such that we get replicate numbers for conditions involving WT\nmy_keys = experimental_replicate_list.keys()\nprint(my_keys)\nmy_keys_of_interest = [the_key for the_key in my_keys if 'mut' in the_key] # edit this line\nprint(my_keys_of_interest)\nprint([experimental_replicate_list[sel_key] for sel_key in my_keys_of_interest])\n\n# The above code uses several lines, can you do this in one line?\nprint(...) # edit this line\n\n\nSpecial maths (Carpentries)\nIn Python 3, the // operator performs integer (whole-number) floor division, the / operator performs floating-point division, and the % (or modulo) operator calculates and returns the remainder from integer division:\nprint('5 // 3:', 5 // 3)\nprint('5 / 3:', 5 / 3)\nprint('5 % 3:', 5 % 3)\nOUTPUT:\n5 // 3: 1\n5 / 3: 1.6666666666666667\n5 % 3: 2\nIf num_subjects is the number of subjects taking part in a study, and num_per_survey is the number that can take part in a single survey, write an expression that calculates the number of surveys needed to reach everyone once.\n\n\nComplex numbers (Carpentries)\nPython provides complex numbers, which are written as 1.0+2.0j. If val is a complex number, its real and imaginary parts can be accessed using dot notation as val.real and val.imag.\na_complex_number = 6 + 2j\nprint(a_complex_number.real)\nprint(a_complex_number.imag)\n\nQuestions:\n\nWhy do you think Python uses j instead of i for the imaginary part?\nWhat do you expect 1 + 2j + 3 to produce?\nWhat do you expect 4j to be? What about 4 j or 4 + j?\n\n\n\n\n\n\n\n\n\nExercises Lesson 4, functions (1)\n\nOrder\n\nExplain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc.\nWhat is the final value of radiance?\n\nradiance = 1.0\nradiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))\n\n\nLast string character\nIf Python starts counting from zero, and len returns the number of characters in a string, what index expression will get the last character in the string name? (Note: we will see a simpler way to do this in a later episode.)\n\n\n\n\n\n\n\nAdditional exercises Lesson 4\n\nWhy not?\nWhy is it that max and min do not return None when they are called with no arguments?\n\n\nSpot the difference\nPredict what each of the print statements in the program below will print. Does max(len(rich), poor) run or produce an error message? If it runs, does its result make any sense?\neasy_string = \"abc\"\nprint(max(easy_string))\nrich = \"gold\"\npoor = \"tin\"\nprint(max(rich, poor))\nprint(max(len(rich), len(poor)))\n\n\n\n(Lesson 5 is a break.)\n\n\n\n\n\n\n\n\nExercises Lesson 6, libraries\n\nExplore\n\nWhat function from the math module can you use to calculate a square root without using sqrt?\nSince the library contains this function, why does sqrt exist?\n\n\n\nFind the right module\nYou want to select a random character from a string:\nbases = 'ACTTGCTTGAC'\n\nWhich standard library module could help you?\nWhich function would you select from that module? Are there alternatives?\nTry to write a program that uses the function.\n\n\n\nHelp!\nWhen a colleague of yours types help(math), Python reports an error:\nNameError: name 'math' is not defined\nWhat has your colleague forgotten to do?\n\n\nImporting with aliases\n\nFill in the blanks so that the program below prints 90.0.\nRewrite the program so that it uses import without as.\nWhich form do you find easier to read?\n\nimport math as m\nangle = ____.degrees(____.pi / 2)\nprint(____)\n\n\nMultiple ways of importing\nMatch the following print statements with the appropriate library calls.\nPrint commands:\n\nprint(\"sin(pi/2) =\", sin(pi/2))\nprint(\"sin(pi/2) =\", m.sin(m.pi/2))\nprint(\"sin(pi/2) =\", math.sin(math.pi/2))\n\nLibrary calls:\n\nfrom math import sin, pi\nimport math\nimport math as m\nfrom math import *\n\n\n\n\n\n\n\n\nAdditional Exercises Lesson 6, Libraries\n\n“Jigsaw”: progamming example\nRearrange the following statements so that a random DNA base is printed and its index in the string. Not all statements may be needed. Feel free to use/add intermediate variables.\nbases=\"ACTTGCTTGAC\"\nimport math\nimport random\n___ = random.randrange(n_bases)\n___ = len(bases)\nprint(\"random base \", bases[___], \"base index\", ___)\n\n\nImporting specific items\n\nFill in the blanks so that the program below prints 90.0.\nDo you find this version easier to read than preceding ones?\nWhy wouldn’t programmers always use this form of import?\n\n____ math import ____, ____\nangle = degrees(pi / 2)\nprint(angle)\n\n\nReading error messages\nRead the code below and try to identify what the errors are without running it. Run the code, and read the error message. What type of error is it?\nfrom math import log\nlog(0)\n\n\nWrite your own function and import it (bioDSC)\nWhen you have a notebook file, you can also create another file, with a .py extension, and write functions in that file. The .py file can be imported like a library, and the functions in the file can be used as if they came from a library.\n\nExercise\nUsing the information below, try to - create two files, one .ipynb (notebook) file and one .py (python plain text code) file. - rename the myfunctionname functions in the .py file and use them in the notebook. - create a third function, which returns C when you provide A and B, assuming A^2+B^2 = C^2, and use it in your notebook.\nUseful things to know: - You can also make .py files. Unlike notebooks, every text here is assumed to be python code. - For Jypiter notebooks: - You can make a .py file with file &gt; new &gt; python file.\n- Save the file to myfilename.py (replacing myfilename with your own favorite name). - For Google colabs: - To create a .py file, right click in the file overview (where you also put the gapminder .csv files), and select ‘new file’. Then create a file ‘myfilename.py’ and double click to edit it. - You can import your file in a python notebook using: - import myfilename where myfilename.py should exist and hold your code. You can also put your file in a directory, but then you need to import it like import mydirectoryname.myfilename.\nYou can write a function using the following template:\n\ndef myfunctionname():\n    print(\"hello world\")\n    \ndef myfunctionname2(input1, input2):\n    print(\"input 1 = \", input1, ', input 2 = ', input2)\n\n\n\n\n\n\n\n\nExercises lesson 7, dataframes (1)\n\nReading other data\nRead the data in gapminder_gdp_americas.csv (which should be in the same directory as gapminder_gdp_oceania.csv) into a variable called data_americas and display its summary statistics.\n\n\nInspecting data\nAfter reading the data for the Americas, use help(data_americas.head) and help(data_americas.tail) to find out what DataFrame.head and DataFrame.tail do.\n\nWhat method call will display the first three rows of this data?\nWhat method call will display the last three columns of this data? (Hint: you may need to change your view of the data.)\n\n\n\nNavigating directories\nThe data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis:\nyour_home_directory\n+-- field_data/\n|   +-- microbes.csv\n+-- thesis/\n    +-- analysis.ipynb\nWhat value(s) should you pass to read_csv to read microbes.csv in analysis.ipynb?\n\n\nWriting data\nAs well as the read_csv function for reading data from a file, Pandas provides a to_csv function to write dataframes to files. Applying what you’ve learned about reading from files, write one of your dataframes to a file called processed.csv. You can use help to get information on how to use to_csv.\n\n\n\n\n\n\n\nAdditional exercises for Lesson 7, dataframes (1)\n\nCreate a plain text file on your computer, and give it the extension .csv.\nFind out what the comma-separated format looks like.\nUse your imagination to complete the following table and put it in the .csv file.\n\n\n\n\nreplicate\ncond1\ncond2\n\n\n\n\n\n10\n\n\n\n\n11\n\n\n\n\n10\n\n\n\n\n12\n\n\n\n\n13\n\n\n\n\n13\n\n\n\n\n\nNow try to read in that table in your python notebook.\nGet the following code to run on your dataframe (referred to as df below):\n\nfrom scipy.stats import ttest_ind\nt_stat, p_value = ttest_ind(df['cond1'], df['cond2'])\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nWhat does that code do?\nLet’s do some unethical data massaging, and tweak the csv such that you get a significant p-val.\n\n\n\n\n\n\n\nExercises for lesson 8, dataframes (2)\n\nSelection individual values\nImport data for europe:\nimport pandas as pd\ndata_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\nFind the GDP Capita of Serbia in 2007.\n\n\nExtent of slicing\n\nDo the two statements below produce the same output?\nBased on this, what rule governs what is included (or not) in numerical slices and named slices in Pandas?\n\nprint(data_europe.iloc[0:2, 0:2])\nprint(data_europe.loc['Albania':'Belgium', 'gdpPercap_1952':'gdpPercap_1962'])\n\n\nReconstructing Data\nExplain what each line in the following short program does: what is in first, second, etc.?\nfirst = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsecond = first[first['continent'] == 'Americas']\nthird = second.drop('Puerto Rico')\nfourth = third.drop('continent', axis = 1)\nfourth.to_csv('result.csv')\n\n\nSelecting Indices\nExplain in simple terms what idxmin and idxmax do in the short program below. When would you use these methods?\ndata = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\nprint(data.idxmin())\nprint(data.idxmax())\n\n\nPractice with Selection\nAssume Pandas has been imported and the Gapminder GDP data for Europe has been loaded. Write an expression to select each of the following:\n\nGDP per capita for all countries in 1982.\nGDP per capita for Denmark for all years.\nGDP per capita for all countries for years after 1985.\nGDP per capita for each country in 2007 as a multiple of GDP per capita for that country in 1952.\n\n\n\nMany Ways of Access\nThere are at least two ways of accessing a value or slice of a DataFrame: by name or index. However, there are many others. For example, a single column or row can be accessed either as a DataFrame or a Series object.\nSuggest different ways of doing the following operations on a DataFrame:\n\nAccess a single column\nAccess a single row\nAccess an individual DataFrame element\nAccess several columns\nAccess several rows\nAccess a subset of specific rows and columns\nAccess a subset of row and column ranges\n\n\n\n\n\n\n\n\nAdditional exercises Lesson 8, dataframes (2)\n\nEpicardial cells\n\nA\nIn the RNA-seq data, we can create another column that reflects the condition of the cells, WT or mutant. Fill in the blanks to achieve this:\ndf_cells_kohela2['Condition'] = ____\n\ndf_cells_kohela2.loc[df_cells_kohela2.index.str.contains('WT_'), 'Condition'] = ____\ndf_cells_kohela2.loc[df_cells_kohela2.index._______, _________] = ______\n\n\n\nB\nWhat is the difference between str.contains and str.match?\n\n\nC\nNow again calculate the mean value of TFAP2A expression in WT cells vs. mutant cells. Does there appear to be more TFAP2A expression in the mutant cells?\n\n\n\n\nGene expression (bioDSC)\n\nConvert the data below to a file you can import (e.g.: csv, tsv), import it to a pandas df, and determine the following:\n\nThe average CRP gene expression per condion.\nThe corresponding standard deviations.\nThe log2-fold change between WT, condition A, and condition B.\nDo the same for ACTA1.\nNormalize all gene expression levels to their average respective wild type levels.\n\n\ngene    expression  condition\nCRP 873 WT\nCRP 324 WT\nCRP 214 WT\nCRP 151 WT\nCRP 1220    A\nCRP 450 A\nCRP 300 A\nCRP 210 A\nCRP 800 B\nCRP 200 B\nCRP 200 B\nCRP 130 B\nACTA1   7457    WT\nACTA1   2342    WT\nACTA1   8000    WT\nACTA1   9000    WT\nACTA1   6500    A\nACTA1   2200    A\nACTA1   7500    A\nACTA1   8000    A\nACTA1   1000    B\nACTA1   1123    B\nACTA1   3211    B\nACTA1   1231    B\n\n\nGDPs (bioDSC)\n\nBetween ’87 and ’92 the GDP of most countries took a hit. Are there any countries which had a positive increase between those two years? Which ones?\nCalculate the average GDP between all European countries per year.\n\nNormalize the dataframe by this trend.\n\n\n\n\nExploring available methods using the dir() function\nPython includes a dir() function that can be used to display all of the available methods (functions) that are built into a data object. In Episode 4, we used some methods with a string. But we can see many more are available by using dir():\nmy_string = 'Hello world!'   # creation of a string object \ndir(my_string)\nThis command returns:\n['__add__',\n...\n'__subclasshook__',\n'capitalize',\n'casefold',\n'center',\n...\n'upper',\n'zfill']\nYou can use help() or Shift+Tab to get more information about what these methods do.\nAssume Pandas has been imported and the Gapminder GDP data for Europe has been loaded as data. Then, use dir() to find the function that prints out the median per-capita GDP across all European countries for each year that information is available.\n\n\nInterpretation\nPoland’s borders have been stable since 1945, but changed several times in the years before then. How would you handle this if you were creating a table of GDP per capita for Poland for the entire twentieth century?\n\n\n\n\n\n\n\nExercises Lesson 9, plotting\nModified by bioDSC\n\nMinima and Maxima (Carpentries, modified)\nFill in the blanks below to plot the minimum GDP per capita over time for all the countries in Europe.\nModify it again to plot the maximum GDP per capita over time for Europe, you need to edit the code beyond the ___ for this.\ndata_europe = pd.read_csv('/Users/m.wehrens/Desktop/python_course/data/gapminder_gdp_europe.csv', index_col='country')\ndata_europe_transposed = data_europe.T\n\ndata_europe_transposed['min'] = data_europe.____\ndata_europe_transposed['max'] = ____\ndata_europe_transposed['year'] = ____\n\nsns.lineplot(data_europe_transposed, x='year', y='min')\nsns.lineplot(data_europe_transposed, x='year', y='max')\nplt.legend(loc='best')\nplt.xticks(rotation=90)\nbioDSC HINT: if you don’t see the solution, take it step by step. Break down the task in subtasks, and adress the first step towards the solution first. Try that first. Running code is free.\n\n\nMean gene expression (bioDSC)\nUse the kohela data again:\n# Load data, note the \".T\" at the end here\ndf_kohela = pd.read_csv('data/kohela-et-al.csv', index_col=0).T\n# create new 'masks'\nepicardial_cells = df_kohela['WT1']&gt;3\nfibroblast_cells = df_kohela['COL2A1']&gt;30\nfat_cells = df_kohela['PPARG']&gt;2\n# Add cell type\ndf_kohela['Celltype'] = 'unknown'\ndf_kohela.loc[epicardial_cells,'Celltype'] = 'epicardial'\ndf_kohela.loc[fibroblast_cells, 'Celltype'] = 'fibroblast'\ndf_kohela.loc[fat_cells, 'Celltype'] = 'fat'\n# Add conditions\ndf_kohela['Condition'] = 'unknown'\ndf_kohela.loc[df_kohela.index.str.contains('WT_'), 'Condition'] = 'WT'\ndf_kohela.loc[df_kohela.index.str.contains('mutant_'), 'Condition'] = 'mutant'\n\n\nA\nUse seaborn to create a ‘stripplot’ plot for WT1 expression per cell type. Then create a similar plot for TBX18. (Epicardial cell markers.) What information can be extracted from this plot?\n\n\n\nB\nNow create a scatter plot, showing WT1 expression vs. TBX18 expression across all cells. What does this tell us?\n\n\n\nC\nColor the scatter plot per cell type. What does this tell us?\n\n\nD\nCalculate the total RNA-seq reads per cell, and make a violin plot per condition for these total reads.\nSome code to start with:\ndf_kohela['Total_reads'] = df_kohela.loc[:,'A1BG':'ZZZ3']._.____\nWhat does the violin plot tell us?\n\n\nMore Correlations (Carpentries, modified)\nThis short program creates a plot showing the correlation between GDP and life expectancy for 2007, normalizing marker size by population:\ndata_all = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsns.scatterplot(data_all, x='gdpPercap_2007', y='lifeExp_2007', s=data_all['pop_2007']/1e6)\nUsing online help and other resources, explain what each argument to plot does.\n\n\nEven more correlations (bioDSC)\n\nA\nUse the code from the “More Correlations” exercise, and try to add the following lines to the plotting code:\nplt.text(data_all.loc['United States','gdpPercap_2007'], data_all.loc['United States','lifeExp_2007'], 'United States')\nplt.text(data_all.loc['Netherlands','gdpPercap_2007'], data_all.loc['Netherlands','lifeExp_2007'], 'Netherlands')\n\nWhat’s happening here? (You might need to use Google.)\nAdd your favorite country too.\n\n\n\nB\nHow would you add labels for the top 10 GDP countries?\nAnswer: this would be very tedious with what you learned currently! In the next lessons (particularly lesson 12), we’ll learn how to automate your code. This will be very useful for this particular challenge.\n\n\n\n\n\n\n\n\nAdditional Exercises Lesson 9, plotting\nModified by bioDSC\n\nSubselection and melting\n\nA\nUsing the kohela data, first create a new dataframe df_kohela_sel with only a selection of a few genes, and the condition and cell type columns.\nSelect the following genes: ['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA']. These are epicardial markers (WT1, TBX18), a transcription factor (TFAP2A), fibroblast markers (COL2A1, ACTA2), and fat markers (PPARG, CEBPA).\n\n\n\nB\nNow melt this dataframe using pd.melt(), and use cell type and condition as identifier variables. What will happen to the gene expression values? What is sensible input for the var_name and value_name parameters? Why is this useful? (For answer, see next questions.)\n\n\n\nC\nUse df_kohela_melted.head() to check whether the output is as expected.\n\n\nD\nNow make a violin or stripplot, with as x-axis the genes, expression on the y-axis, and colored for condition. Is there an issue with this plot?\n\n\n\nE\nLook at the following example:\nimport numpy as np\n\n# A custom function, which normalizes a series by its mean\n# We'll learn more about functions in Lesson 16\ndef gene_normalization(X):\n    return X / np.mean(X)\n\n# Create a subset of the data\ncell_subset = ['mutant_rep1_cell174', 'WT_rep2_cell348', 'mutant_rep1_cell160',\n       'WT_rep1_cell022', 'mutant_rep1_cell069']\ngene_subset = ['WT1', 'TBX18', 'TFAP2A', 'COL2A1', 'ACTA2', 'PPARG', 'CEBPA']\n\n# Normalize gene expression\ndf_kohela_subset2 = df_kohela.loc[cell_subset, gene_subset]\ndf_kohela_subset2_normalized = df_kohela_subset2.transform(gene_normalization)\n\n# Print the result\nprint(df_kohela_subset2_normalized)\n\nCheck out what gene_normalization(df_kohela_subset2['WT1']) does.\nWhat does the transform method do in the above code?\n\n\n\nF\nEdit the following code (replacing blanks by code) to normalize the gene expression by the total expression of each gene. Hint: look at exercise E.\ndf_kohela_grouped = df_kohela_melted.groupby(_______)\ndf_kohela_melted['Expression_normalized'] = df_kohela_grouped['Expression']._______(gene_normalization)\nThen, similar to D, plot the normalized gene expression using both the sns.barplot and sns.stripplot. For the stripplot, use the additional parameter dodge=True.\nThe barplot looks nice, but does it contain all information?\n\n\n\nG\nFinally, you might want to change the order in these plots. Use the following line:\ndf_kohela_melted['Condition'] = pd.Categorical(df_kohela_melted['Condition'], categories=['WT', 'mutant'], ordered=True)\nAnd then run your plotting code again. What has happened?\n\n\n\nSaving your plot\nYou might want to save your plot. You can use the plt.savefig function for this.\nCheck out this code with some additional convenient options. Change '/your/location/your-filename.pdf' to a convenient path where you save your figure.\nimport matplotlib.pyplot as plt\n\n# Bang Wong colorblind-friendly color scheme (https://www.nature.com/articles/nmeth.1618)\ncolors_bangwong = [\n    \"#E69F00\",  # Orange\n    \"#56B4E9\",  # Sky Blue\n    \"#009E73\",  # Bluish Green\n    \"#F0E442\",  # Yellow\n    \"#0072B2\",  # Blue\n    \"#D55E00\",  # Vermillion\n    \"#CC79A7\",  # Reddish Purple\n    \"#000000\"   # Black\n]\n\nplt.style.use('default')\nfig, ax = plt.subplots(1,1, figsize=(10/2.54,10/2.54))\nax.plot([1,2,3,4], [1,4,9,16], linestyle='--', color=colors_bangwong[1], label=r'$x^2$')\nax.plot([1,2,3,4], [1,5,11,19], linestyle=':', color=colors_bangwong[2], label=r'$x^2+(x-1)$')\nax.legend()\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_title('Sample Plot', fontsize=12)\nax.legend(fontsize=12)\nax.tick_params(axis='both', which='major', labelsize=12)\nplt.tight_layout()\n\nplt.savefig('/your/location/your-filename.pdf', dpi=300, bbox_inches='tight')\nplt.close(fig)\n\n# Use this command to show the figure when not using (Jupyter) notebooks.\n# plt.show()\n\n\nCorrelations (Carpentries, modified)\nModify the code from “Minima and Maxima” exercise to create a scatter plot showing the relationship between the minimum and maximum GDP per capita across the countries in Asia, with each point in the plot corresponding to a year. What relationship do you see (if any)?\n\n\nCorrelations (continued) (Carpentries, modified)\nYou might note that the variability in the maximum is much higher than that of the minimum. Take a look at the maximum and the max indexes:\ndata_asia = pd.read_csv('data/gapminder_gdp_asia.csv', index_col='country')\n\ndf_max_GDP = pd.DataFrame()\ndf_max_GDP['GDP_max'] = data_asia.max()\ndf_max_GDP['Year']    = data_asia.columns.str.replace('gdpPercap_','').astype(int)\n\nplt.plot(df_max_GDP['Year'], df_max_GDP['GDP_max'])\n\nprint(data_asia.idxmax())\nprint(data_asia.idxmin())\n\n\nNormalized dataframe (bioDSC)\n\nIn the previous lesson about dataframes (in the additional exercises), we normalized the GDP data against the average trend. Plot the data from this normalized dataframe.\n\nIs this helpful in any way?\n\n\n\n\nCrude oil (bioDSC)\nCrude oil prices can be found at: https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=PET&s=F000000__3&f=A\nThis gives the data below:\nDecade  Year-0  Year-1  Year-2  Year-3  Year-4  Year-5  Year-6  Year-7  Year-8  Year-9\n  1850's                                        16.00\n  1860's    9.59    0.49    1.05    3.15    8.06    6.59    3.74    2.41    3.62    5.64\n  1870's    3.86    4.34    3.64    1.83    1.17    1.35    2.52    2.38    1.17    0.86\n  1880's    0.94    0.92    0.78    1.10    0.85    0.88    0.71    0.67    0.65    0.77\n  1890's    0.77    0.56    0.51    0.60    0.72    1.09    0.96    0.68    0.80    1.13\n  1900's    1.19    0.96    0.80    0.94    0.86    0.62    0.73    0.72    0.72    0.70\n  1910's    0.61    0.61    0.74    0.95    0.81    0.64    1.10    1.56    1.98    2.01\n  1920's    3.07    1.73    1.61    1.34    1.43    1.68    1.88    1.30    1.17    1.27\n  1930's    1.19    0.65    0.87    0.67    1.00    0.97    1.09    1.18    1.13    1.02\n  1940's    1.02    1.14    1.19    1.20    1.21    1.22    1.41    1.93    2.60    2.54\n  1950's    2.51    2.53    2.53    2.68    2.78    2.77    2.79    3.09    3.01    2.90\n  1960's    2.88    2.89    2.90    2.89    2.88    2.86    2.88    2.92    2.94    3.09\n  1970's    3.18    3.39    3.39    3.89    6.87    7.67    8.19    8.57    9.00    12.64\n  1980's    21.59   31.77   28.52   26.19   25.88   24.09   12.51   15.40   12.58   15.86\n  1990's    20.03   16.54   15.99   14.25   13.19   14.62   18.46   17.23   10.87   15.56\n  2000's    26.72   21.84   22.51   27.56   36.77   50.28   59.69   66.52   94.04   56.35\n  2010's    74.71   95.73   94.52   95.99   87.39   44.39   38.29   48.05   61.40   55.59\n  2020's    36.86   65.84   93.97   76.10                       \nSave that data to a .tsv file, and upload it.\nNow try to understand the code below:\nimport pandas as pd\n\n# Load the data\ndf_crudeoil = \\\n    pd.read_csv('/Users/m.wehrens/Data_UVA/2024_teaching/2025-03-gapminder/crude_oil/crude_oil_prices.tsv', sep='\\t')\n\n# reshape the data, such that it becomes a long list\ndf_crudeoil_melted = df_crudeoil.melt(id_vars='Decade', var_name='lastdigit')\n\n# now reformat the year information\n# search and replace first\ndf_crudeoil_melted.loc[:,'Decade'] = df_crudeoil_melted.loc[:,'Decade'].str.replace(\"0's\",'')\ndf_crudeoil_melted.loc[:,'lastdigit'] = df_crudeoil_melted.loc[:,'lastdigit'].str.replace('Year-','')\n# now combine information to create a new column \"Year\"\ndf_crudeoil_melted.loc[:,'Year'] = df_crudeoil_melted.loc[:,'Decade'] + df_crudeoil_melted.loc[:,'lastdigit']\n# Inspect the result\nprint(df_crudeoil_melted.head())\nUsing your new plotting skills, compare this data against the trends in the Asian GDPs showed earlier.\n\n\n\n(Lesson 10 is a break.)\n\n\n\n\n\n\n\n\nExercises Lesson 11, Lists\n\nFill in the blanks\nFill in the blanks so that the program below produces the output shown.\nvalues = ____\nvalues.____(1)\nvalues.____(3)\nvalues.____(5)\nprint('first time:', values)\nvalues = values[____]\nprint('second time:', values)\nOutput:\nfirst time: [1, 3, 5]\nsecond time: [3, 5]\n\n\nHow Large is a Slice?\nIf start and stop are both non-negative integers, how long is the list values[start:stop]?\n\n\nFrom Strings to Lists and Back\nGiven this:\nprint('string to list:', list('tin'))\nprint('list to string:', ''.join(['g', 'o', 'l', 'd']))\nOUTPUT:\nstring to list: ['t', 'i', 'n']\nlist to string: gold\n\nWhat does list(‘some string’) do?\nWhat does ‘-’.join([‘x’, ‘y’, ‘z’]) generate?\n\n\n\nWorking With the End\nWhat does the following program print?\nelement = 'helium'\nprint(element[-1])\n\nHow does Python interpret a negative index?\nIf a list or string has N elements, what is the most negative index that can safely be used with it, and what location does that index represent?\nIf values is a list, what does del values[-1] do?\nHow can you display all elements but the last one without changing values? (Hint: you will need to combine slicing and negative indexing.)\n\n\n\nStepping Through a List\nWhat does the following program print?\nelement = 'fluorine'\nprint(element[::2])\nprint(element[::-1])\n\nIf we write a slice as low:high:stride, what does stride do?\nWhat expression would select all of the even-numbered items from a collection?\n\n\n\n\n\n\n\n\nAdditional Exercises Lesson 11, Lists\n\nSlice Bounds\nWhat does the following program print?\nelement = 'lithium'\nprint(element[0:20])\nprint(element[-1:3])\n\n\nSort and Sorted\nWhat do these two programs print? In simple terms, explain the difference between sorted(letters) and letters.sort().\n# Program A\nletters = list('gold')\nresult = sorted(letters)\nprint('letters is', letters, 'and result is', result)\n# Program B\nletters = list('gold')\nresult = letters.sort()\nprint('letters is', letters, 'and result is', result)\n\n\nCopying (or Not)\nWhat do these two programs print? In simple terms, explain the difference between new = old and new = old[:].\n# Program A\nold = list('gold')\nnew = old      # simple assignment\nnew[0] = 'D'\nprint('new is', new, 'and old is', old)\n# Program B\nold = list('gold')\nnew = old[:]   # assigning a slice\nnew[0] = 'D'\nprint('new is', new, 'and old is', old)\n\n\nGo to lesson 3 (bioDSC)\n\nDo the additional exercises of lesson 3 if you haven’t already.\n\n\n\nNegative slicing (bioDSC)\nelement = 'lithium'\nWhat does element[-7:3] print and why?\nExplanation: https://www.geeksforgeeks.org/slicing-with-negative-numbers-in-python/\n\n\nMore list comprehensions and filtering (bioDSC)\n\nEdit the following code such that the list consists of values 2x+x^2-1 where x is the index of the list element.\n\n[x for x in range(10)]\n\nNow from that list, select values that are &gt; 10, by modifying the following code:\n\n[x for x in your_list if ______]\n\nConvert your list to a np.array, and do the same in a more elegant way.\nGiven: list_withtop = [1000+-10*(x-7)**2 for x in range(20)]\n\nFind the position of the maximum value in this array.\nEdit the code above such that the maximum value shifts to an index of your choice.\n\nCheck whether you succeedded by finding the maximum value.\n\nMultiply your list with -1, and put the result in another list.\n\nWhere are now the maximum and minimum values?\nDoes this make sense?\n\nlist_line = [70*x-1000 for x in range(20)]\n\nWhat’s the biggest value, either negative or positive, in this list?\nAnd the index of that number?\nWhat’s the standard deviation?\nCan you calculate the correlation between list_withtop and list_line?\nCan you make a scatter plot of list_withtop versus list_line?\n\n\n\n\n\n\n\n\n\n\nExercises Lesson 12, Loops\n\nTracing Execution\nCreate a table showing the numbers of the lines that are executed when this program runs, and the values of the variables after each line is executed.\ntotal = 0\nfor char in \"tin\":\n    total = total + 1\n\n\n\nStep\nLine Number\nVariable Values\n\n\n\n\n1\n1\ntotal = 0\n\n\n2\n..\ntotal = 0, char = ‘t’\n\n\n3\n..\n..\n\n\n4\n..\n..\n\n\n5\n..\n..\n\n\n..\n..\n..\n\n\n\n\n\nPractice Accumulating\nFill in the blanks in each of the programs below to produce the indicated result.\n# Total length of the strings in the list: [\"red\", \"green\", \"blue\"] =&gt; 12\ntotal = 0\nfor word in [\"red\", \"green\", \"blue\"]:\n    ____ = ____ + len(word)\nprint(total)\n\n\nPractice Accumulating 2 (continued)\n# List of word lengths: [\"red\", \"green\", \"blue\"] =&gt; [3, 5, 4]\nlengths = ____\nfor word in [\"red\", \"green\", \"blue\"]:\n    lengths.____(____)\nprint(lengths)\n\n\nPractice Accumulating 3 (continued)\n# Concatenate all words: [\"red\", \"green\", \"blue\"] =&gt; \"redgreenblue\"\nwords = [\"red\", \"green\", \"blue\"]\nresult = ____\nfor ____ in ____:\n    ____\nprint(result)\n\n\n\n\n\n\n\nAdditional Exercises Lesson 12, Loops\n\nPlotting automation\nRemember this code from Lesson 9?\ndata_all = pd.read_csv('data/gapminder_all.csv', index_col='country')\nsns.scatterplot(data_all, x='gdpPercap_2007', y='lifeExp_2007', s=data_all['pop_2007']/1e6)\nplt.text(df_all.loc['United States','gdpPercap_2007'], df_all.loc['United States','lifeExp_2007'], 'United States')\nplt.text(df_all.loc['Netherlands','gdpPercap_2007'], df_all.loc['Netherlands','lifeExp_2007'], 'Netherlands')\nTry to annotate 10 selected countries automatically.\n\n\nReversing a String\nFill in the blanks in the program below so that it prints “nit” (the reverse of the original character string “tin”).\noriginal = \"tin\"\nresult = ____\nfor char in original:\n    result = ____\nprint(result)\nbioDSC hint If this is challenging: - try to first reproduce the word tin in result, using this loop. - Use a similar approach as the examples we used.\n\n\nPractice Accumulating (continued from above)\nCreate an acronym: Starting from the list [“red”, “green”, “blue”], create the acronym “RGB” using a for loop.\n\nbioDSC remark: Note the capitals in “RGB”!\nHint: You may need to use a string method to properly format the acronym.\n\n\n\nIdentifying Item Errors\n\nRead the code below and try to identify what the errors are without running it.\nRun the code, and read the error message. What type of error is it?\nFix the error.\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[4])\n\n\nCumulative Sum (code puzzle)\nReorder and properly indent the lines of code below so that they print a list with the cumulative sum of data. The result should be [1, 3, 5, 10].\ncumulative.append(total)\nfor number in data:\ncumulative = []\ntotal = total + number\ntotal = 0\nprint(cumulative)\ndata = [1,2,2,5]\n\n\nIdentifying Variable Name Errors\n\nRead the code below and try to identify what the errors are without running it.\nRun the code and read the error message. What type of NameError do you think this is? Is it a string with no quotes, a misspelled variable, or a variable that should have been defined but was not?\nFix the error.\nRepeat steps 2 and 3, until you have fixed all the errors.\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (Number % 3) == 0:\n        message = message + a\n    else:\n        message = message + \"b\"\nprint(message)\n\n\nClassifying Errors\nIs an indentation error a syntax error or a runtime error?\n\n\n\n\n\n\n\nExercises Lesson 13, Conditionals\n\nTracing Execution\nWhat does this program print?\npressure = 71.9\nif pressure &gt; 50.0:\n    pressure = 25.0\nelif pressure &lt;= 50.0:\n    pressure = 0.0\nprint(pressure)\n\n\nTrimming Values\nFill in the blanks so that this program creates a new list containing zeroes where the original list’s values were negative and ones where the original list’s values were positive.\noriginal = [-1.5, 0.2, 0.4, 0.0, -1.3, 0.4]\nresult = ____\nfor value in original:\n    if ____:\n        result.append(0)\n    else:\n        ____\nprint(result)\nDesired output:\n[0, 1, 1, 1, 0, 1]\n\n\nProcessing Small Files\nModify this program so that it only processes files with fewer than 50 records.\nimport glob\nimport pandas as pd\nfor filename in glob.glob('data/*.csv'):\n    contents = pd.read_csv(filename)\n    ____:\n        print(filename, len(contents))\n\n\n\n\n\n\n\nAdditional Exercises Lesson 13, Conditionals\n\nList comprehension (bioDSC)\n\nA\nAdapt the following code to select only positive values:\nexample_list = [1,2,3,4,-5,1,34,6,-10, 39]\nexample_list_pos = [___ for item in example_list if ___]\nprint(example_list_pos)\n\n\nB\nUse the same code, but: - select values between 30 and 40 - select items &lt;0 or &gt;10\n\n\nC\nUse a np.array (see additional exercises Lesson 3) to do the same more elegantly.\n\n\n\nEnumerate, zip (bioDSC)\nThese exercises introduce two new concepts. You might need google.\n\nA\nWhat does the following code do? What is the meaning of the output?\nfor idx, item in enumerate([1,2,3,4,-5,1,34,6,-10]):\n    \n    if item&gt;5:\n        print(idx)\n\n\nB\nModify the following code such that it will compare each item i in apples with each item i in pears, and tell you which one is heavier. You need to edit the code.\napples = [123, 436, 123, 654, 117, 193, 120]\npears  = [543, 163, 178, 165, 123, 187, 190]\n\nfor apple_weight, pear_weight in zip(apples, pears):\n    print('='*10)\n    print('weigth apple: ', apple_weight)\n    print('weigth pear: ',pear_weight)\n    \n    print('the XXX is heavier')\n\n\n\nInitializing\nModify this program so that it finds the largest and smallest values in the list no matter what the range of values originally is.\nvalues = [...some test data...]\nsmallest, largest = None, None\nfor v in values:\n    if ____:\n        smallest, largest = v, v\n    ____:\n        smallest = min(____, v)\n        largest = max(____, v)\nprint(smallest, largest)\nWhat are the advantages and disadvantages of using this method to find the range of the data?\nbioDSC hints\nThe loop could also look as follows:\nvalues = [...some test data...]\nsmallest, largest = None, None\nfor v in values:\n    smallest = min(____, v)\n    largest = max(____, v)\nWhy wouldn’t this work?\nThis is why the if statement is needed.\nHow can we test whether we are in the first iteration?\n\n\n\n\n\n\n\nExercises Lesson 14, Looping over data\n\nDetermining Matches\nWhich of these files is not matched by the expression glob.glob(‘data/as.csv’)?\ndata/gapminder_gdp_africa.csv\ndata/gapminder_gdp_americas.csv\ndata/gapminder_gdp_asia.csv\n\n\nMinimum File Size\nModify this program so that it prints the number of records in the file that has the fewest records.\nimport glob\nimport pandas as pd\nfewest = ____\nfor filename in glob.glob('data/*.csv'):\n    dataframe = pd.____(filename)\n    fewest = min(____, dataframe.shape[0])\nprint('smallest file has', fewest, 'records')\nNote that the DataFrame.shape() method returns a tuple with the number of rows and columns of the data frame.\n\n\n\n\n\n\n\nAdditional Exercises Lesson 14, Looping over data\n\nComparing Data\nWrite a program that reads in the regional data sets and plots the average GDP per capita for each region over time in a single chart.\nPandas will raise an error if it encounters non-numeric columns in a dataframe computation so you may need to either filter out those columns or tell pandas to ignore them.\n\n\n\n(Lesson 15 is a break.)\n\n\n\n\n\n\n\n\nExercises Lesson 16, Functions\n\nIdentifying Syntax Errors\nRead the code below and try to identify what the errors are without running it. Run the code and read the error message. Is it a SyntaxError or an IndentationError? Fix the error.\ndef another_function\n  print(\"Syntax errors are annoying.\")\n   print(\"But at least python tells us about them!\")\n  print(\"So they are usually not too hard to fix.\")\n\n\nWhat does the following program print?\ndef report(pressure):\n    print('pressure is', pressure)\n\nprint('calling', report, 22.5)\n\n\nEncapsulation\nFill in the blanks to create a function that takes a single filename as an argument, loads the data in the file named by the argument, and returns the minimum value in that data.\nimport pandas as pd\n\ndef min_in_data(____):\n    data = ____\n    return ____\n\n\n\n\n\n\n\nAdditional Exercises Lesson 16, Functions\n\nOrder of Operations\nWhat’s wrong in this example?\nresult = print_time(11, 37, 59)\n\ndef print_time(hour, minute, second):\n   time_string = str(hour) + ':' + str(minute) + ':' + str(second)\n   print(time_string)\nAfter fixing the problem above, explain why running this example code:\nresult = print_time(11, 37, 59)\nprint('result of call is:', result)\ngives this output:\n11:37:59\nresult of call is: None\nWhy is the result of the call None?\n\n\nFind the First\nFill in the blanks to create a function that takes a list of numbers as an argument and returns the first negative value in the list. What does your function do if the list is empty? What if the list has no negative numbers?\ndef first_negative(values):\n    for v in ____:\n        if ____:\n            return ____\n\n\nCalling by Name\nEarlier we saw this function:\ndef print_date(year, month, day):\n    joined = str(year) + '/' + str(month) + '/' + str(day)\n    print(joined)\nWe saw that we can call the function using named arguments, like this:\nprint_date(day=1, month=2, year=2003)\n\nWhat does print_date(day=1, month=2, year=2003) print?\nWhen have you seen a function call like this before?\nWhen and why is it useful to call functions this way?\n\n\n\nWrite your own function\nSee the exercise “Write your own function and import it” from the additional exercises in lesson 6.\n\n\nOmitted exercises\nContinue with more exercises at: https://swcarpentry.github.io/python-novice-gapminder/instructor/16-writing-functions.html\n(See: “Encapsulation of an If/Print Block”.)\n\n\nPrimes\nWrite a function that looks as follows:\ndef calculate_primes(N):\n    ...\nthat returns an array with prime numbers between 0 and N.\nHints: - Start with writing a function is_number_prime(X), that checks whether \\(X\\) is a prime number. - You probably need the following ingredients for that function: - How to test if a number is divisible by any number? - Use a for loop to test whether \\(X\\) can be divided by all numbers \\(&lt;X\\). - Could be convenient to make smart use of the return function.\ndef is_number_prime(X):\n    \n    # &lt;insert explanatory comment&gt;\n    for y in range(X):\n    \n        # &lt;insert explanatory comment&gt;\n        if ...:\n            return False\n    \n    # &lt;insert explanatory comment&gt;\n    return True\n\ndef calculate_primes(N):\n    ..."
  },
  {
    "objectID": "workshop-materials/py-intro/03-data-types.html",
    "href": "workshop-materials/py-intro/03-data-types.html",
    "title": "Exercises Lesson 3, types",
    "section": "",
    "text": "disclaimer\n\n\n\nMost questions are copied from the carpentry lesson “Plotting and Programming in Python”\n\n\n\n\nExercises Lesson 3, types\n\n1. Types\n\n\n\n\n\n\nQuestion A\n\n\n\nWhat type of value is 3.4? How can you find out?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a floating-point number (often abbreviated “float”). It is possible to find out by using the built-in function type().\nprint(type(3.4))\n&lt;class 'float'&gt;\n\n\n\n\n\n\n\n\n\n\n\nQuestion B\n\n\n\nWhat type of value is 3.25 + 4?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a float: integers are automatically converted to floats as necessary.\nresult = 3.25 + 4\nprint(result, 'is', type(result))\n7.25 is &lt;class 'float'&gt;\n\n\n\n\n\n\n\n\n\nQuestion C\n\n\n\nWhat type of value (integer, floating point number, or character string) would you use to represent each of the following? Try to come up with more than one good answer for each problem. For example, in # 1, when would counting days with a floating point variable make more sense than using an integer?\n\nNumber of days since the start of the year.\nTime elapsed from the start of the year until now in days.\nSerial number of a piece of lab equipment.\nA lab specimen’s age\nCurrent population of a city.\nAverage population of a city over time.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe answers to the questions are:\n\nInteger, since the number of days would lie between 1 and 365.\nFloating point, since fractional days are required\nCharacter string if serial number contains letters and numbers, otherwise integer if the serial number consists only of numerals\nThis will vary! How do you define a specimen’s age? whole days since collection (integer)? date and time (string)?\nChoose floating point to represent population as large aggregates (eg millions), or integer to represent population in units of individuals.\nFloating point number, since an average is likely to have a fractional part.\n\n\n\n\n\n\n\n\n\n\nQuestion D (bioDSC)\n\n\n\nWhy wouldn’t you always use floats, and never use integers?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAn integer has exact precision and a larger range, whereas float has limited precision and can represent numbers with a decimal part.\n\n\n\n\n\n\n2. Strings to numbers\nWhere reasonable, float() will convert a string to a floating point number, and int() will convert a floating point number to an integer:\nprint(\"string to float:\", float(\"3.4\"))\nprint(\"float to int:\", int(3.4))\nOUTPUT:\nstring to float: 3.4\nfloat to int: 3\nIf the conversion doesn’t make sense, however, an error message will occur.\nprint(\"string to float:\", float(\"Hello world!\"))\n\n\n\n\n\n\nQuestion E\n\n\n\n\nWhat do you expect the following program to do?\nWhat does it actually do?\nWhy do you think it does that?\n\nprint(\"fractional string to int:\", int(\"3.4\"))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWhat do you expect this program to do? It would not be so unreasonable to expect the Python 3 int command to convert the string “3.4” to 3.4 and an additional type conversion to 3. After all, Python 3 performs a lot of other magic - isn’t that part of its charm?\nHowever, Python 3 throws an error.\n\nprint(int(\"3.4\"))\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: invalid literal for int() with base 10: '3.4'\n\nWhy? To be consistent, possibly. If you ask Python to perform two consecutive typecasts, you must convert it explicitly in code.\n\nprint(int(float(\"3.4\")))\n3\n\n\n\n\n\n\n\n\n\nQuestion F\n\n\n\nWhich of the following will return the floating point number 2.0? Note: there may be more than one right answer.\nfirst = 1.0\nsecond = \"1\"\nthird = \"1.1\"\n1. first + float(second)\n2. float(second) + float(third)\n3. first + int(third)\n4. first + int(float(third))\n5. int(first) + int(float(third))\n6. 2.0 * second\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n2.0\n2.1\nValueError\n2.0\n2\nTypeError\n\nSo, 1 and 4\n\n\n\n\n\n\n\n\n\n\nAdditional exercises\n\n3. Lists\nWe’ll cover lists later in lesson 11, but let’s already take a brief look.\nA list is a series of elements bound together, where each element can have a value. They are defined as follows:\nnumbers = [1,2,3]\nfruits = ['apples', 'pears', 'oranges']\nphysical_constants = ['pi', 3.14, 'c', 299_792_458, 'mole', 6.022e23]\nElements can be accessed the same way as we saw with strings.\n\n\n\n\n\n\nQuestion G\n\n\n\nWhat will numbers[1] return? And physical_constants[2:4]?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nnumbers[1] will return 1 because in Python indexing vectors starts at position 0 (=0+1)\nnumbers[2:4] returns ['c',299792458] because indexing starts at the 3rd position (=2+1) and the slicing parameters stops at the position 4-1 (=3+1)\n\n\n\n\n\n\n\n\n\nQuestion H\n\n\n\nWhat is the type of\n\nnumbers?\nnumbers[1]?\nphysical_constants?\nphysical_constants[1]?\nphysical_constants[2]?\nphysical_constants[3]?\nfruits?\nfruits[1]?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntype(numbers) &lt;class 'list'&gt;\ntype(numbers[1]) &lt;class 'int'&gt;\ntype(physical_constants) &lt;class 'list'&gt;\ntype(physical_constants[1]) &lt;class 'float'&gt;\ntype(physical_constants[2]) &lt;class 'str'&gt;\ntype(physical_constants[3]) &lt;class 'int'&gt;\ntype(fruits) &lt;class 'list'&gt;\ntype(fruits[1]) &lt;class 'str'&gt;\n\n\n\n\n\n\n\n\n\n\nQuestion I\n\n\n\nCan the elements in a list have different types? (This can be seen from the previous answer.)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes, a single list may contain numbers, strings, and anything else.\n\n\n\n\n\n3. np.array\nLists can be a useful tool, but for example in image analysis, don’t offer the full mathematical options one might like. numpy arrays introduce a new type of series, in which you can do more manipulations. See some examples below:\nimport numpy as np\nmy_array = [1,2,3,5]\nmy_array_np = np.array([1,2,3,5])\n\n\n\n\n\n\nQuestion I\n\n\n\nwhat’s the difference here:\n\nmy_array+1\nmy_array_np+1\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array+1, throws an error message because there is no operator defined that accepts a list type and an int type. If you want to concatenate the two, use my_array+[1]\nmy_array_np+1, returns array([2, 3, 4, 6]), an np.array where every element from my_array_np has been increased by 1\n\n\n\n\n\n\n\n\n\n\nQuestion J\n\n\n\nwhat’s the difference here:\n\nmy_array * 3\nmy_array_np * 3\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array * 3, returns [1, 2, 3, 5, 1, 2, 3, 5, 1, 2, 3, 5] because the * operator repeats the list 3 times\nmy_array_np * 3, returns array([3, 6, 9, 15]) because in numpy, the * multplies the value of each element 3 times\n\n\n\n\n\n\n\n\n\n\nQuestion K\n\n\n\nwhat’s the difference here:\n\nmy_array + [1,2,3,4]\nmy_array_np + np.array([1, 2, 3, 4])\nnp.sin(my_array_np)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array + [1,2,3,4], returns a concatenated list [1, 2, 3, 5, 1, 2, 3, 4]. Like in question J, the + operator concatenates two lists.\nmy_array_np + np.array([1, 2, 3, 4]), returns the element wise sum of both vectors, so array([2, 4 ,6 ,9])\nnp.sin(my_array_np), returns the result of applying the sin function to each element in my_array_np, array([ 0.84147098, 0.90929743, 0.14112001, -0.95892427])\n\n\n\n\n\n\n\n\n\n\nQuestion L\n\n\n\nnumpy-specific things, what is happening?\n\nmy_array_np[range(1,4,2)]\nmy_array_np[my_array_np&gt;1]\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_array_np[range(1,4,2)], returns array([2,5]), the second and fourth element of my_array_np. range(1, 4, 2) generates numbers from 1 to 4 with a stepsize of 2\nmy_array_np[my_array_np&gt;1], returns array([2,3,5]), the &gt; 1 clause only selects those elements whos values is larger than 1.\n\n\n\n\n\n\n\n\n\n\nQuestion M\n\n\n\nCan numpy arrays have different types? What is the type of the elements in these two arrays?\n\nnp.array([1,2,3,'4'])\nnp.array([1,2,3,'hello'])\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nnp.array([1,2,3,'4']), returns array([‘1’, ‘2’, ‘3’, ‘4’], dtype=‘&lt;U21’)\nnp.array([1,2,3,'hello']), returns array([‘1’, ‘2’, ‘3’, ‘hello’], dtype=‘&lt;U21’)\n\nThe elements of a NumPy array must all be of the same type, whereas the elements of a Python list can be of completely different types.\n\n\n\n\n\n4. Dict(ionary)\nPython has more types. A dict is sometimes very convenient, and is also used later, when creating tables.\nexperimental_replicate_list = {'WT': 12, 'mut': 32, 'WT.cond1': 10, 'mut.cond1': 12}\nprint(experimental_replicate_list)\nprint(experimental_replicate_list['WT'])\n\n\n\n\n\n\nQuestion N\n\n\n\nEdit the following code such that we get the associated numbers for conditions involving WT\nmy_keys = experimental_replicate_list.keys()\nprint(my_keys)\nmy_keys_of_interest = [the_key for the_key in my_keys if 'mut' in the_key] # edit this line\nprint(my_keys_of_interest)\nprint([experimental_replicate_list[sel_key] for sel_key in my_keys_of_interest])\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmy_keys_of_interest = [the_key for the_key in my_keys if 'WT' in the_key]\nprint(my_keys_of_interest), returns [‘WT’, ‘WT.cond1’]\nprint([experimental_replicate_list[sel_key] for sel_key in my_keys_of_interest]), returns [12,10]\n\n\n\n\n\n\n\n\n\n\nQuestion O\n\n\n\nThe above code uses several lines, can you do this in one line?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nprint([experimental_replicate_list[key] for key in experimental_replicate_list.keys() if 'WT' in key])\n\n\n\n\n\n\nSpecial maths (Carpentries)\nIn Python 3, the // operator performs integer (whole-number) floor division, the / operator performs floating-point division, and the % (or modulo) operator calculates and returns the remainder from integer division:\nprint('5 // 3:', 5 // 3)\nprint('5 / 3:', 5 / 3)\nprint('5 % 3:', 5 % 3)\nOUTPUT:\n5 // 3: 1\n5 / 3: 1.6666666666666667\n5 % 3: 2\nIf num_subjects is the number of subjects taking part in a study, and num_per_survey is the number that can take part in a single survey, write an expression that calculates the number of surveys needed to reach everyone once.\n\n\nComplex numbers (Carpentries)\nPython provides complex numbers, which are written as 1.0+2.0j. If val is a complex number, its real and imaginary parts can be accessed using dot notation as val.real and val.imag.\na_complex_number = 6 + 2j\nprint(a_complex_number.real)\nprint(a_complex_number.imag)\n\n\n\n\n\n\nQuestion P\n\n\n\n\nWhy do you think Python uses j instead of i for the imaginary part?\nWhat do you expect 1 + 2j + 3 to produce?\nWhat do you expect 4j to be? What about 4 j or 4 + j?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nPython uses the symbol ‘j’ for the imaginary part of complex numbers instead of ‘i’ to prevent confusion with electric current, which is represented by ‘i’ in electrical engineering. This choice caters to the professional background of many Python developers.\n4 + 2j, the real parts can be added and the imaginary part remains the same\n4j is 4 times the complex number j (4*1j), 4 j will generate a syntax error, it’s missing an operator. 4 + j will treat j as a variable and try to take the sum of both but throws an error if j is not defined or of the wrong type."
  },
  {
    "objectID": "workshop-materials/py-intro/03-data-types.html#answer-9",
    "href": "workshop-materials/py-intro/03-data-types.html#answer-9",
    "title": "Exercises Lesson 3, types",
    "section": "Answer",
    "text": "Answer\n\nmy_array+1, throws an error message because there is no operator defined that accepts a list type and an int type. If you want to concatenate the two, use my_array+[1]\nmy_array_np+1, returns array([2, 3, 4, 6]), an np.array where every element from my_array_np has been increased by 1 :::"
  },
  {
    "objectID": "workshop-materials/py-intro/04-functions.html",
    "href": "workshop-materials/py-intro/04-functions.html",
    "title": "Exercises Lesson 4, functions (1)",
    "section": "",
    "text": "disclaimer\n\n\n\nMost questions are copied from the carpentry lesson “Plotting and Programming in Python”\n\n\n\n\nExercises Lesson 4, functions (1)\n\nOrder\n\n\n\n\n\n\nQuestion A\n\n\n\n\nExplain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc.\nWhat is the final value of radiance?\n\nradiance = 1.0\nradiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nradiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5))\nDivision and multiplication precede subtraction and addtition. The use of parentheses can change the evaluation order. In this example the order of operations:\n\n1.1 * radiance = 1.1\n1.1 - 0.5 = 0.6\nmin(radiance, 0.6) = 0.6\n2.0 + 0.6 = 2.6\nmax(2.1, 2.6) = 2.6\nAt the end, radiance = 2.6\n\n\n\n\n\n\nLast string character\n\n\n\n\n\n\nQuestion B\n\n\n\nIf Python starts counting from zero, and len returns the number of characters in a string, what index expression will get the last character in the string name? (Note: we will see a simpler way to do this in a later episode.)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nname[len(name) - 1]\n\n\n\n\n\n\nAdditional exercises Lesson 4\n\nWhy not?\n\n\n\n\n\n\nQuestion C\n\n\n\nWhy is it that max and min do not return None when they are called with no arguments?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmax and min return TypeErrors in this case because the correct number of parameters was not supplied. If it just returned None, the error would be much harder to trace as it would likely be stored into a variable and used later in the program, only to likely throw a runtime error.\n\n\n\n\n\nSpot the difference\n\n\n\n\n\n\nQuestion D\n\n\n\nPredict what each of the print statements in the program below will print. Does max(len(rich), poor) run or produce an error message? If it runs, does its result make any sense?\neasy_string = \"abc\"\nprint(max(easy_string))\nrich = \"gold\"\npoor = \"tin\"\nprint(max(rich, poor))\nprint(max(len(rich), len(poor)))\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nprint(max(easy_string)), returns c, the largest of the characters (in Unicode values) in the string\nprint(max(rich, poor)), returns tin, the string that contains the largest character (in Unicode values)\nprint(max(len(rich), len(poor))), returns 4, the length of the string that contains the most characters (i.e. rich)"
  },
  {
    "objectID": "workshop-materials/py-intro/06-libraries.html",
    "href": "workshop-materials/py-intro/06-libraries.html",
    "title": "Exercises Lesson 6, libraries",
    "section": "",
    "text": "disclaimer\n\n\n\nMost questions are copied from the carpentry lesson “Plotting and Programming in Python”\n\n\n\n\nExercises Lesson 6, libraries\n\nExplore\n\n\n\n\n\n\nQuestion A\n\n\n\n\nWhat function from the math module can you use to calculate a square root without using sqrt?\nSince the library contains this function, why does sqrt exist?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nUsing help(math) we see that we’ve got pow(x,y) in addition to sqrt(x), so we could use pow(x, 0.5) to find a square root.\nThe sqrt(x) function is arguably more readable than pow(x, 0.5) when implementing equations. Readability is a cornerstone of good programming, so it makes sense to provide a special function for this specific common case.\n\nAlso, the design of Python’s math library has its origin in the C standard, which includes both sqrt(x) and pow(x,y), so a little bit of the history of programming is showing in Python’s function names.\n\n\n\n\n\nFind the right module\nYou want to select a random character from a string:\nbases = 'ACTTGCTTGAC'\n\n\n\n\n\n\nQuestion B\n\n\n\nWhich standard library module could help you? Which function would you select from that module? Are there alternatives? Try to write a program that uses the function.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe random module seems like it could help.\n\nThe string has 11 characters, each having a positional index from 0 to 10. You could use the random.randrange or random.randint functions to get a random integer between 0 and 10, and then select the bases character at that index:\nfrom random import randrange\nrandom_index = randrange(len(bases))\nprint(bases[random_index])\nor more compactly:\nfrom random import randrange\nprint(bases[randrange(len(bases))])\nPerhaps you found the random.sample function? It allows for slightly less typing but might be a bit harder to understand just by reading:\nfrom random import sample\nprint(sample(bases, 1)[0])\nNote that this function returns a list of values. We will learn about lists in episode 11.\nThe simplest and shortest solution is the random.choice function that does exactly what we want:\nfrom random import choice\nprint(choice(bases))\n\n\n\n\n\nHelp!\nWhen a colleague of yours types help(math), Python reports an error:\nNameError: name 'math' is not defined\n\n\n\n\n\n\nQuestion C\n\n\n\nWhat has your colleague forgotten to do?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour colleague forgot to import the math module (import math)\n\n\n\n\n\nImporting with aliases\n\n\n\n\n\n\nQuestion D\n\n\n\n\nFill in the blanks so that the program below prints 90.0.\nRewrite the program so that it uses import without as.\nWhich form do you find easier to read?\n\nimport math as m\nangle = ____.degrees(____.pi / 2)\nprint(____)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nFilling in the blanks\n\nimport math as m\nangle = m.degrees(m.pi / 2)\nprint(angle)\n\ncan be written as:\n\nimport math\nangle = math.degrees(math.pi / 2)\nprint(angle)\n\nSince you just wrote the code and are familiar with it, you might actually find the first version easier to read. But when trying to read a huge piece of code written by someone else, or when getting back to your own huge piece of code after several months, non-abbreviated names are often easier, except where there are clear abbreviation conventions.\n\n\n\n\n\n\n\n\n\n\nQuestion E\n\n\n\nMultiple ways of importing\nMatch the following print statements with the appropriate library calls.\nPrint commands:\n\nprint(\"sin(pi/2) =\", sin(pi/2))\nprint(\"sin(pi/2) =\", m.sin(m.pi/2))\nprint(\"sin(pi/2) =\", math.sin(math.pi/2))\n\nLibrary calls:\n\nfrom math import sin, pi\nimport math\nimport math as m\nfrom math import *\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLibrary calls 1 and 4. In order to directly refer to sin and pi without the library name as prefix, you need to use the from … import … statement. Whereas library call 1 specifically imports the two functions sin and pi, library call 4 imports all functions in the math module.\nLibrary call 3. Here sin and pi are referred to with a shortened library name m instead of math. Library call 3 does exactly that using the import … as … syntax - it creates an alias for math in the form of the shortened name m.\nLibrary call 2. Here sin and pi are referred to with the regular library name math, so the regular import … call suffices.\n\nNote: although library call 4 works, importing all names from a module using a wildcard import is not recommended as it makes it unclear which names from the module are used in the code. In general it is best to make your imports as specific as possible and to only import what your code uses. In library call 1, the import statement explicitly tells us that the sin function is imported from the math module, but library call 4 does not convey this information.\n\n\n\n\n\n\nAdditional Exercises Lesson 6, Libraries\n\n\n\n\n\n\nQuestion F\n\n\n\n“Jigsaw”: progamming example\nRearrange the following statements so that a random DNA base is printed and its index in the string. Not all statements may be needed. Feel free to use/add intermediate variables.\nbases=\"ACTTGCTTGAC\"\nimport math\nimport random\n___ = random.randrange(n_bases)\n___ = len(bases)\nprint(\"random base \", bases[___], \"base index\", ___)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport math \nimport random\nbases = \"ACTTGCTTGAC\" \nn_bases = len(bases)\nidx = random.randrange(n_bases)\nprint(\"random base\", bases[idx], \"base index\", idx)\n\n\n\n\nImporting specific items\n\n\n\n\n\n\nQuestion G\n\n\n\n\nFill in the blanks so that the program below prints 90.0.\nDo you find this version easier to read than preceding ones?\nWhy wouldn’t programmers always use this form of import?\n\n____ math import ____, ____\nangle = degrees(pi / 2)\nprint(angle)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom math import degrees, pi\nangle = degrees(pi / 2)\nprint(angle)\n\n\n\n\n\nReading error messages\n\n\n\n\n\n\nQuestion H\n\n\n\n\nRead the code below and try to identify what the errors are without running it.\nRun the code, and read the error message. What type of error is it?\n\nfrom math import log\nlog(0)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-1-d72e1d780bab&gt; in &lt;module&gt;\n      1 from math import log\n----&gt; 2 log(0)\n\nValueError: math domain error\n\nThe logarithm of x is only defined for x &gt; 0, so 0 is outside the domain of the function.\nYou get an error of type ValueError, indicating that the function received an inappropriate argument value. The additional message “math domain error” makes it clearer what the problem is.\n\n\n\n\n\n\nWrite your own function and import it\nWhen you have a notebook file, you can also create another file, with a .py extension, and write functions in that file. The .py file can be imported like a library, and the functions in the file can be used as if they came from a library.\n\n\nExercise\n\n\n\n\n\n\nQuestion I\n\n\n\nUsing the information below, try to - create two files, one .ipynb (notebook) file and one .py (python plain text code) file. - rename the myfunctionname functions in the .py file and use them in the notebook. - create a third function, which returns C when you provide A and B, assuming A^2+B^2 = C^2, and use it in your notebook.\nUseful things to know: - You can also make .py files. Unlike notebooks, every text here is assumed to be python code. - For Jupyter notebooks: - You can make a .py file with file &gt; new &gt; python file.\n- Save the file to myfilename.py (replacing myfilename with your own favorite name). - For Google colabs: - To create a .py file, right click in the file overview (where you also put the gapminder .csv files), and select ‘new file’. Then create a file ‘myfilename.py’ and double click to edit it. - You can import your file in a python notebook using: - import myfilename where myfilename.py should exist and hold your code. You can also put your file in a different directory, but then you need to import it like import mydirectoryname.myfilename.\nYou can write a function using the following template:\n\ndef myfunctionname():\n    print(\"hello world\")\n    \ndef myfunctionname2(input1, input2):\n    print(\"input 1 = \", input1, ', input 2 = ', input2)"
  },
  {
    "objectID": "workshop-materials/py-intro/07-dataframes-1.html",
    "href": "workshop-materials/py-intro/07-dataframes-1.html",
    "title": "Exercises lesson 7, dataframes (1)",
    "section": "",
    "text": "disclaimer\n\n\n\nMost questions originate from the carpentry lesson “Plotting and Programming in Python”\n\n\n\n\nExercises lesson 7, dataframes (1)\n\nReading other data\n\n\n\n\n\n\nQuestion A\n\n\n\nRead the data in gapminder_gdp_americas.csv (which should be in the same directory as gapminder_gdp_oceania.csv) into a variable called data_americas and display its summary statistics.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo read in a CSV, we use pd.read_csv and pass the filename ‘data/gapminder_gdp_americas.csv’ to it. We also once again pass the column name ‘country’ to the parameter index_col in order to index by country. The summary statistics can be displayed with the DataFrame.describe() method.\ndata_americas = pd.read_csv('data/gapminder_gdp_americas.csv', index_col='country')\ndata_americas.describe()\n\n\n\n\n\nInspecting data\n\n\n\n\n\n\nQuestion B\n\n\n\nAfter reading the data for the Americas, use help(data_americas.head) and help(data_americas.tail) to find out what DataFrame.head and DataFrame.tail do.\n\nWhat method call will display the first three rows of this data?\nWhat method call will display the last three columns of this data? (Hint: you may need to change your view of the data.)\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWe can check out the first five rows of data_americas by executing data_americas.head() which lets us view the beginning of the DataFrame. We can specify the number of rows we wish to see by specifying the parameter n in our call to data_americas.head(). To view the first three rows, execute:\n\ndata_americas.head(n=3)\ncontinent  gdpPercap_1952  gdpPercap_1957  gdpPercap_1962  \\\ncountry\nArgentina  Americas     5911.315053     6856.856212     7133.166023\nBolivia    Americas     2677.326347     2127.686326     2180.972546\nBrazil     Americas     2108.944355     2487.365989     3336.585802\n\n          gdpPercap_1967  gdpPercap_1972  gdpPercap_1977  gdpPercap_1982  \\\ncountry\nArgentina     8052.953021     9443.038526    10079.026740     8997.897412\nBolivia       2586.886053     2980.331339     3548.097832     3156.510452\nBrazil        3429.864357     4985.711467     6660.118654     7030.835878\n\n           gdpPercap_1987  gdpPercap_1992  gdpPercap_1997  gdpPercap_2002  \\\ncountry\nArgentina     9139.671389     9308.418710    10967.281950     8797.640716\nBolivia       2753.691490     2961.699694     3326.143191     3413.262690\nBrazil        7807.095818     6950.283021     7957.980824     8131.212843\n\n           gdpPercap_2007\ncountry\nArgentina    12779.379640\nBolivia       3822.137084\nBrazil        9065.800825\n\nTo check out the last three rows of data_americas, we would use the command, americas.tail(n=3), analogous to head() used above. However, here we want to look at the last three columns so we need to change our view and then use tail(). To do so, we create a new DataFrame in which rows and columns are switched:\n\namericas_flipped = data_americas.T\nWe can then view the last three columns of americas by viewing the last three rows of americas_flipped:\namericas_flipped.tail(n=3)\ncountry        Argentina  Bolivia   Brazil   Canada    Chile Colombia  \\\ngdpPercap_1997   10967.3  3326.14  7957.98  28954.9  10118.1  6117.36\ngdpPercap_2002   8797.64  3413.26  8131.21    33329  10778.8  5755.26\ngdpPercap_2007   12779.4  3822.14   9065.8  36319.2  13171.6  7006.58\n\ncountry        Costa Rica     Cuba Dominican Republic  Ecuador    ...     \\\ngdpPercap_1997    6677.05  5431.99             3614.1  7429.46    ...\ngdpPercap_2002    7723.45  6340.65            4563.81  5773.04    ...\ngdpPercap_2007    9645.06   8948.1            6025.37  6873.26    ...\n\ncountry          Mexico Nicaragua   Panama Paraguay     Peru Puerto Rico  \\\ngdpPercap_1997   9767.3   2253.02  7113.69   4247.4  5838.35     16999.4\ngdpPercap_2002  10742.4   2474.55  7356.03  3783.67  5909.02     18855.6\ngdpPercap_2007  11977.6   2749.32  9809.19  4172.84  7408.91     19328.7\n\ncountry        Trinidad and Tobago United States  Uruguay Venezuela\ngdpPercap_1997             8792.57       35767.4  9230.24   10165.5\ngdpPercap_2002             11460.6       39097.1     7727   8605.05\ngdpPercap_2007             18008.5       42951.7  10611.5   11415.8\nThis shows the data that we want, but we may prefer to display three columns instead of three rows, so we can flip it back:\namericas_flipped.tail(n=3).T    \nNote: we could have done the above in a single line of code by ‘chaining’ the commands:\ndata_americas.T.tail(n=3).T\n\n\n\n\n\nNavigating directories\n\n\n\n\n\n\nQuestion C\n\n\n\nThe data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis:\nyour_home_directory\n+-- field_data/\n|   +-- microbes.csv\n+-- thesis/\n    +-- analysis.ipynb\nWhat value(s) should you pass to read_csv to read microbes.csv in analysis.ipynb?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe need to specify the path to the file of interest in the call to pd.read_csv. We first need to ‘jump’ out of the folder thesis using ‘../’ and then into the folder field_data using ‘field_data/’. Then we can specify the filename `microbes.csv. The result is as follows:\ndata_microbes = pd.read_csv('../field_data/microbes.csv')\n\n\n\n\n\nWriting data\nAs well as the read_csv function for reading data from a file, Pandas provides a to_csv function to write dataframes to files. Applying what you’ve learned about reading from files, write one of your dataframes to a file called processed.csv. You can use help to get information on how to use to_csv.\n\n\n\n\n\n\n\nAdditional exercises\n\n\n\n\n\n\nQuestion D\n\n\n\n\nCreate some sample data\n\nCreate a plain text file on your computer, and give it the extension .csv.\nFind out what the comma-separated format looks like.\nUse your imagination to complete the following table and put it in the .csv file.\n\n\n\nreplicate\ncond1\ncond2\n\n\n\n\n\n10\n\n\n\n\n11\n\n\n\n\n10\n\n\n\n\n12\n\n\n\n\n13\n\n\n\n\n13\n\n\n\n\nLet’s do some unethical data massaging, and tweak the csv such that you get a significant p-val between the two conditions\n\nNow try to read in that table in your python notebook.\nGet the following code to run on your dataframe (referred to as df below):\n\nfrom scipy.stats import ttest_ind\nt_stat, p_value = ttest_ind(df['cond1'], df['cond2'])\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\nWhat does that code do?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nIn a csv file, the values are separated by a comma. As an example we compare cond1 vs cond2 for 6 replicate samples (R1 through R6). A possible data file (saved as mydata.csv) could look like:\n\nreplicate,cond1,cond2\nR1, 10, 1\nR2, 11, 2\nR3, 10, 2\nR4, 12, 3\nR5, 13, 2\nR6, 13, 1\n\nTo load this file in a dataframe, use\n\ndf = pd.read_csv('mydata.csv',header=0)\n\nRunning the code results in\n\nT-statistic: 15.076382102391054, P-value: 3.329703635867358e-08\n\nThe code calulates the t-statistic between the two conditions and its associated p-value."
  }
]