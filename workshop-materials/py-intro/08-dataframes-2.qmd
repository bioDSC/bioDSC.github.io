
::: {.callout-note}
## Note
Most questions originate from the carpentry lesson ["Plotting and Programming in Python"](http://swcarpentry.github.io/python-novice-gapminder/)
:::
---

<!-- ################################################################################ -->


# Exercises for lesson 8, dataframes (2)

### Selection individual values

::: {.callout-tip icon='false'}
## Question 
Import data for europe:

```python
import pandas as pd
data_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')
```

Find the GDP Capita of Serbia in 2007.
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
The selection can be done by using the labels for both the row (“Serbia”) and the column (“gdpPercap_2007”):

```Python
print(data_europe.loc['Serbia', 'gdpPercap_2007'])
```
:::



### Extent of slicing

::: {.callout-tip icon='false'}
## Question 
- Do the two statements below produce the same output?
- Based on this, what rule governs what is included (or not) in numerical slices and named slices in Pandas?

```python
print(data_europe.iloc[0:2, 0:2])
print(data_europe.loc['Albania':'Belgium', 'gdpPercap_1952':'gdpPercap_1962'])
```
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
No, they do not produce the same output! The output of the first statement is:

```
        gdpPercap_1952  gdpPercap_1957
country
Albania     1601.056136     1942.284244
Austria     6137.076492     8842.598030
```

The second statement gives:

```
        gdpPercap_1952  gdpPercap_1957  gdpPercap_1962
country
Albania     1601.056136     1942.284244     2312.888958
Austria     6137.076492     8842.598030    10750.721110
Belgium     8343.105127     9714.960623    10991.206760
```

Clearly, the second statement produces an additional column and an additional row compared to the first statement.
What conclusion can we draw? We see that a numerical slice, 0:2, omits the final index (i.e. index 2) in the range provided, 
while a named slice, ‘gdpPercap_1952’:‘gdpPercap_1962’, includes the final element.
:::


### Reconstructing Data

::: {.callout-tip icon='false'}
## Question 
Explain what each line in the following short program does: what is in first, second, etc.?

```python
first = pd.read_csv('data/gapminder_all.csv', index_col='country')
second = first[first['continent'] == 'Americas']
third = second.drop('Puerto Rico')
fourth = third.drop('continent', axis = 1)
fourth.to_csv('result.csv')
```
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
Let’s go through this piece of code line by line.

```Python
first = pd.read_csv('data/gapminder_all.csv', index_col='country')
```
This line loads the dataset containing the GDP data from all countries into a dataframe called first. The index_col='country' parameter selects which column to use as the row labels in the dataframe.


```Python
second = first[first['continent'] == 'Americas']
```
This line makes a selection: only those rows of first for which the ‘continent’ column matches ‘Americas’ are extracted. Notice how the Boolean expression inside the brackets, first['continent'] == 'Americas', is used to select only those rows where the expression is true. Try printing this expression! Can you print also its individual True/False elements? (hint: first assign the expression to a variable)


```Python
third = second.drop('Puerto Rico')
```
As the syntax suggests, this line drops the row from second where the label is ‘Puerto Rico’. The resulting dataframe third has one row less than the original dataframe second.


```Python
fourth = third.drop('continent', axis = 1)
```
Again we apply the drop function, but in this case we are dropping not a row but a whole column. To accomplish this, we need to specify also the axis parameter (we want to drop the second column which has index 1).


```Python
fourth.to_csv('result.csv')
```
The final step is to write the data that we have been working on to a csv file. Pandas makes this easy with the to_csv() function. The only required argument to the function is the filename. Note that the file will be written in the directory from which you started the Jupyter or Python session.

:::



### Selecting Indices

::: {.callout-tip icon='false'}
## Question 
Explain in simple terms what idxmin and idxmax do in the short program below. When would you use these methods?

```python
data = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')
print(data.idxmin())
print(data.idxmax())
```
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
For each column in data, idxmin will return the index value corresponding to each column’s minimum; idxmax will do accordingly the same for each column’s maximum value.

You can use these functions whenever you want to get the row index of the minimum/maximum value and not the actual minimum/maximum value.
:::


### Practice with Selection

::: {.callout-tip icon='false'}
## Question 
Assume Pandas has been imported and the Gapminder GDP data for Europe has been loaded. Write an expression to select each of the following:

- GDP per capita for all countries in 1982.
- GDP per capita for Denmark for all years.
- GDP per capita for all countries for years after 1985.
- GDP per capita for each country in 2007 as a multiple of GDP per capita for that country in 1952.
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
1:

```Python
data['gdpPercap_1982']
```

2:

```Python
data.loc['Denmark',:]
```

3:

```Python
data.loc[:,'gdpPercap_1985':]
```
Pandas is smart enough to recognize the number at the end of the column label and does not give you an error, although no column named gdpPercap_1985 actually exists. This is useful if new columns are added to the CSV file later.

4:

```Python
data['gdpPercap_2007']/data['gdpPercap_1952']
```
:::



### Epicardial cells

*Disclaimer: we are analyzing single cell data in this exercise. Within the constraints
of this introductory course, we don't use proper statistic or methodological approaches
to analyze the data. These exercises are meant to teach you Python concepts, but also
give you a flavor of biological data analysis.*

::: {.callout-note appearance="simple"}
## To load the data from the kohela study [Kohela et al.](https://doi.org/10.1126/scitranslmed.abf2750) (GSE149331) we can also download it directly from the <i>bio</i>DSC website, and immediately transpose it in one line:

```Python
# Import kohela data; note the .T at the end, which immediately transposes!
df_cells_kohela2 = \
    pd.read_csv("https://biodsc.nl/static/data/kohela-et-al.csv",header=0,index_col=0).T
```
:::
::: {.callout-tip icon='false'}
## Question A
In the [RNA-seq data](/static/data/kohela-et-al.csv), we can create another column that reflects the condition
of the cells, WT or mutant. Fill in the blanks to achieve this:

```python
df_cells_kohela2['Condition'] = ____

df_cells_kohela2.loc[df_cells_kohela2.index.str.contains('WT_'), 'Condition'] = ____
df_cells_kohela2.loc[df_cells_kohela2.index._______, _________] = ______
```
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer

```Python
df_cells_kohela2['Condition'] = 'unknown'
df_cells_kohela2.loc[df_cells_kohela2.index.str.contains('WT_'), 'Condition'] = 'WT'
df_cells_kohela2.loc[df_cells_kohela2.index.str.contains('mutant_'), 'Condition'] = 'mutant'
```

You can inspect the values in the new column to check if the contents are as expected:
```
print(df_cells_kohela2['Condition'])
```

:::

::: {.callout-tip icon='false'}
## Question B
What is the difference between str.contains and str.match?
:::
::: {.callout-caution collapse='true' icon='false'}
## Answer
See documentation for [str.contains](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html) and [str.match](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.match.html).
`str.contains` tests whether the search string is contained in each string (at any location), whilst `str.match` determines if each string starts with a match.
:::


::: {.callout-tip icon='false'}
## Question C

Now calculate the mean value of TFAP2A expression in WT cells vs. mutant cells. 
Does there appear to be more TFAP2A expression in the mutant cells?
:::
::: {.callout-caution collapse='true' icon='false'}
## Answer
```
df_cells_kohela2.loc[:,['TFAP2A','Condition']].groupby('Condition').mean()
```
Yields
```
             TFAP2A
Condition          
WT         1.535655
mutant     9.601550
```
so this indeed appears to be the case.
:::










<!-- ################################################################################ --> 



# Additional exercises Lesson 8



### Many Ways of Access

::: {.callout-tip icon='false'}
## Question 
There are at least two ways of accessing a value or slice of a DataFrame: by name or index. However, there are many others. For example, a single column or row can be accessed either as a DataFrame or a Series object.

Suggest different ways of doing the following operations on a DataFrame:

- Access a single column
- Access a single row
- Access an individual DataFrame element
- Access several columns
- Access several rows
- Access a subset of specific rows and columns
- Access a subset of row and column ranges
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
1. Access a single column:

```Python
# by name
data["col_name"]   # as a Series
data[["col_name"]] # as a DataFrame

# by name using .loc
data.T.loc["col_name"]  # as a Series
data.T.loc[["col_name"]].T  # as a DataFrame

# Dot notation (Series)
data.col_name

# by index (iloc)
data.iloc[:, col_index]   # as a Series
data.iloc[:, [col_index]] # as a DataFrame

# using a mask
data.T[data.T.index == "col_name"].T
```

2. Access a single row:

```Python
# by name using .loc
data.loc["row_name"] # as a Series
data.loc[["row_name"]] # as a DataFrame

# by name
data.T["row_name"] # as a Series
data.T[["row_name"]].T # as a DataFrame

# by index
data.iloc[row_index]   # as a Series
data.iloc[[row_index]]   # as a DataFrame

# using mask
data[data.index == "row_name"]
```



3. Access an individual DataFrame element:

```Python
# by column/row names
data["column_name"]["row_name"]         # as a Series

data[["col_name"]].loc["row_name"]  # as a Series
data[["col_name"]].loc[["row_name"]]  # as a DataFrame

data.loc["row_name"]["col_name"]  # as a value
data.loc[["row_name"]]["col_name"]  # as a Series
data.loc[["row_name"]][["col_name"]]  # as a DataFrame

data.loc["row_name", "col_name"]  # as a value
data.loc[["row_name"], "col_name"]  # as a Series. Preserves index. Column name is moved to `.name`.
data.loc["row_name", ["col_name"]]  # as a Series. Index is moved to `.name.` Sets index to column name.
data.loc[["row_name"], ["col_name"]]  # as a DataFrame (preserves original index and column name)

# by column/row names: Dot notation
data.col_name.row_name

# by column/row indices
data.iloc[row_index, col_index] # as a value
data.iloc[[row_index], col_index] # as a Series. Preserves index. Column name is moved to `.name`
data.iloc[row_index, [col_index]] # as a Series. Index is moved to `.name.` Sets index to column name.
data.iloc[[row_index], [col_index]] # as a DataFrame (preserves original index and column name)

# column name + row index
data["col_name"][row_index]
data.col_name[row_index]
data["col_name"].iloc[row_index]

# column index + row name
data.iloc[:, [col_index]].loc["row_name"]  # as a Series
data.iloc[:, [col_index]].loc[["row_name"]]  # as a DataFrame

# using masks
data[data.index == "row_name"].T[data.T.index == "col_name"].T
```


4. Access several columns:


```Python
# by name
data[["col1", "col2", "col3"]]
data.loc[:, ["col1", "col2", "col3"]]

# by index
data.iloc[:, [col1_index, col2_index, col3_index]]
```


5. Access several rows

```Python
# by name
data.loc[["row1", "row2", "row3"]]

# by index
data.iloc[[row1_index, row2_index, row3_index]]
```


6. Access a subset of specific rows and columns

```Python
# by names
data.loc[["row1", "row2", "row3"], ["col1", "col2", "col3"]]

# by indices
data.iloc[[row1_index, row2_index, row3_index], [col1_index, col2_index, col3_index]]

# column names + row indices
data[["col1", "col2", "col3"]].iloc[[row1_index, row2_index, row3_index]]

# column indices + row names
data.iloc[:, [col1_index, col2_index, col3_index]].loc[["row1", "row2", "row3"]]
```


7. Access a subset of row and column ranges

```Python
# by name
data.loc["row1":"row2", "col1":"col2"]

# by index
data.iloc[row1_index:row2_index, col1_index:col2_index]

# column names + row indices
data.loc[:, "col1_name":"col2_name"].iloc[row1_index:row2_index]

# column indices + row names
data.iloc[:, col1_index:col2_index].loc["row1":"row2"]
```

:::



### GDPs
::: {.callout-tip icon='false'}
## Question E

1. Between '87 and '92 the GDP of many countries took a hit. Are there any countries which had an *increase* between those two years? Which ones?
2. Calculate the average GDP between all European countries per year.
3. Normalize the dataframe by this trend.
:::


::: {.callout-caution collapse='true' icon='false'}
## Answer


###### 1. Are there any countries which had a positive increase between those two years? Which ones?
```
# First load the data again
data_europe = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')

# We can calculate the difference between those two years
GDP_change_87_92 = \
    data_europe.loc[:, 'gdpPercap_1992'] - data_europe.loc[:, 'gdpPercap_1987']
# And check which countries show a positive change
print(GDP_change_87_92.index[GDP_change_87_92>0])
```

###### 2. Calculate the average GDP between all European countries per year.
```
data_europe_avgGDPperyear = data_europe.mean()
```

###### 3. Normalize the dataframe by this trend.
data_europe_relative = data_europe.div(data_europe_avgGDPperyear, axis=1)

We can now more easily see how countries compared to other countries over 
the years. Greece, for example, starts out its economy with a GDP of 62%
of the average European country, but ends up with a GDP that's 110% of 
the average European country.

:::



### Gene expression

::: {.callout-tip icon='false'}
## Question D
- Convert the data below to a file you can import (e.g.: csv, tsv), import it to a pandas df, and determine the following:
    1. The average CRP gene expression per condion.
    2. The corresponding standard deviations.
    3. The log2-fold change between WT, condition A, and condition B.
    4. Do the same for ACTA1.
    5. Normalize all gene expression levels to their average respective wild type levels.

```
gene	expression	condition
CRP	873	WT
CRP	324	WT
CRP	214	WT
CRP	151	WT
CRP	1220	A
CRP	450	A
CRP	300	A
CRP	210	A
CRP	800	B
CRP	200	B
CRP	200	B
CRP	130	B
ACTA1	7457	WT
ACTA1	2342	WT
ACTA1	8000	WT
ACTA1	9000	WT
ACTA1	6500	A
ACTA1	2200	A
ACTA1	7500	A
ACTA1	8000	A
ACTA1	1000	B
ACTA1	1123	B
ACTA1	3211	B
ACTA1	1231	B
```

:::


::: {.callout-caution collapse='true' icon='false'}
## Answer

#### Convert data to .csv and load the file 

This exercise is also meant to illustrate that it can sometimes be tedious to handle data.
Many data files are however plain text, and you can open them to see what's going on.
One way to convert the exercise data to a convenient format, could be:

- Open a text editor like Notepad (Windows), TextEdit (Mac), or your own favorite.
- Copy the text to the text editor.
- Use search and replace to replace double spaces by single spaces.
- Repeat this until you only have single spaces.
- Now replace spaces by commas.
- You now have a comma separated file, save it e.g. as gene_expression.csv.

See also this example [gene_expression.csv](/static/data/gene_expression.csv).

To load the file, use:
```Python
df_gene_expression = \
    pd.read_csv('gene_expression.csv')
```

#### The average CRP gene expression per condion.


This can be done in multiple ways. Here's an elegant solution that gives
information about the std and mean in one go:
```Python
df_gene_expression.groupby(['gene', 'condition']).describe()
```

Alternatively, we can do this using group_by, the mean and std function.
We can use the column names directly as argument for the group by, 
then apply mean, then apply "reset_index()" to convert the multi-index to columns.

```Python
df_gene_expression_avg = df_gene_expression.groupby(by = ['gene', 'condition']).mean().reset_index()
# CRP expression
print(df_gene_expression_avg.loc[df_gene_expression_avg['gene']=='CRP',:])
```

#### The standard deviation
The corresponding standard deviations can be calculated in a similar way:
```Python
df_gene_expression_std = df_gene_expression.groupby(by = ['gene', 'condition']).std().reset_index()
print(df_gene_expression_std.loc[df_gene_expression_std['gene']=='CRP',:])
```

#### An alternative strategy
A disadvantage of this strategy is that for large dataframes, you might be calculating many mean values
that you're not interested in in the end.
You could also first select the gene of interest.

```Python
df_gene_expression_avg_CRP = df_gene_expression.loc[df_gene_expression['gene']=='CRP',:].groupby(by = ['gene', 'condition']).mean().reset_index()
df_gene_expression_std_CRP = df_gene_expression.loc[df_gene_expression['gene']=='CRP',:].groupby(by = ['gene', 'condition']).std().reset_index()
print(df_gene_expression_avg_CRP)
print(df_gene_expression_std_CRP)
```

#### The log2-fold change between WT, condition A, and condition B.

Note that for a real-world analysis, you might use specialized tools for this, which 
take into account more statistical considerations (e.g. like DESeq2).
But to directly calculate the log2-fold change, we need to divide mean condition X by mean WT, 
calculate the ratios, and take the logarithm with base 2.

There are multiple ways to do this. The solution below exploits the fact that when dividing 
two dataframes, the indices are "aligned". Meaning here that if the index of the numerator dataframe
corresponds to ACTA1, it will be divided by a record from the denominator's dataframe that's also 
has an ACTA1 index, even if the numerator and denominator's dataframes are of different length.
This also applies to series.

```Python
# Calculate averages again: group by, selecting the grouped expression series, calculate the mean
gene_expression_avg = df_gene_expression.groupby(['gene','condition'])['expression'].mean()
# Identify the WT records with a boolean array
WT_records = gene_expression_avg.index.get_level_values(1)=='WT'
# Select such that we divide a non-WT series by a WT series
# We use "reset index" to set the index to gene names only, this allows for alignment
ratios = gene_expression_avg.loc[~WT_records].reset_index(level=1,drop=True)/gene_expression_avg.loc[WT_records].reset_index(level=1,drop=True)
# The parameters above were series, let's convert back to dataframe
df_ratios = ratios.to_frame()
# Now we need to restore the index to identify the conditions
df_ratios.index=gene_expression_avg[~WT_records].index
# And we can take the log2
df_log2fc = np.log2(df_ratios)
# search for "expression" column name and replace by "log2fc"
df_log2fc = df_log2fc.rename(columns={'expression':'log2fc'})
print(df_log2fc)
```

#### Do the same for ACTA1.

See above.


#### Normalize all gene expression levels to their average respective wild type levels.

We can use a same strategy as above.

```Python
# First set the index to the gene column, then take the expression values
gene_expression = df_gene_expression.set_index('gene')['expression']
# Take the average information from above, and also set the index to the gene value,
# such that we have average values indexed in the proper way to match the gene_expression.
wt_avg_values   = gene_expression_avg.loc[WT_records].reset_index(level=1,drop=True)
# We can now divide one by the other
gene_expression_normalized = gene_expression/wt_avg_values
# We can create a new dataframe with normalized data
df_gene_expression_norm = df_gene_expression.copy()
df_gene_expression_norm['expression_normalized'] = gene_expression_normalized.values
```



#### Alternative strategy for the above two exercises.
An alternative could be to pivot the dataframe.
This can also be used to calculate log2fc values.

```Python
# Create a copy of the dataframe to avoid confusion with other exercises.
df_gene_expression_2 = df_gene_expression.copy()
# To pivot, it is required to be able to match rows uniquely. Add an index to do this.
df_gene_expression_2['rows'] = df_gene_expression_2.groupby(['gene','condition']).cumcount()
# Pivot df_gene_expression_2 such that the gene column is used to expand the 'expression', remove 'rows'
df_gene_expression_wide = df_gene_expression_2.pivot(index=['condition','rows'], columns='gene', values='expression').reset_index().drop(columns='rows')
# Now calculate the averages
df_averages =  df_gene_expression_wide.groupby(['condition']).mean()
# Now divide df_gene_expression_wide columns by df_averages
values_WT = df_averages.T['WT'].values
# We can use the dataframe's division method to divide all columns by the WT averages
df_gene_expression_norm2 = df_gene_expression_wide.loc[:,'ACTA1':'CRP'].div(values_WT, axis=1)
# Add annotation condition
df_gene_expression_norm2['condition'] = df_gene_expression_wide['condition']
# We now have the normalized dataframe
print(df_gene_expression_norm2)
# Calculate L2FC:
df_log2fc_2 = df_gene_expression_norm2.groupby(['condition']).mean().apply(np.log2)
# We now have the log2-fold changes:
print(df_log2fc_2)
```

:::

<!-- # Solution by Frans

1. The average CRP gene expression per condion.    
```Python
df[df.gene=='CRP'].groupby(['gene','condition'])['expression'].mean()
```

2. The corresponding standard deviations.
```Python
df.groupby(['gene','condition'])['expression'].std()
```

3. The log2-fold change between WT, condition A, and condition B.
4. Do the same for ACTA1.
```Python
dfmean = df.groupby(['gene','condition'])['expression'].mean().to_frame()
WT = dfmean.index.get_level_values(1)=='WT'
ratios = dfmean.loc[~WT].reset_index(level=1,drop=True)/dfmean.loc[WT].reset_index(level=1,drop=True)
ratios.index=dfmean[~WT].index
print(np.log2(ratios))
```

5. Normalize all gene expression levels to their average respective wild type levels.
```Python
dfw = df.copy()
CRP = dfw.gene=='CRP'
dfw.loc[ CRP,'expression']  = dfw.loc[CRP,'expression']/dfmean.loc[('CRP','WT')].values
dfw.loc[~CRP,'expression'] = dfw.loc[~CRP,'expression']/dfmean.loc[('ACTA1','WT')].values
```
-->





### Exploring available methods using the dir() function

Python includes a dir() function that can be used to display all of the available methods (functions) that are built into a data object. In Episode 4, we used some methods with a string. But we can see many more are available by using dir():

```python
my_string = 'Hello world!'   # creation of a string object 
dir(my_string)
```

This command returns:

```python
['__add__',
...
'__subclasshook__',
'capitalize',
'casefold',
'center',
...
'upper',
'zfill']
```

You can use help() or Shift+Tab to get more information about what these methods do.

::: {.callout-tip icon='false'}
## Question E
Assume Pandas has been imported and the Gapminder GDP data for Europe has been loaded as data. Then, use dir() to find the function that prints out the median per-capita GDP across all European countries for each year that information is available.
:::

::: {.callout-caution collapse='true' icon='false'}
## Answer
Among many choices, dir() lists the median() function as a possibility. Thus,
```Python
data.median()
```
:::
